# Background {#sec-bg}


## Factor analytic model

Factor analysis (FA) is a statistical method which attempts to use fewer
underlying factors to explain the correlation between a large set of
observed variables [@mardiaMultivariateAnalysis1979]. FA provides a useful tool for exploring the covariance structure among observable variables [@hiroseSparseEstimationNonconcave2015].

An FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g. in
psychology where certain variables, like intelligence, cannot be directly measured [@mardiaMultivariateAnalysis1979].

Suppose we have an observable random vector $\boldsymbol{y}\in \mathbb{R}^p$
with mean $\mathbb{E}(\boldsymbol{y})=\boldsymbol{\mu}$ and variance
$\mathbb{V}(\boldsymbol{y})=\boldsymbol{\Sigma}$. Then a $k$-order factor analysis model
for $\boldsymbol{y}$ can be given by \begin{equation}
\boldsymbol{y}=\boldsymbol{\Lambda} \boldsymbol{f}+\boldsymbol{\mu}+\boldsymbol{\epsilon},
\end{equation} where $\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}$   $\boldsymbol{f} \in \mathbb{R}^{k}$ is and $\boldsymbol{\epsilon} \in \mathbb{R}^{p}$ is are called the _loading matrix_, _common factors_  and _unique factors_, respectively. 

To make the model well-defined, we may assume

$$\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_k\\\boldsymbol{0}_p\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_k & \mathbf{0}_{k\times p}\\\mathbf{0}_{p\times k} & \mathbf{\Psi}\end{bmatrix},$$

where $\boldsymbol{\Psi}$ is a diagonal matrix where we denote the $i$-th diagonal entry as $\Psi_{ii}$. Based on this assumption, the covariance of observable vector $\boldsymbol{y}$ can
be modelled by

$$\mathbb{V}(\boldsymbol{y}|\boldsymbol{\Lambda},\boldsymbol{\Psi} )=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.$$




::: callout-note

## Indeterminacy of the loading matrix

One can easily see that if our factor analytic model is given by (1),
then it can also be modelled as
$$\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}$$
where the matrix $\boldsymbol{M}$ is orthogonal and simultaneously the
variance of $\boldsymbol{y}$ given by (2) still holds, since
$$\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.$$
Therefore a rotated loading matrix $\boldsymbol{\Lambda}\boldsymbol{M}$ is still a valid
loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like [@mardiaMultivariateAnalysis1979]
$$\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}$$

:::




## Parameter Estimation

We denote the set of parameters by $\boldsymbol{\theta} = \{\text{vec}(\boldsymbol{\Lambda}),\text{vec}(\boldsymbol{\Psi})\}$ where $\text{vec}(\cdot)$ is the vectorisation of the input.

Traditionally, a two-step procedure is used to construct a factor
analytic model: estimate parameters by maximum likelihood estimation
(aka, MLE) and then use rotation techniques to find an interpretable
model.  

### Maximum Likelihood Estimation 

Suppose we have $n$ independent and identically distributed observations
$\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_N$ from a p-dimensional
multi-variate normal distribution $N_p(\boldsymbol{\mu},\boldsymbol{\Sigma})$ and by our
hypothesis, we have $\boldsymbol{\Sigma}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}$. Then the likelihood function is given by

$$
L(\boldsymbol{\theta})=\prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].
$$ {#eq-likelihood}

The maximum likelihood estimate (MLE) of $\boldsymbol{\theta}$, denoted as $\hat{\boldsymbol{\theta}}$, is found by finding $\boldsymbol{\theta}$ that maximises @eq-likelihood. However, it is often more convenient to maximise the log likelihood function. 

::: {#lem-MLE}

The log-likelihood is given by 

$$\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi}) =  -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right]$$ {#eq-loglikelihood}

where $\boldsymbol{S}$ is the sample covariance defined as $\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top$. 

:::

::: {.proof}

\begin{align*}
\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})=& \sum^n_{i=1}\left[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=& -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=& -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right],
\end{align*}

[TODO: show step from line 2 to 3].

:::



The MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)

\begin{align}
\frac{\partial}{\partial \boldsymbol{\Lambda}}\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})&=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})&= 0.
\end{align}

However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximisation (EM) algorithm [@Rubin1982EMAlgorithms]. 



## The EM Algorithm {#sec-EM}

The Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data [@Ng2012EMAlgorithm]. As we mentioned, the key of maximum likelihood estimation is solving equation
$$
\frac{\partial}{\partial \beta}l=0.
$$
However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, _augmented data_(or complete) refers to the integration of this unobserved component, enhancing the algorithm's ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete data sets effectively.

### The E-step and M-step

Let $\boldsymbol{x}$ denote the vector containing complete data, $\boldsymbol{y}$ denote the observed incomplete data and $\boldsymbol{z}$ denote the vector containing the missing data. Here 'missing data' is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later) [@Ng2012EMAlgorithm]. Also we assume $\boldsymbol{\beta}$ as the parameter we want to estimate over the parameter space $\boldsymbol{\Omega}$.

Now denote $f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})$ as the probability density function (p.d.f.) of the random vector $\boldsymbol{X}$ corresponding to $\boldsymbol{x}$. Then the complete-data log-likelihood function when complete data is fully observed can be given by 

$$\log L_{\boldsymbol{X}}(\boldsymbol{\beta})=\log f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta}).$$

The EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of $\log L_{\boldsymbol{X}}(\boldsymbol{\beta})$. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given $\boldsymbol{y}$ and current fit for $\boldsymbol{\beta}$.

On the $(k+1)^th$ iteration, we have
$$
\begin{align*}
&\text{E-step: Compute } Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}):=\mathbb{E}_{\boldsymbol{X}}[\log L_{\boldsymbol{X}}(\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
&\text{M-step: Update }\boldsymbol{\beta}^{(k+1)} \text{ as }\boldsymbol{{\beta}}^{(k+1)}:=\text{arg}\max_{\boldsymbol{\beta}} Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) \text{ (Original EM)}\\
& \text{ Or update }\boldsymbol{{\beta}}^{(k+1)} \text{ such that } Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\text{ (Generalized EM)}.
\end{align*} 
$$
We keep iterating between E-step and M-step until convergence, which may be determined by a criteria such as $||\boldsymbol{\beta}^{(k+1)}-\boldsymbol{\beta}^{(k)}||_p\leq \epsilon$ for some p-norm $||\cdot||_p$ and positive $\epsilon$.

Meanwhile, the M-step in both original and generalized algorithms defines a mapping from the parameter space $\boldsymbol{\Omega}$ to itself by 
$$
\begin{align*}
M: \boldsymbol{\Omega} &\to \boldsymbol{\Omega}\\
   \boldsymbol{\beta}^{(k)} &\mapsto \boldsymbol{\beta}^{(k+1)}.
\end{align*}
$$

### Convergence of the EM Algorithm

By the definition of conditional likelihood, our likelihood of complete data can be expressed by
$$
L_{\boldsymbol{X}}(\boldsymbol{\beta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})=L_{\boldsymbol{Y}}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
$$
and hence the log-likelihood is given by
$$
\log L_{\boldsymbol{X}}(\boldsymbol{\beta}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
$$
Take expectation to both sides of the equation with respect to $\boldsymbol{x|y}$ and replace $\boldsymbol{\beta}$ by $\boldsymbol{\beta}^{(k)}$, we will have 
$$
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
$$
Now consider the difference of log-likelihood of $\boldsymbol{Y}$ function between two iterations, we have
$$
\begin{align*}
  \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k+1)})-\log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k)}) =
  &\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
$$
By the procedure of EM-algorithm, we always have $Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})$. By the Jensen's inequality, we have $\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].$
Therefore during iterations, the log-likelihood of observed data $\boldsymbol{Y}$ keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.



### Rotation techniques
After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method [@hiroseSparseEstimationNonconcave2015].

Suppose $Q(\boldsymbol{\Lambda})$ is an criterion for $\boldsymbol{\Lambda}$ in the rotation procedure, and we may express it as $Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})$ where $P(\cdot)$ is some loss function[@hiroseSparseEstimationNonconcave2015]. Specifically, if we set $P(\cdot)=|\cdot|$, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as [@Jennrich2004Rotation]

\begin{align*}
&\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}

where $\boldsymbol{\Lambda}_0$ is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is 

\begin{align*}
&\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}

where $\hat{\boldsymbol{\Lambda}}_{\text{MLE}}$ is the maximum likelihood estimator.

### Discussion about two-step method

The traditional two-step method faces significant shortcomings, primarily its unsuitability [@hiroseSparseEstimationNonconcave2015]. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by
$$
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 & 0 \\
0 & 0.8 \\
0 & 0\\
0 & 0\\
0 & 0\\
\end{bmatrix}.
$$
We generate $\boldsymbol{Y}\in \mathbb{R}^{{100}\times 5}$ using common factor $\boldsymbol{f}\in \mathbb{R}^{{100}\times 2}$ where each factor is generated randomly from standard normal distribution $N(0,1)$. We utilize the R-function `factanal()` to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is

$$
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
-0.158 & 0.985\\
0 & 0\\
0.997 & 0\\
0.207 & 0\\
0 & 0\\
\end{bmatrix},
$$
which is neither precise nor sparse.

Simulation code:

```{r}
set.seed(123)
n <- 100 # Number of observations
p <- 5   # Number of features

factor1 <- rnorm(n, 0, 1) # set factors
factor2 <- rnorm(n, 0, 1)

X <- matrix(NA, nrow=n, ncol=p)
X[,1] <- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading
X[,2] <- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading
X[,3] <- rnorm(n, 0, 1)  # Noise, no strong loading on any factor
X[,4] <- rnorm(n, 0, 1)  
X[,5] <- rnorm(n, 0, 1) 

fa_result <- factanal(factors = 2, covmat = cor(X))
rotated_fa <- varimax(fa_result$loadings)

```




Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model [@mardiaMultivariateAnalysis1979]. We have
\begin{align*}
  &&&H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &&&H_a: \text{ Otherwise}.
\end{align*}
The test statistics is given by $\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})$ which has an asymptotic $\chi^2_s$ distribution with $s=\frac{1}{2}(p-k)^2-(p+k)$ by the property of MLE, where $F$ is given by (@mardiaMultivariateAnalysis1979)
$$
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
$$
Typically, the test is conducted at a $5\%$ significant level. We can start the procedure from a very small $k$, say, $k=1$ or $k=2$ and then increase $k$ until the null hypothesis is not rejected.



### Penalized Likelihood Method

Penalized likelihood method can be viewed as a generalization of two-step method mentioned above [@hiroseSparseEstimationNonconcave2015]. A penalized factor analytic model can be obtained by solving following optimization problem
$$
 \max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}l_p := l(\boldsymbol{\Lambda},\boldsymbol{\Psi})-\rho\sum^p_{i=1}\sum^k_{j=1}P(|\boldsymbol{\Lambda}_{ij}|).
$$
where we call $l_p$ as the penalized likelihood, $\rho$ is called regularization parameter and we can treat $P(\cdot)$ as a penalized function. There are many types of penalized functions developed, such as LASSO ($P(\cdot)=|\cdot|$) and MC+ ($P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|<\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)$) [@hiroseSparseEstimationNonconcave2015]. In this article, we will mainly focus on the LASSO penalty.

### LASSO penalty and LASSO estimator

Again, recall that the LASSO penalized likelihood is given by
$$l_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|$$
and the LASSO estimator, denoted as $(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)$, can be obtained via

\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}

An E-M algorithm can be applied for evaluating the LASSO estimator.



## The EM Algorithm in LASSO Factor Analytic Models {#sec-penEM}

### Model Setup

Suppose we have the $n$ centralized observations, $\{\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\}$, where $\boldsymbol{y}_j\in \mathbb{R}^p$ and hence the mean, $\boldsymbol{\mu}_j=\boldsymbol{0}_p$ ($j=1,2,\dots,n$). For each of the observation $\boldsymbol{y}_j$, the common factor and unique factor are $\boldsymbol{f}_j$ and $\boldsymbol{\epsilon}_j$ respectively. Denote the response matrix as $\boldsymbol{Y}=[\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n]^\top$, the common factor matrix as $\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_n]^\top$, and the unique factor matrix as $\boldsymbol{\hat\epsilon}=[\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2,\dots,\boldsymbol{\epsilon}_n]^\top$. Then the model can be written as $$\boldsymbol{Y}=\boldsymbol{F}\boldsymbol{\Lambda}^\top+\boldsymbol{\hat\epsilon}.$$\
We also assume

1.  $\boldsymbol{\epsilon}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{\Psi}$ for $j=1,2,\dots,n$, where $\boldsymbol{\Psi}$ is a diagonal matrix defined by $\boldsymbol{\Psi}=diag(\psi_{11},\psi_{22},\dots,\psi_{pp})$.

2.  $\boldsymbol{f}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{I}_k$ for $j=1,2,\dots,n$.

3.  $\boldsymbol{y}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}$ for $j=1,2,\dots,n$. Those $\boldsymbol{y}_j$ are pairwisely independent.

### The EM Algorithm

We treat the common factor matrix $\boldsymbol{F}$ as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of $(\boldsymbol{Y},\boldsymbol{F})$ given $\boldsymbol{Y}$ and $(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})$, where $(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})$ is the parameter we got in the $k$-th iteration ($k>1$) and $(\boldsymbol{\Lambda}_{(0)},\boldsymbol{\Psi}_{(0)})$ is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get $(\boldsymbol{\Lambda}_{(k+1)},\boldsymbol{\Psi}_{(k+1)})$.

### E-Step

First, the joint likelihood of $(\boldsymbol{Y},\boldsymbol{F})$, denoted as $L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$, is given by

$$
\begin{align*}
L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&\prod_{i=1}^nf(\boldsymbol{y}_i,\boldsymbol{f}_i)\\
=&\prod_{i=1}^nf(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\\
=&\prod_{i=1}^nN(\boldsymbol{y}_i;\boldsymbol{\Lambda}\boldsymbol{f}_i,\boldsymbol{\Psi})N(\boldsymbol{f}_i;\boldsymbol{0}_k,\boldsymbol{I}_k)\\
=&\prod_{i=1}^n[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Psi})^{-\frac{1}{2}}\exp\{-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}\\
&(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\}][(2\pi)^{-\frac{k}{2}}\det({\boldsymbol{I}_k})^{-\frac{1}{2}}\exp\{-\frac{1}{2}\boldsymbol{f}_i^\top\boldsymbol{I}_k^{-1}\boldsymbol{f}_i\}]\\
= &(2\pi)^{-\frac{n}{2}(p+k)}(\prod_{i=1}^p\psi_{jj})^{-\frac{n}{2}}\exp\{-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}\}\\
&\exp\{-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\}.
\end{align*}
$$


