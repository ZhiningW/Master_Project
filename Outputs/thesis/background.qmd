# Background {#sec-bg}


## Factor analytic model

Factor analysis (FA) is a statistical method which attempts to use fewer
underlying factors to explain the correlation between a large set of
observed variables [@mardiaMultivariateAnalysis1979]. FA provides a useful tool for exploring the covariance structure among observable variables [@hiroseSparseEstimationNonconcave2015].

An FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g. in
psychology where certain variables, like intelligence, cannot be directly measured [@mardiaMultivariateAnalysis1979].

Suppose we have an observable random vector $\boldsymbol{y}_i\in \mathbb{R}^p$ for the $i$-th subject
with mean $\mathbb{E}(\boldsymbol{y}_i)=\boldsymbol{\mu}$ and variance
$\mathbb{V}(\boldsymbol{y}_i)=\boldsymbol{\Sigma}$. Then a $k$-order factor analysis model
for $\boldsymbol{y}_i$ can be given by \begin{equation}
\boldsymbol{y}_i=\boldsymbol{\mu}+\boldsymbol{\Lambda} \boldsymbol{f}_i+\boldsymbol{\epsilon}_i,
\end{equation} where $\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}$,   $\boldsymbol{f}_i \in \mathbb{R}^{k}$ is and $\boldsymbol{\epsilon}_i \in \mathbb{R}^{p}$ are called the _loading matrix_, _common factors_  and _unique (or specific) factors_, respectively. The order $k$ is usually much smaller than $p$. For simplicity, we can assume that $\boldsymbol{\mu} = \boldsymbol{0}_p$.

To make the model well-defined, we may assume

$$\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_k\\\boldsymbol{0}_p\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_k & \mathbf{0}_{k\times p}\\\mathbf{0}_{p\times k} & \mathbf{\Psi}\end{bmatrix},$$

where $\boldsymbol{\Psi}$ is a $p\times p$ diagonal matrix where we denote the $i$-th diagonal entry as $\psi_{ii}$. Based on this assumption, the covariance of observable vector $\boldsymbol{y}_i$ can
be modelled by

$$\mathbb{V}(\boldsymbol{y}_i|\boldsymbol{\Lambda},\boldsymbol{\Psi} )=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.$$

Alternatively, we can write the model as 


$$\boldsymbol{y} = (\mathbf{I}_n \otimes \boldsymbol{\Lambda})\boldsymbol{f} + \boldsymbol{\epsilon}$$
where $\boldsymbol{y} = (\boldsymbol{y}_1, \boldsymbol{y}_2, \cdots,  \boldsymbol{y}_n)$, $\boldsymbol{f} = (\boldsymbol{f}_1, \boldsymbol{f}_2, \cdots,  \boldsymbol{f}_n)$ and $\boldsymbol{\epsilon} = (\boldsymbol{\epsilon}_1, \boldsymbol{\epsilon}_2, \cdots,  \boldsymbol{\epsilon}_n)$. Thus we have


$$\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_{nk}\\\boldsymbol{0}_{np}\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_{nk} & \mathbf{0}_{nk\times np}\\\mathbf{0}_{np\times nk} & \mathbf{I}_n \otimes \mathbf{\Psi}\end{bmatrix}.$$



::: callout-note

## Indeterminacy of the loading matrix

One can easily see that if our factor analytic model is given by (1),
then it can also be modelled as
$$\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}$$
where the matrix $\boldsymbol{M}$ is orthogonal and simultaneously the
variance of $\boldsymbol{y}$ given by (2) still holds, since
$$\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.$$
Therefore a rotated loading matrix $\boldsymbol{\Lambda}\boldsymbol{M}$ is still a valid
loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like [@mardiaMultivariateAnalysis1979]
$$\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}$$

::: 




## Parameter Estimation

We denote the set of parameters by $\boldsymbol{\theta} = \{\text{vec}(\boldsymbol{\Lambda}), \text{diag}(\boldsymbol{\Psi})\}$ where $\text{vec}(\cdot)$ is the vectorisation of the input matrix and $\text{diag}(\cdot)$ is the diagonal elements of the input matrix.

Traditionally, a two-step procedure is used to construct a factor
analytic model: estimate parameters by maximum likelihood estimation
(aka, MLE) and then use rotation techniques to find an interpretable
model.  

### Maximum Likelihood Estimation 

Suppose we have $n$ independent and identically distributed observations
$\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n$ from a $p$-dimensional
multi-variate normal distribution $N_p(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Now denote $f_{\boldsymbol{y}}(\boldsymbol{y};\boldsymbol{\theta})$ as the probability density function of the random vector $\boldsymbol{Y}$ corresponding to $\boldsymbol{y}$. Then the likelihood is given by

$$
L(\boldsymbol{\theta};\boldsymbol{y})=f_{\boldsymbol{y}}(\boldsymbol{y};\boldsymbol{\theta}) = \prod^n_{i=1}f_{\boldsymbol{y}_i}(\boldsymbol{y}_i;\boldsymbol{\theta}) =  \prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].
$$ {#eq-likelihood}

The maximum likelihood estimate (MLE) of $\boldsymbol{\theta}$, denoted as $\hat{\boldsymbol{\theta}}$, is found by finding $\boldsymbol{\theta}$ that maximises @eq-likelihood. However, it is often more convenient to maximise the log likelihood function. 

::: {#lem-MLE}

The log-likelihood is given by 

$$\ell(\boldsymbol{\theta}) =  -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right]$$ {#eq-loglikelihood}

where $\boldsymbol{S}$ is the sample covariance defined as $\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top$. 

:::

::: {.proof}

\begin{align*}
\ell(\boldsymbol{\theta})=& \sum^n_{i=1}\left[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=& -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=& -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right],
\end{align*}

[TODO: show step from line 2 to 3].

:::



The MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)

\begin{align}
\frac{\partial}{\partial \boldsymbol{\Lambda}}\ell(\boldsymbol{\theta})&=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}\ell(\boldsymbol{\theta})&= 0.
\end{align}

However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximisation (EM) algorithm [@Rubin1982EMAlgorithms]. 



## Rotation techniques
After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method [@hiroseSparseEstimationNonconcave2015].

Suppose $Q(\boldsymbol{\Lambda})$ is an criterion for $\boldsymbol{\Lambda}$ in the rotation procedure, and we may express it as $Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})$ where $P(\cdot)$ is some loss function[@hiroseSparseEstimationNonconcave2015]. Specifically, if we set $P(\cdot)=|\cdot|$, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as [@Jennrich2004Rotation]

\begin{align*}
&\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}

where $\boldsymbol{\Lambda}_0$ is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is 

\begin{align*}
&\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}

where $\hat{\boldsymbol{\Lambda}}_{\text{MLE}}$ is the maximum likelihood estimator.

### Discussion about two-step method

The traditional two-step method faces significant shortcomings, primarily its unsuitability [@hiroseSparseEstimationNonconcave2015]. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by
$$
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 & 0 \\
0 & 0.8 \\
0.6 & 0\\
0 & 0.7\\
0 & 0.6\\
\end{bmatrix}.
$$
and the real $\boldsymbol{\Psi}$ is given by 
$$
\boldsymbol{\Psi}=\text{diag}(0.1,0.2,0.2,0.1,0.1).
$$
We generate $\boldsymbol{Y}\in \mathbb{R}^{{1000}\times 5}$ from a multivariate normal distribution with mean $\mathbb{0}_{5}$ and covariance $\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}$. We utilize the R-function `factanal()` to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is

$$
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
0 & 0.814\\
0.873 & 0\\
0 & 0.926\\
0.912 & 0\\
0.881 & 0\\
\end{bmatrix},
\hat{\boldsymbol{\Psi}}=\text{diag}(0.333,0.238,0.138,0.166,0.224)
$$
This result gives the same sparsity result, however, it overloads on the last three dimension of responses.


Simulation code:

```{r}
set.seed(123)
n <- 1000 # Number of observations
# True loading matrix
true_loading <- matrix(c(0.8,0,0.6,0,0,0,0.8,0,0.7,0.6),nrow=5,ncol=2) 
# True psi matrix
true_psi <- diag(c(0.1,0.2,0.2,0.1,0.1))
# Generate samples from multivariate normal distribution
samples <- MASS::mvrnorm(n = 1000, mu = rep(0, 5), 
                         Sigma = true_loading %*% t(true_loading) + true_psi)
# Traditional two step procedure
fa_result <- factanal(factors = 2, covmat = cor(samples))
rotated_fa <- varimax(fa_result$loadings)
# Result
fa_result
rotated_fa
```




Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model [@mardiaMultivariateAnalysis1979]. We have
\begin{align*}
  &&&H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &&&H_a: \text{ Otherwise}.
\end{align*}
The test statistics is given by $\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})$ which has an asymptotic $\chi^2_s$ distribution with $s=\frac{1}{2}(p-k)^2-(p+k)$ by the property of MLE, where $F$ is given by (@mardiaMultivariateAnalysis1979)
$$
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
$$
Typically, the test is conducted at a $5\%$ significant level. We can start the procedure from a very small $k$, say, $k=1$ or $k=2$ and then increase $k$ until the null hypothesis is not rejected.



## Penalized Likelihood Method

Penalized likelihood method can be viewed as a generalization of two-step method mentioned above [@hiroseSparseEstimationNonconcave2015]. A penalized factor analytic model can be obtained by solving following optimization problem
$$
 \max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}l_p := l(\boldsymbol{\Lambda},\boldsymbol{\Psi})-\rho\sum^p_{i=1}\sum^k_{j=1}P(|\boldsymbol{\Lambda}_{ij}|).
$$
where we call $l_p$ as the penalized likelihood, $\rho$ is called regularization parameter and we can treat $P(\cdot)$ as a penalized function. There are many types of penalized functions developed, such as LASSO ($P(\cdot)=|\cdot|$) and MC+ ($P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|<\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)$) [@hiroseSparseEstimationNonconcave2015]. In this article, we will mainly focus on the LASSO penalty.

### LASSO penalty and LASSO estimator

Again, recall that the LASSO penalized likelihood is given by
$$l_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|$$
and the LASSO estimator, denoted as $(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)$, can be obtained via

\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}

An E-M algorithm can be applied for evaluating the LASSO estimator.

## The EM Algorithm {#sec-EM}

The Expectation-Maximization (EM) algorithm is a widely used iterative method to find the MLE, especially when the model depends on latent variables. The EM algorithm iteratively apply two distinct steps: the Expectation step (E-step) and the Maximization step (M-step). More specifically, we define 

$$Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) = \mathbb{E}\left(\ell (\boldsymbol{\theta}; \boldsymbol{y})\bigg|\boldsymbol{y},\boldsymbol{\theta}^{(t)}\right).$$
Then for the $(t + 1)$-th interation, the steps involve:

- *E-step*: Compute $Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})$.
- *M-step*: Update $\boldsymbol{\theta}^{(t+1)}$ as $\boldsymbol{\theta}$ that maximses $Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})$. 

The M-step may be replaced with updating $\boldsymbol{\theta}^{(t+1)}$ such that $Q(\boldsymbol{\theta}^{(t+1)}|\boldsymbol{\theta}^{(t)}) \geq Q(\boldsymbol{\theta}^{(t)}|\boldsymbol{\theta}^{(t)})$.

We iterate between E-step and M-step until convergence. The convergence may be determined by a criteria such as $||\boldsymbol{\theta}^{(t+1)}-\boldsymbol{\theta}^{(t)}||_p\leq \epsilon$ for some $p$-norm $||\cdot||_p$ and $\epsilon > 0$.



The EM algorithm approaches the problem of solving the likelihood equation indirectly by proceeding iteratively in terms of $\ell(\boldsymbol{\theta};\boldsymbol{y})$. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given $\boldsymbol{y}$ and current fit for $\boldsymbol{\theta}$.

::: {#lem-convergence}

## Convergence of the EM Algorithm
For all $\epsilon > 0$, there exists $t > t_0$ such that
$||\boldsymbol{\theta}^{(t)} - \hat{\boldsymbol{\theta}}|| < \epsilon$ where $\hat{\boldsymbol{\theta}}$ is the MLE.

:::

::: proof


By the definition of conditional likelihood, our likelihood of complete data can be expressed by
$$
L_{\boldsymbol{X}}(\boldsymbol{\beta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})=L_{\boldsymbol{Y}}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
$$
and hence the log-likelihood is given by
$$
\log L_{\boldsymbol{X}}(\boldsymbol{\beta}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
$$
Take expectation to both sides of the equation with respect to $\boldsymbol{x|y}$ and replace $\boldsymbol{\beta}$ by $\boldsymbol{\beta}^{(k)}$, we will have 
$$
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
$$
Now consider the difference of log-likelihood of $\boldsymbol{Y}$ function between two iterations, we have
$$
\begin{align*}
  \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k+1)})-\log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k)}) =
  &\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
$$
By the procedure of EM-algorithm, we always have $Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})$. By the Gibbs's inequality, we have $\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].$
Therefore during iterations, the log-likelihood of observed data $\boldsymbol{Y}$ keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.

:::

