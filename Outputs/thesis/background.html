<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>background</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="background_files/libs/clipboard/clipboard.min.js"></script>
<script src="background_files/libs/quarto-html/quarto.js"></script>
<script src="background_files/libs/quarto-html/popper.min.js"></script>
<script src="background_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="background_files/libs/quarto-html/anchor.min.js"></script>
<link href="background_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="background_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="background_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="background_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="background_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="sec-bg" class="level1">
<h1>Background</h1>
<section id="factor-analytic-model" class="level2">
<h2 class="anchored" data-anchor-id="factor-analytic-model">Factor analytic model</h2>
<p>Factor analysis (FA) is a statistical method which attempts to use fewer underlying factors to explain the correlation between a large set of observed variables <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>. FA provides a useful tool for exploring the covariance structure among observable variables <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>.</p>
<p>An FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g.&nbsp;in psychology where certain variables, like intelligence, cannot be directly measured <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>.</p>
<p>Suppose we have an observable random vector <span class="math inline">\(\boldsymbol{y}_i\in \mathbb{R}^p\)</span> for the <span class="math inline">\(i\)</span>-th subject with mean <span class="math inline">\(\mathbb{E}(\boldsymbol{y}_i)=\boldsymbol{\mu}\)</span> and variance <span class="math inline">\(\mathbb{V}(\boldsymbol{y}_i)=\boldsymbol{\Sigma}\)</span>. Then a <span class="math inline">\(k\)</span>-order factor analysis model for <span class="math inline">\(\boldsymbol{y}_i\)</span> can be given by <span class="math display">\[\begin{equation}
\boldsymbol{y}_i=\boldsymbol{\mu}+\boldsymbol{\Lambda} \boldsymbol{f}_i+\boldsymbol{\epsilon}_i,
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}\)</span>, <span class="math inline">\(\boldsymbol{f}_i \in \mathbb{R}^{k}\)</span> is and <span class="math inline">\(\boldsymbol{\epsilon}_i \in \mathbb{R}^{p}\)</span> are called the <em>loading matrix</em>, <em>common factors</em> and <em>unique (or specific) factors</em>, respectively. The order <span class="math inline">\(k\)</span> is usually much smaller than <span class="math inline">\(p\)</span>. For simplicity, we can assume that <span class="math inline">\(\boldsymbol{\mu} = \boldsymbol{0}_p\)</span>.</p>
<p>To make the model well-defined, we may assume</p>
<p><span class="math display">\[\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_k\\\boldsymbol{0}_p\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_k &amp; \mathbf{0}_{k\times p}\\\mathbf{0}_{p\times k} &amp; \mathbf{\Psi}\end{bmatrix},\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Psi}\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix where we denote the <span class="math inline">\(i\)</span>-th diagonal entry as <span class="math inline">\(\psi_{ii}\)</span>. Based on this assumption, the covariance of observable vector <span class="math inline">\(\boldsymbol{y}_i\)</span> can be modelled by</p>
<p><span class="math display">\[\mathbb{V}(\boldsymbol{y}_i|\boldsymbol{\Lambda},\boldsymbol{\Psi} )=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span></p>
<p>Alternatively, we can write the model as</p>
<p><span class="math display">\[\boldsymbol{y} = (\mathbf{I}_n \otimes \boldsymbol{\Lambda})\boldsymbol{f} + \boldsymbol{\epsilon}\]</span> where <span class="math inline">\(\boldsymbol{y} =
\begin{pmatrix}
\boldsymbol{y}_1 \\ \boldsymbol{y}_2 \\ \vdots \\ \boldsymbol{y}_n
\end{pmatrix}\)</span>, <span class="math inline">\(\boldsymbol{f} =
\begin{pmatrix}
\boldsymbol{f}_1 \\ \boldsymbol{f}_2 \\ \vdots \\  \boldsymbol{f}_n
\end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol{\epsilon} =
\begin{pmatrix}
\boldsymbol{\epsilon}_1 \\ \boldsymbol{\epsilon}_2 \\ \vdots \\  \boldsymbol{\epsilon}_n
\end{pmatrix}\)</span>. Thus we have</p>
<p><span class="math display">\[\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_{nk}\\\boldsymbol{0}_{np}\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_{nk} &amp; \mathbf{0}_{nk\times np}\\\mathbf{0}_{np\times nk} &amp; \mathbf{I}_n \otimes \mathbf{\Psi}\end{bmatrix}.\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Indeterminacy of the loading matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>One can easily see that if our factor analytic model is given by (1), then it can also be modelled as <span class="math display">\[\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}\]</span> where the matrix <span class="math inline">\(\boldsymbol{M}\)</span> is orthogonal and simultaneously the variance of <span class="math inline">\(\boldsymbol{y}\)</span> given by (2) still holds, since <span class="math display">\[\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span> Therefore a rotated loading matrix <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{M}\)</span> is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span> <span class="math display">\[\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}\]</span></p>
</div>
</div>
</section>
<section id="parameter-estimation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimation">Parameter Estimation</h2>
<p>We denote the set of parameters by <span class="math inline">\(\boldsymbol{\theta} = \{\text{vec}(\boldsymbol{\Lambda}), \text{diag}(\boldsymbol{\Psi})\}\)</span> where <span class="math inline">\(\text{vec}(\cdot)\)</span> is the vectorisation of the input matrix and <span class="math inline">\(\text{diag}(\cdot)\)</span> is the diagonal elements of the input matrix.</p>
<p>Traditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.</p>
<section id="maximum-likelihood-estimation" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> independent and identically distributed observations <span class="math inline">\(\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\)</span> from a <span class="math inline">\(p\)</span>-dimensional multi-variate normal distribution <span class="math inline">\(\mathcal{N}_p(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span>. Now denote <span class="math inline">\(f_{\boldsymbol{y}}(\boldsymbol{y};\boldsymbol{\theta})\)</span> as the probability density function of the random vector <span class="math inline">\(\boldsymbol{Y}\)</span> corresponding to <span class="math inline">\(\boldsymbol{y}\)</span>. Then the likelihood is given by</p>
<p><span id="eq-likelihood"><span class="math display">\[
L(\boldsymbol{\theta};\boldsymbol{y})=f_{\boldsymbol{y}}(\boldsymbol{y};\boldsymbol{\theta}) = \prod^n_{i=1}f_{\boldsymbol{y}_i}(\boldsymbol{y}_i;\boldsymbol{\theta}) =  \prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].
\tag{1}\]</span></span></p>
<p>The maximum likelihood estimate (MLE) of <span class="math inline">\(\boldsymbol{\theta}\)</span>, denoted as <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, is found by finding <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximizes <a href="#eq-likelihood" class="quarto-xref">Equation&nbsp;1</a>. However, it is often more convenient to maximize the log likelihood function. To be more computational friendly, a better form of log likelihood is given by</p>
<div id="lem-MLE" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> The log-likelihood is given by</p>
<p><span id="eq-loglikelihood"><span class="math display">\[\ell(\boldsymbol{\theta}) =  -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right] \tag{2}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{S}\)</span> is the sample covariance defined as <span class="math inline">\(\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\ell(\boldsymbol{\theta})=&amp; \sum^n_{i=1}\left[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\frac{1}{n}\sum_{i=1}^n(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right].
\end{align*}
\]</span> The last equality holds from the following fact: <span class="math display">\[
\begin{align*}
&amp;\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\\
\Leftrightarrow \ &amp; \boldsymbol{\Sigma}^{-1}\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\\
\Leftrightarrow \ &amp; \text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}) = \frac{1}{n} \sum^n_{i=1} \text{tr} (\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top)\\
\Leftrightarrow \ &amp; \text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}) = \frac{1}{n}\sum_{i=1}^n(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})
\end{align*}
\]</span> Q.E.D</p>
</div>
<p>The MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)</p>
<p><span class="math display">\[\begin{align}
\frac{\partial}{\partial \boldsymbol{\Lambda}}\ell(\boldsymbol{\theta})&amp;=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}\ell(\boldsymbol{\theta})&amp;= 0.
\end{align}\]</span></p>
<p>However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximization (EM) algorithm <span class="citation" data-cites="Rubin1982EMAlgorithms">[@Rubin1982EMAlgorithms]</span>, we will discuss this algorithm in the last part of background.</p>
</section>
</section>
<section id="rotation-techniques" class="level2">
<h2 class="anchored" data-anchor-id="rotation-techniques">Rotation Techniques</h2>
<p>After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>.</p>
<p>Suppose <span class="math inline">\(Q(\boldsymbol{\Lambda})\)</span> is an criterion for <span class="math inline">\(\boldsymbol{\Lambda}\)</span> in the rotation procedure, and we may express it as <span class="math inline">\(Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})\)</span> where <span class="math inline">\(P(\cdot)\)</span> is some loss function<span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. Specifically, if we set <span class="math inline">\(P(\cdot)=|\cdot|\)</span>, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as <span class="citation" data-cites="Jennrich2004Rotation">[@Jennrich2004Rotation]</span></p>
<p><span class="math display">\[\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda}_0\)</span> is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is</p>
<p><span class="math display">\[\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\)</span> is the maximum likelihood estimator.</p>
<section id="discussion-about-two-step-method" class="level3">
<h3 class="anchored" data-anchor-id="discussion-about-two-step-method">Discussion about two-step method</h3>
<p>The traditional two-step method faces significant shortcomings, primarily its unsuitability <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by <span class="math display">\[
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 &amp; 0 \\
0 &amp; 0.8 \\
0.6 &amp; 0\\
0 &amp; 0.7\\
0 &amp; 0.6\\
\end{bmatrix}.
\]</span> and the real <span class="math inline">\(\boldsymbol{\Psi}\)</span> is given by <span class="math display">\[
\boldsymbol{\Psi}=\text{diag}(0.1,0.2,0.2,0.1,0.1).
\]</span> We generate <span class="math inline">\(\boldsymbol{Y}\in \mathbb{R}^{{10000}\times 5}\)</span> from a multivariate normal distribution with mean <span class="math inline">\(\mathbb{0}_{5}\)</span> and covariance <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\)</span>. We utilize the R-function <code>factanal()</code> to produce the loading matrix via MLE, where this R-function will unitize varimax method to achieve rotation automatically. What we finally get is</p>
<p><span class="math display">\[
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
0 &amp; 0.997\\
0.872 &amp; 0\\
0 &amp; 0.748\\
0.915 &amp; 0\\
0.882 &amp; 0\\
\end{bmatrix},
\hat{\boldsymbol{\Psi}}=\text{diag}(0.005,0.240,0.440,0.163,0.223)
\]</span> This result gives the same sparsity result. However, the result of loading matrix in the first and last two dimensions are much larger than the real one, which implies overfitness.</p>
<p>Simulation code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># Number of observations</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True loading matrix</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>true_loading <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.8</span>,<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.8</span>,<span class="dv">0</span>,<span class="fl">0.7</span>,<span class="fl">0.6</span>),<span class="at">nrow=</span><span class="dv">5</span>,<span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># True psi matrix</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>true_psi <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">c</span>(<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.2</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples from multivariate normal distribution</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="at">n =</span> n, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Sigma =</span> true_loading <span class="sc">%*%</span> <span class="fu">t</span>(true_loading) <span class="sc">+</span> true_psi)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Traditional two step procedure</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># the function factanal has already used varimax method to get the optimal loading</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>fa_result <span class="ot">&lt;-</span> <span class="fu">factanal</span>(<span class="at">factors =</span> <span class="dv">2</span>, <span class="at">covmat =</span> <span class="fu">cor</span>(samples))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Result</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>fa_result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
factanal(factors = 2, covmat = cor(samples))

Uniquenesses:
[1] 0.005 0.240 0.440 0.163 0.222

Loadings:
     Factor1 Factor2
[1,]          0.997 
[2,]  0.872         
[3,]          0.748 
[4,]  0.915         
[5,]  0.882         

               Factor1 Factor2
SS loadings      2.375   1.555
Proportion Var   0.475   0.311
Cumulative Var   0.475   0.786

The degrees of freedom for the model is 1 and the fit was 2e-04 </code></pre>
</div>
</div>
<p>Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>. We have <span class="math display">\[\begin{align*}
  &amp;&amp;&amp;H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &amp;&amp;&amp;H_a: \text{ Otherwise}.
\end{align*}\]</span> The test statistics is given by <span class="math inline">\(\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})\)</span> which has an asymptotic <span class="math inline">\(\chi^2_s\)</span> distribution with <span class="math inline">\(s=\frac{1}{2}(p-k)^2-(p+k)\)</span> by the property of MLE, where <span class="math inline">\(F\)</span> is given by (<span class="citation" data-cites="mardiaMultivariateAnalysis1979">@mardiaMultivariateAnalysis1979</span>) <span class="math display">\[
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
\]</span> Typically, the test is conducted at a <span class="math inline">\(5\%\)</span> significant level. We can start the procedure from a very small <span class="math inline">\(k\)</span>, say, <span class="math inline">\(k=1\)</span> or <span class="math inline">\(k=2\)</span> and then increase <span class="math inline">\(k\)</span> until the null hypothesis is not rejected.</p>
</section>
</section>
<section id="penalized-likelihood-method" class="level2">
<h2 class="anchored" data-anchor-id="penalized-likelihood-method">Penalized Likelihood Method</h2>
<p>Penalized likelihood method can be viewed as a generalization of two-step method mentioned above <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. A penalized factor analytic model can be obtained by solving following optimization problem <span class="math display">\[
\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}\ell_p := \ell(\boldsymbol{\theta})-\rho\sum^p_{i=1}\sum^k_{j=1}P(\lambda_{ij}).
\]</span> where we call <span class="math inline">\(l_p\)</span> as the penalized likelihood, <span class="math inline">\(\rho\)</span> is called regularization parameter and we can treat <span class="math inline">\(P(\cdot)\)</span> as a penalized function. There are many types of penalized functions developed, such as LASSO (<span class="math inline">\(P(\cdot)=|\cdot|\)</span>) and MC+ (<span class="math inline">\(P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|&lt;\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)\)</span>) <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>，where <span class="math inline">\(\gamma\)</span> is ….. In this article, we will mainly focus on the LASSO penalty.</p>
<section id="lasso-penalty-and-lasso-estimator" class="level3">
<h3 class="anchored" data-anchor-id="lasso-penalty-and-lasso-estimator">LASSO penalty and LASSO estimator</h3>
<p>Again, recall that the LASSO penalized likelihood is given by <span class="math display">\[\ell_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\]</span> and the LASSO estimator, denoted as <span class="math inline">\((\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)\)</span>, can be obtained via</p>
<p><span class="math display">\[\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &amp;= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&amp;=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}\]</span></p>
<p>An E-M algorithm can be applied for evaluating the LASSO estimator.</p>
</section>
</section>
<section id="sec-EM" class="level2">
<h2 class="anchored" data-anchor-id="sec-EM">The EM Algorithm</h2>
<p>The Expectation-Maximization (EM) algorithm is a widely used iterative method to find the MLE, especially when the model depends on latent variables. The EM algorithm iteratively apply two distinct steps: the Expectation step (E-step) and the Maximization step (M-step). More specifically, we define</p>
<p><span class="math display">\[Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) = \mathbb{E}\left(\ell (\boldsymbol{\theta}; \boldsymbol{y})\bigg|\boldsymbol{y},\boldsymbol{\theta}^{(t)}\right).\]</span> Then for the <span class="math inline">\((t + 1)\)</span>-th interation, the steps involve:</p>
<ul>
<li><em>E-step</em>: Compute <span class="math inline">\(Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})\)</span>.</li>
<li><em>M-step</em>: Update <span class="math inline">\(\boldsymbol{\theta}^{(t+1)}\)</span> as <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximses <span class="math inline">\(Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})\)</span>.</li>
</ul>
<p>The M-step may be replaced with updating <span class="math inline">\(\boldsymbol{\theta}^{(t+1)}\)</span> such that <span class="math inline">\(Q(\boldsymbol{\theta}^{(t+1)}|\boldsymbol{\theta}^{(t)}) \geq Q(\boldsymbol{\theta}^{(t)}|\boldsymbol{\theta}^{(t)})\)</span>.</p>
<p>We iterate between E-step and M-step until convergence. The convergence may be determined by a criteria such as <span class="math inline">\(||\boldsymbol{\theta}^{(t+1)}-\boldsymbol{\theta}^{(t)}||_p\leq \epsilon\)</span> for some <span class="math inline">\(p\)</span>-norm <span class="math inline">\(||\cdot||_p\)</span> and <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<p>The EM algorithm approaches the problem of solving the likelihood equation indirectly by proceeding iteratively in terms of <span class="math inline">\(\ell(\boldsymbol{\theta};\boldsymbol{y})\)</span>. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given <span class="math inline">\(\boldsymbol{y}\)</span> and current fit for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<div id="lem-convergence" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2 (Convergence of the EM Algorithm)</strong></span> For all <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists <span class="math inline">\(t &gt; t_0\)</span> such that <span class="math inline">\(||\boldsymbol{\theta}^{(t)} - \hat{\boldsymbol{\theta}}|| &lt; \epsilon\)</span> where <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is the MLE.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the definition of conditional likelihood, our likelihood of complete data can be expressed by <span class="math display">\[
L_{\boldsymbol{X}}(\boldsymbol{\beta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})=L_{\boldsymbol{Y}}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
\]</span> and hence the log-likelihood is given by <span class="math display">\[
\log L_{\boldsymbol{X}}(\boldsymbol{\beta}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
\]</span> Take expectation to both sides of the equation with respect to <span class="math inline">\(\boldsymbol{x|y}\)</span> and replace <span class="math inline">\(\boldsymbol{\beta}\)</span> by <span class="math inline">\(\boldsymbol{\beta}^{(k)}\)</span>, we will have <span class="math display">\[
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
\]</span> Now consider the difference of log-likelihood of <span class="math inline">\(\boldsymbol{Y}\)</span> function between two iterations, we have <span class="math display">\[
\begin{align*}
  \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k+1)})-\log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k)}) =
  &amp;\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &amp;-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &amp;-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
\]</span> By the procedure of EM-algorithm, we always have <span class="math inline">\(Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\)</span>. By the Gibbs’s inequality, we have <span class="math inline">\(\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].\)</span> Therefore during iterations, the log-likelihood of observed data <span class="math inline">\(\boldsymbol{Y}\)</span> keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.</p>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>