<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>background</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="background_files/libs/clipboard/clipboard.min.js"></script>
<script src="background_files/libs/quarto-html/quarto.js"></script>
<script src="background_files/libs/quarto-html/popper.min.js"></script>
<script src="background_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="background_files/libs/quarto-html/anchor.min.js"></script>
<link href="background_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="background_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="background_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="background_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="background_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="sec-bg" class="level1">
<h1>Background</h1>
<section id="factor-analytic-model" class="level2">
<h2 class="anchored" data-anchor-id="factor-analytic-model">Factor analytic model</h2>
<p>Factor analysis (FA) is a statistical method which attempts to use fewer underlying factors to explain the correlation between a large set of observed variables <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>. FA provides a useful tool for exploring the covariance structure among observable variables <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>.</p>
<p>An FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g.&nbsp;in psychology where certain variables, like intelligence, cannot be directly measured <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>.</p>
<p>Suppose we have an observable random vector <span class="math inline">\(\boldsymbol{y}\in \mathbb{R}^p\)</span> with mean <span class="math inline">\(\mathbb{E}(\boldsymbol{y})=\boldsymbol{\mu}\)</span> and variance <span class="math inline">\(\mathbb{V}(\boldsymbol{y})=\boldsymbol{\Sigma}\)</span>. Then a <span class="math inline">\(k\)</span>-order factor analysis model for <span class="math inline">\(\boldsymbol{y}\)</span> can be given by <span class="math display">\[\begin{equation}
\boldsymbol{y}=\boldsymbol{\Lambda} \boldsymbol{f}+\boldsymbol{\mu}+\boldsymbol{\epsilon},
\end{equation}\]</span> where <span class="math inline">\(\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}\)</span> <span class="math inline">\(\boldsymbol{f} \in \mathbb{R}^{k}\)</span> is and <span class="math inline">\(\boldsymbol{\epsilon} \in \mathbb{R}^{p}\)</span> is are called the <em>loading matrix</em>, <em>common factors</em> and <em>unique factors</em>, respectively.</p>
<p>To make the model well-defined, we may assume</p>
<p><span class="math display">\[\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_k\\\boldsymbol{0}_p\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_k &amp; \mathbf{0}_{k\times p}\\\mathbf{0}_{p\times k} &amp; \mathbf{\Psi}\end{bmatrix},\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Psi}\)</span> is a diagonal matrix where we denote the <span class="math inline">\(i\)</span>-th diagonal entry as <span class="math inline">\(\Psi_{ii}\)</span>. Based on this assumption, the covariance of observable vector <span class="math inline">\(\boldsymbol{y}\)</span> can be modelled by</p>
<p><span class="math display">\[\mathbb{V}(\boldsymbol{y}|\boldsymbol{\Lambda},\boldsymbol{\Psi} )=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Indeterminacy of the loading matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>One can easily see that if our factor analytic model is given by (1), then it can also be modelled as <span class="math display">\[\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}\]</span> where the matrix <span class="math inline">\(\boldsymbol{M}\)</span> is orthogonal and simultaneously the variance of <span class="math inline">\(\boldsymbol{y}\)</span> given by (2) still holds, since <span class="math display">\[\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span> Therefore a rotated loading matrix <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{M}\)</span> is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span> <span class="math display">\[\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}\]</span></p>
</div>
</div>
</section>
<section id="parameter-estimation" class="level2">
<h2 class="anchored" data-anchor-id="parameter-estimation">Parameter Estimation</h2>
<p>We denote the set of parameters by <span class="math inline">\(\boldsymbol{\theta} = \{\text{vec}(\boldsymbol{\Lambda}),\text{vec}(\boldsymbol{\Psi})\}\)</span> where <span class="math inline">\(\text{vec}(\cdot)\)</span> is the vectorisation of the input.</p>
<p>Traditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.</p>
<section id="maximum-likelihood-estimation" class="level3">
<h3 class="anchored" data-anchor-id="maximum-likelihood-estimation">Maximum Likelihood Estimation</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> independent and identically distributed observations <span class="math inline">\(\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_N\)</span> from a p-dimensional multi-variate normal distribution <span class="math inline">\(N_p(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> and by our hypothesis, we have <span class="math inline">\(\boldsymbol{\Sigma}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\)</span>. Then the likelihood function is given by</p>
<p><span id="eq-likelihood"><span class="math display">\[
L(\boldsymbol{\theta})=\prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].
\tag{1}\]</span></span></p>
<p>The maximum likelihood estimate (MLE) of <span class="math inline">\(\boldsymbol{\theta}\)</span>, denoted as <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, is found by finding <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximises <a href="#eq-likelihood" class="quarto-xref">Equation&nbsp;1</a>. However, it is often more convenient to maximise the log likelihood function.</p>
<div id="lem-MLE" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> The log-likelihood is given by</p>
<p><span id="eq-loglikelihood"><span class="math display">\[\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi}) =  -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right] \tag{2}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{S}\)</span> is the sample covariance defined as <span class="math inline">\(\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&amp; \sum^n_{i=1}\left[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right],
\end{align*}\]</span></p>
<p>[TODO: show step from line 2 to 3].</p>
</div>
<p>The MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)</p>
<p><span class="math display">\[\begin{align}
\frac{\partial}{\partial \boldsymbol{\Lambda}}\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})&amp;=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}\ell(\boldsymbol{\Lambda},\boldsymbol{\Psi})&amp;= 0.
\end{align}\]</span></p>
<p>However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximisation (EM) algorithm <span class="citation" data-cites="Rubin1982EMAlgorithms">[@Rubin1982EMAlgorithms]</span>.</p>
</section>
</section>
<section id="sec-EM" class="level2">
<h2 class="anchored" data-anchor-id="sec-EM">The EM Algorithm</h2>
<p>The Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data <span class="citation" data-cites="Ng2012EMAlgorithm">[@Ng2012EMAlgorithm]</span>. As we mentioned, the key of maximum likelihood estimation is solving equation <span class="math display">\[
\frac{\partial}{\partial \beta}l=0.
\]</span> However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, <em>augmented data</em>(or complete) refers to the integration of this unobserved component, enhancing the algorithm’s ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete data sets effectively.</p>
<section id="the-e-step-and-m-step" class="level3">
<h3 class="anchored" data-anchor-id="the-e-step-and-m-step">The E-step and M-step</h3>
<p>Let <span class="math inline">\(\boldsymbol{x}\)</span> denote the vector containing complete data, <span class="math inline">\(\boldsymbol{y}\)</span> denote the observed incomplete data and <span class="math inline">\(\boldsymbol{z}\)</span> denote the vector containing the missing data. Here ‘missing data’ is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later) <span class="citation" data-cites="Ng2012EMAlgorithm">[@Ng2012EMAlgorithm]</span>. Also we assume <span class="math inline">\(\boldsymbol{\beta}\)</span> as the parameter we want to estimate over the parameter space <span class="math inline">\(\boldsymbol{\Omega}\)</span>.</p>
<p>Now denote <span class="math inline">\(f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})\)</span> as the probability density function (p.d.f.) of the random vector <span class="math inline">\(\boldsymbol{X}\)</span> corresponding to <span class="math inline">\(\boldsymbol{x}\)</span>. Then the complete-data log-likelihood function when complete data is fully observed can be given by</p>
<p><span class="math display">\[\log L_{\boldsymbol{X}}(\boldsymbol{\beta})=\log f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta}).\]</span></p>
<p>The EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of <span class="math inline">\(\log L_{\boldsymbol{X}}(\boldsymbol{\beta})\)</span>. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given <span class="math inline">\(\boldsymbol{y}\)</span> and current fit for <span class="math inline">\(\boldsymbol{\beta}\)</span>.</p>
<p>On the <span class="math inline">\((k+1)^th\)</span> iteration, we have <span class="math display">\[
\begin{align*}
&amp;\text{E-step: Compute } Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}):=\mathbb{E}_{\boldsymbol{X}}[\log L_{\boldsymbol{X}}(\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
&amp;\text{M-step: Update }\boldsymbol{\beta}^{(k+1)} \text{ as }\boldsymbol{{\beta}}^{(k+1)}:=\text{arg}\max_{\boldsymbol{\beta}} Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) \text{ (Original EM)}\\
&amp; \text{ Or update }\boldsymbol{{\beta}}^{(k+1)} \text{ such that } Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\text{ (Generalized EM)}.
\end{align*}
\]</span> We keep iterating between E-step and M-step until convergence, which may be determined by a criteria such as <span class="math inline">\(||\boldsymbol{\beta}^{(k+1)}-\boldsymbol{\beta}^{(k)}||_p\leq \epsilon\)</span> for some p-norm <span class="math inline">\(||\cdot||_p\)</span> and positive <span class="math inline">\(\epsilon\)</span>.</p>
<p>Meanwhile, the M-step in both original and generalized algorithms defines a mapping from the parameter space <span class="math inline">\(\boldsymbol{\Omega}\)</span> to itself by <span class="math display">\[
\begin{align*}
M: \boldsymbol{\Omega} &amp;\to \boldsymbol{\Omega}\\
   \boldsymbol{\beta}^{(k)} &amp;\mapsto \boldsymbol{\beta}^{(k+1)}.
\end{align*}
\]</span></p>
</section>
<section id="convergence-of-the-em-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="convergence-of-the-em-algorithm">Convergence of the EM Algorithm</h3>
<p>By the definition of conditional likelihood, our likelihood of complete data can be expressed by <span class="math display">\[
L_{\boldsymbol{X}}(\boldsymbol{\beta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})=L_{\boldsymbol{Y}}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
\]</span> and hence the log-likelihood is given by <span class="math display">\[
\log L_{\boldsymbol{X}}(\boldsymbol{\beta}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
\]</span> Take expectation to both sides of the equation with respect to <span class="math inline">\(\boldsymbol{x|y}\)</span> and replace <span class="math inline">\(\boldsymbol{\beta}\)</span> by <span class="math inline">\(\boldsymbol{\beta}^{(k)}\)</span>, we will have <span class="math display">\[
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
\]</span> Now consider the difference of log-likelihood of <span class="math inline">\(\boldsymbol{Y}\)</span> function between two iterations, we have <span class="math display">\[
\begin{align*}
  \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k+1)})-\log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k)}) =
  &amp;\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &amp;-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &amp;-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
\]</span> By the procedure of EM-algorithm, we always have <span class="math inline">\(Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\)</span>. By the Jensen’s inequality, we have <span class="math inline">\(\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].\)</span> Therefore during iterations, the log-likelihood of observed data <span class="math inline">\(\boldsymbol{Y}\)</span> keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.</p>
</section>
<section id="rotation-techniques" class="level3">
<h3 class="anchored" data-anchor-id="rotation-techniques">Rotation techniques</h3>
<p>After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>.</p>
<p>Suppose <span class="math inline">\(Q(\boldsymbol{\Lambda})\)</span> is an criterion for <span class="math inline">\(\boldsymbol{\Lambda}\)</span> in the rotation procedure, and we may express it as <span class="math inline">\(Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})\)</span> where <span class="math inline">\(P(\cdot)\)</span> is some loss function<span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. Specifically, if we set <span class="math inline">\(P(\cdot)=|\cdot|\)</span>, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as <span class="citation" data-cites="Jennrich2004Rotation">[@Jennrich2004Rotation]</span></p>
<p><span class="math display">\[\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda}_0\)</span> is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is</p>
<p><span class="math display">\[\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\)</span> is the maximum likelihood estimator.</p>
</section>
<section id="discussion-about-two-step-method" class="level3">
<h3 class="anchored" data-anchor-id="discussion-about-two-step-method">Discussion about two-step method</h3>
<p>The traditional two-step method faces significant shortcomings, primarily its unsuitability <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by <span class="math display">\[
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 &amp; 0 \\
0 &amp; 0.8 \\
0 &amp; 0\\
0 &amp; 0\\
0 &amp; 0\\
\end{bmatrix}.
\]</span> We generate <span class="math inline">\(\boldsymbol{Y}\in \mathbb{R}^{{100}\times 5}\)</span> using common factor <span class="math inline">\(\boldsymbol{f}\in \mathbb{R}^{{100}\times 2}\)</span> where each factor is generated randomly from standard normal distribution <span class="math inline">\(N(0,1)\)</span>. We utilize the R-function <code>factanal()</code> to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is</p>
<p><span class="math display">\[
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
-0.158 &amp; 0.985\\
0 &amp; 0\\
0.997 &amp; 0\\
0.207 &amp; 0\\
0 &amp; 0\\
\end{bmatrix},
\]</span> which is neither precise nor sparse.</p>
<p>Simulation code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># Number of observations</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span>   <span class="co"># Number of features</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>factor1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># set factors</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>factor2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>p)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.8</span><span class="sc">*</span>factor1 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Strong loading</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fl">0.8</span><span class="sc">*</span>factor2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Strong loading</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Noise, no strong loading on any factor</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fa_result <span class="ot">&lt;-</span> <span class="fu">factanal</span>(<span class="at">factors =</span> <span class="dv">2</span>, <span class="at">covmat =</span> <span class="fu">cor</span>(X))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>rotated_fa <span class="ot">&lt;-</span> <span class="fu">varimax</span>(fa_result<span class="sc">$</span>loadings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model <span class="citation" data-cites="mardiaMultivariateAnalysis1979">[@mardiaMultivariateAnalysis1979]</span>. We have <span class="math display">\[\begin{align*}
  &amp;&amp;&amp;H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &amp;&amp;&amp;H_a: \text{ Otherwise}.
\end{align*}\]</span> The test statistics is given by <span class="math inline">\(\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})\)</span> which has an asymptotic <span class="math inline">\(\chi^2_s\)</span> distribution with <span class="math inline">\(s=\frac{1}{2}(p-k)^2-(p+k)\)</span> by the property of MLE, where <span class="math inline">\(F\)</span> is given by (<span class="citation" data-cites="mardiaMultivariateAnalysis1979">@mardiaMultivariateAnalysis1979</span>) <span class="math display">\[
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
\]</span> Typically, the test is conducted at a <span class="math inline">\(5\%\)</span> significant level. We can start the procedure from a very small <span class="math inline">\(k\)</span>, say, <span class="math inline">\(k=1\)</span> or <span class="math inline">\(k=2\)</span> and then increase <span class="math inline">\(k\)</span> until the null hypothesis is not rejected.</p>
</section>
<section id="penalized-likelihood-method" class="level3">
<h3 class="anchored" data-anchor-id="penalized-likelihood-method">Penalized Likelihood Method</h3>
<p>Penalized likelihood method can be viewed as a generalization of two-step method mentioned above <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. A penalized factor analytic model can be obtained by solving following optimization problem <span class="math display">\[
\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}l_p := l(\boldsymbol{\Lambda},\boldsymbol{\Psi})-\rho\sum^p_{i=1}\sum^k_{j=1}P(|\boldsymbol{\Lambda}_{ij}|).
\]</span> where we call <span class="math inline">\(l_p\)</span> as the penalized likelihood, <span class="math inline">\(\rho\)</span> is called regularization parameter and we can treat <span class="math inline">\(P(\cdot)\)</span> as a penalized function. There are many types of penalized functions developed, such as LASSO (<span class="math inline">\(P(\cdot)=|\cdot|\)</span>) and MC+ (<span class="math inline">\(P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|&lt;\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)\)</span>) <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">[@hiroseSparseEstimationNonconcave2015]</span>. In this article, we will mainly focus on the LASSO penalty.</p>
</section>
<section id="lasso-penalty-and-lasso-estimator" class="level3">
<h3 class="anchored" data-anchor-id="lasso-penalty-and-lasso-estimator">LASSO penalty and LASSO estimator</h3>
<p>Again, recall that the LASSO penalized likelihood is given by <span class="math display">\[l_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\]</span> and the LASSO estimator, denoted as <span class="math inline">\((\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)\)</span>, can be obtained via</p>
<p><span class="math display">\[\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &amp;= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&amp;=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}\]</span></p>
<p>An E-M algorithm can be applied for evaluating the LASSO estimator.</p>
</section>
</section>
<section id="sec-penEM" class="level2">
<h2 class="anchored" data-anchor-id="sec-penEM">The EM Algorithm in LASSO Factor Analytic Models</h2>
<section id="model-setup" class="level3">
<h3 class="anchored" data-anchor-id="model-setup">Model Setup</h3>
<p>Suppose we have the <span class="math inline">\(n\)</span> centralized observations, <span class="math inline">\(\{\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\}\)</span>, where <span class="math inline">\(\boldsymbol{y}_j\in \mathbb{R}^p\)</span> and hence the mean, <span class="math inline">\(\boldsymbol{\mu}_j=\boldsymbol{0}_p\)</span> (<span class="math inline">\(j=1,2,\dots,n\)</span>). For each of the observation <span class="math inline">\(\boldsymbol{y}_j\)</span>, the common factor and unique factor are <span class="math inline">\(\boldsymbol{f}_j\)</span> and <span class="math inline">\(\boldsymbol{\epsilon}_j\)</span> respectively. Denote the response matrix as <span class="math inline">\(\boldsymbol{Y}=[\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n]^\top\)</span>, the common factor matrix as <span class="math inline">\(\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_n]^\top\)</span>, and the unique factor matrix as <span class="math inline">\(\boldsymbol{\hat\epsilon}=[\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2,\dots,\boldsymbol{\epsilon}_n]^\top\)</span>. Then the model can be written as <span class="math display">\[\boldsymbol{Y}=\boldsymbol{F}\boldsymbol{\Lambda}^\top+\boldsymbol{\hat\epsilon}.\]</span><br>
We also assume</p>
<ol type="1">
<li><p><span class="math inline">\(\boldsymbol{\epsilon}_j\)</span> follows a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\boldsymbol{\Psi}\)</span> for <span class="math inline">\(j=1,2,\dots,n\)</span>, where <span class="math inline">\(\boldsymbol{\Psi}\)</span> is a diagonal matrix defined by <span class="math inline">\(\boldsymbol{\Psi}=diag(\psi_{11},\psi_{22},\dots,\psi_{pp})\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{f}_j\)</span> follows a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\boldsymbol{I}_k\)</span> for <span class="math inline">\(j=1,2,\dots,n\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{y}_j\)</span> follows a normal distribution with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\)</span> for <span class="math inline">\(j=1,2,\dots,n\)</span>. Those <span class="math inline">\(\boldsymbol{y}_j\)</span> are pairwisely independent.</p></li>
</ol>
</section>
<section id="the-em-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-em-algorithm">The EM Algorithm</h3>
<p>We treat the common factor matrix <span class="math inline">\(\boldsymbol{F}\)</span> as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of <span class="math inline">\((\boldsymbol{Y},\boldsymbol{F})\)</span> given <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\((\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})\)</span>, where <span class="math inline">\((\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})\)</span> is the parameter we got in the <span class="math inline">\(k\)</span>-th iteration (<span class="math inline">\(k&gt;1\)</span>) and <span class="math inline">\((\boldsymbol{\Lambda}_{(0)},\boldsymbol{\Psi}_{(0)})\)</span> is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get <span class="math inline">\((\boldsymbol{\Lambda}_{(k+1)},\boldsymbol{\Psi}_{(k+1)})\)</span>.</p>
</section>
<section id="e-step" class="level3">
<h3 class="anchored" data-anchor-id="e-step">E-Step</h3>
<p>First, the joint likelihood of <span class="math inline">\((\boldsymbol{Y},\boldsymbol{F})\)</span>, denoted as <span class="math inline">\(L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\)</span>, is given by</p>
<p><span class="math display">\[
\begin{align*}
L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&amp;\prod_{i=1}^nf(\boldsymbol{y}_i,\boldsymbol{f}_i)\\
=&amp;\prod_{i=1}^nf(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\\
=&amp;\prod_{i=1}^nN(\boldsymbol{y}_i;\boldsymbol{\Lambda}\boldsymbol{f}_i,\boldsymbol{\Psi})N(\boldsymbol{f}_i;\boldsymbol{0}_k,\boldsymbol{I}_k)\\
=&amp;\prod_{i=1}^n[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Psi})^{-\frac{1}{2}}\exp\{-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}\\
&amp;(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\}][(2\pi)^{-\frac{k}{2}}\det({\boldsymbol{I}_k})^{-\frac{1}{2}}\exp\{-\frac{1}{2}\boldsymbol{f}_i^\top\boldsymbol{I}_k^{-1}\boldsymbol{f}_i\}]\\
= &amp;(2\pi)^{-\frac{n}{2}(p+k)}(\prod_{i=1}^p\psi_{jj})^{-\frac{n}{2}}\exp\{-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}\}\\
&amp;\exp\{-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\}.
\end{align*}
\]</span></p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>