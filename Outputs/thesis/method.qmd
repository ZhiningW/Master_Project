# Method {#sec-method}
As described in @sec-bg, there are various methods to fit a factor analytic model to multivariate data. We will discuss the standard EM-algorithm and the penalized EM-algorithm in this chapter. We treat the common factor matrix $\boldsymbol{f}$ as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of $(\boldsymbol{y},\boldsymbol{f})$ given $\boldsymbol{y}$ and $\boldsymbol{\theta}_{(t)}$, where $\boldsymbol{\theta}_{(t)}$ is the parameter we got in the $t$-th iteration ($t>1$) and $\boldsymbol{\theta}_{(0)}$ is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get $\boldsymbol{\theta}_{(t + 1)}$. The outline is as follow: in subsection

## E-Step

::: {#lem-fa-likelihood}
The joint log-likelihood of $(\boldsymbol{y},\boldsymbol{f})$ is given by
$$\ell_{\boldsymbol{y},\boldsymbol{f}}(\boldsymbol{\theta})= -\frac{n}{2} \sum_{j=1}^p\log\psi_{jj}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p \frac{(y_{ij} - \sum_{k=1}^m \lambda_{jk}f_{ik})^2}{\psi_{jj}} - \sum_{i=1}^n\sum_{j=1}^pf_{ij}^2 + \text{constant}.\\$$
:::

::: proof

First, the joint log-likelihood of $(\boldsymbol{y},\boldsymbol{f})$ is given by
$$
\ell_{\boldsymbol{y},\boldsymbol{f}}(\boldsymbol{\theta})=\sum_{i=1}^n\log f(\boldsymbol{y}_i,\boldsymbol{f}_i) =\sum_{i=1}^n\log \left(f(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\right)
$$
where $\boldsymbol{y}_i|\boldsymbol{f}_i \sim \mathcal{N}(\boldsymbol{\Lambda}\boldsymbol{f}_i, \boldsymbol{\Psi})$ and $\boldsymbol{f}_i \sim \mathcal{N}(\boldsymbol{0}_m,\boldsymbol{I}_m)$. Therefore we have

$$
\begin{align*}
\ell(\boldsymbol{\theta})=& \sum_{i=1}^n\log f(\boldsymbol{y}_i|\boldsymbol{f}_i) + \sum_{i=1}^n\log f(\boldsymbol{f}_i)\\
=& -\frac{np}{2}\log(2\pi)-\frac{n}{2}\log\left[\det(\boldsymbol{\Psi})\right]-\frac{1}{2}\sum_{i=1}^n(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\\
&\quad -\frac{kp}{2}\log(2\pi)-\frac{1}{2}\log\left[\det(\boldsymbol{I}_m)\right]-\frac{1}{2}(\boldsymbol{f}_i-\boldsymbol{0}_k)^\top\boldsymbol{I}_m^{-1}(\boldsymbol{f}_i-\boldsymbol{0}_m)\\
=& -\frac{(n+k)p}{2}\log(2\pi)-\frac{n}{2}\sum_{j=1}^p\log \psi_{jj}-\frac{1}{2}\sum_{i=1}^n(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)- \frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\\
=& -\frac{(n+k)p}{2}\log(2\pi)-\frac{n}{2} \sum_{j=1}^p\log\psi_{jj}-\frac{1}{2}\sum_{i=1}^n\sum_{j}^p \frac{y_{ij}^2}{\psi_{jj}} + \frac{1}{2}\sum_{i=1}^n \boldsymbol{f}_i^\top\mathbf{\Lambda}^\top\mathbf{\Psi}^{-1}\boldsymbol{y}_i+ \frac{1}{2}\sum_{i=1}^n\boldsymbol{y}_i^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i  - \frac{1}{2}\sum_{i=1}^n
\boldsymbol{f}_i^\top\mathbf{\Lambda}^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i -\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\\
=& -\frac{(n+k)p}{2}\log(2\pi)-\frac{n}{2} \sum_{j=1}^p\log\psi_{jj}-\frac{1}{2}\sum_{i=1}^n\sum_{j}^p \frac{y_{ij}^2}{\psi_{jj}} +   \sum_{i=1}^n \boldsymbol{y}_i^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i  - \frac{1}{2}\sum_{i=1}^n
\boldsymbol{f}_i^\top\mathbf{\Lambda}^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i -\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\\

=& -\frac{(n+k)p}{2}\log(2\pi)-\frac{n}{2} \sum_{j=1}^p\log\psi_{jj}-\frac{1}{2}\sum_{i=1}^n\sum_{j}^p \frac{y_{ij}^2}{\psi_{jj}} +   \sum_{i=1}^n \boldsymbol{y}_i^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i  - \frac{1}{2}\sum_{i=1}^n\text{tr}\left(
\mathbf{\Lambda}^\top\mathbf{\Psi}^{-1}\mathbf{\Lambda}\boldsymbol{f}_i\boldsymbol{f}_i^\top\right) -\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\\
\end{align*}
$$

:::

Now let us deduce the conditional expectation to $\boldsymbol{f}$ given $\boldsymbol{y},\boldsymbol{\theta}_{(t)}$, denoted as $\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})\right)$. In @eq-loglikelihood, the first term is independent of $\boldsymbol{f}$, hence stay the same under conditional expectation. The last term is independent of $\boldsymbol{\theta}$, therefore we can regard it as a constant in $\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})\right)$. 

Without ambiguity, denote $\mathbb{E}[f_{ik}|_{(t)}]$ to be the conditional expectation $\mathbb{E}[f_{ik}|\boldsymbol{y},\boldsymbol{\theta}_{(t)}]$ for simplification. Then the conditional expectation $\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})\right)$ is given by

$$
\begin{align*}
\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})|_{(t)}\right)=& \ \text{constant}-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}-\\
&\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p \frac{\mathbb{E}\left[(y_{ij} - \sum_{k=1}^m \lambda_{jk}f_{ik})^2|_{(t)}\right]}{\psi_{jj}} - \frac{1}{2}\sum_{i=1}^n\mathbb{E}(\boldsymbol{f}_i^\top\boldsymbol{f}_i|_{(t)})\\
\end{align*}
$$

To deal with $\mathbb{E}[\boldsymbol{f}_i|_{(t)}]$ and $\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(t)}]$, we only need to know the mean and variance of conditional distribution $\boldsymbol{f}_i|\boldsymbol{Y},\boldsymbol{\Lambda}^\top_{(t)},\boldsymbol{\Psi}_{(t)}$, or equivalently $\boldsymbol{f}_i|\boldsymbol{y}_i,\boldsymbol{\Lambda}^\top_{(t)},\boldsymbol{\Psi}_{(t)}$ because of the independency of $\boldsymbol{f}_i$ to $\boldsymbol{y}_j$ for $i\neq j$. This is because we can always treat $\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(t)}]$ as

$$
\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(t)}]= \mathbb{V}[\boldsymbol{f}_i|_{(t)}]+ \mathbb{E}[\boldsymbol{f}_i|_{(t)}]\mathbb{E}[\boldsymbol{f}_i|_{(t)}]^\top.
$$ 
where $\mathbb{V}[\boldsymbol{f}_i|_{(t)}]$ is the variance of conditional distribution. To deal with this, wed need the following lemma.



In our scenario, using @lem-cond, we have 

$$
\begin{align*}
& \boldsymbol{\mu}_{\boldsymbol{f}_i}= \boldsymbol{0}_k, \boldsymbol{\Sigma}_{\boldsymbol{f}_{i}}=\boldsymbol{I}_k\\
& \boldsymbol{\mu}_{\boldsymbol{y}_i}= \boldsymbol{0}_p, \boldsymbol{\Sigma}_{\boldsymbol{y}_i}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}
\end{align*} 
$$

and also 

$$
\text{Cov}(\boldsymbol{y}_i,\boldsymbol{f}_i)=\text{Cov}(\boldsymbol{\Lambda f}_i+\boldsymbol{\epsilon}_i,\boldsymbol{f}_i)=\boldsymbol{\Lambda}^\top.
$$ 

Therefore we have 

$$
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(t)})=\boldsymbol{\mu}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{\Lambda}_{(t)}^\top(\boldsymbol{\Lambda}_{(t)}\boldsymbol{\Lambda}_{(t)}^\top+\boldsymbol{\Psi}_{(t)})^{-1}\boldsymbol{y}_i\\
&\mathbb{V}(\boldsymbol{f}_i|_{(t)})=\boldsymbol{\Sigma}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(t)}^\top(\boldsymbol{\Lambda}_{(t)}\boldsymbol{\Lambda}_{(t)}^\top+\boldsymbol{\Psi}_{(t)})^{-1}\boldsymbol{\Lambda}_{(t)}.\\
\end{align*} 
$$

For simplification, let us denote 

$$
\begin{align*}
&\boldsymbol{A}:=\boldsymbol{\Lambda}_{(t)}^\top(\boldsymbol{\Lambda}_{(t)}\boldsymbol{\Lambda}_{(t)}^\top+\boldsymbol{\Psi}_{(t)})^{-1}\\
&\boldsymbol{B}:=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(t)}^\top(\boldsymbol{\Lambda}_{(t)}\boldsymbol{\Lambda}_{(t)}^\top+\boldsymbol{\Psi}_{(t)})^{-1}\boldsymbol{\Lambda}_{(t)},\\
\end{align*} 
$$

we will get 

$$
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(t)})= \boldsymbol{A}\boldsymbol{y}_i\\
&\mathbb{E}(\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(t)})= \boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top.
\end{align*}
$$

Our expectation will finally be confirmed by 

$$
\begin{align*}
\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})|_{(t)}\right)= & -\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2-2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj}}+\text{constant}\\
=& -\frac{n}{2}\log(\det(\boldsymbol{\Psi}))-\frac{1}{2}\text{tr}(\boldsymbol{Y}\boldsymbol{\Psi}^{-1}\boldsymbol{Y}^\top)+\text{tr}(\boldsymbol{\Lambda}\boldsymbol{A}\boldsymbol{Y}^\top\boldsymbol{Y}\boldsymbol{\Psi}^{-1})-\frac{1}{2}\text{tr}(\boldsymbol{\Lambda}(\boldsymbol{n\boldsymbol{B}}+\boldsymbol{A}\boldsymbol{Y}^\top\boldsymbol{Y}\boldsymbol{A}^\top)\boldsymbol{\Lambda}^\top\boldsymbol{\Psi}^{-1})
\end{align*}
$$ {#eq-expectation}

## M-step: Standard EM-algorithm

In M-step, we need to maximize so called $Q$-function with respect to parameters. In the standard scenario, the $Q$-function is directly the expectation given by @eq-expectation, i.e.
$$
Q(\boldsymbol{\Lambda},\boldsymbol{\Psi}) = \mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})|_{(t)}\right)
$$
When execute M-step, we use the following strategy[@Ng2012EMAlgorithm]:

1. Find $\boldsymbol{\Psi}_{(t+1)}$ using current $\boldsymbol{\Lambda}_{(t)}$, i.e.
$$
  \boldsymbol{\Psi}_{(t+1)} = \arg \max_{\boldsymbol{\Psi}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(t)}).
$$
2. Find $\boldsymbol{\Lambda}_{(k+1)}$ using $\boldsymbol{\Psi}_{(k+1)}$ we got in previous step, i.e. 
$$
  \boldsymbol{\Lambda}_{(t+1)} = \arg \max_{\boldsymbol{\Lambda}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(t+1)}).
$$

For step 1, take partial derivative with respect to each $\psi_{jj}$ and let it equal to zero to find the local maximizer of $\psi_{jj}$, i.e. 

$$
\frac{\partial}{\partial \psi_{jj}}Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(t)})=0.
$$

By simple calculation, we will have 

$$
\begin{align*}
\frac{\partial}{\partial \psi_{jj}}Q((\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(t)})=
&-\frac{n}{2}\frac{1}{\psi_{jj}}+\frac{1}{2}\sum_{i=1}^n
\frac{y_{ij}^2-2y_{ij}\boldsymbol{\Lambda}_{(t)}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj}^2} \\
&+ \frac{1}{2}\sum_{i=1}^n\frac{\boldsymbol{\Lambda}_{(t)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(t)}(j,)^\top}{\psi_{jj}^2}.\\
\end{align*}
$$

Thus the update of $\boldsymbol{\Psi}$ will be elementwisely given by 

$$
\begin{align*}
\psi_{jj,(t+1)} &= \frac{1}{n} \sum_{i=1}^n y_{ij}^2 - \frac{2}{n} \sum_{i=1}^n y_{ij} \boldsymbol{\Lambda}_{(t)}(j,) \boldsymbol{A} \boldsymbol{y}_i + \frac{1}{n}\sum_{i=1}^n \boldsymbol{\Lambda}_{(t)}(j,) (\boldsymbol{B} + \boldsymbol{A} \boldsymbol{y}_i \boldsymbol{y}_i^\top \boldsymbol{A}^\top) \boldsymbol{\Lambda}_{(t)}(j,)^\top \\
&= \frac{1}{n} \boldsymbol{Y}[,j]^\top \boldsymbol{Y}[,j] - \frac{2}{n} \boldsymbol{\Lambda}_{(t)}[j,] \boldsymbol{A} \boldsymbol{Y}^\top \boldsymbol{Y}[\cdot,j] + \frac{1}{n} \boldsymbol{\Lambda}_{(t)}[j,] (n\boldsymbol{B} + \boldsymbol{A} \boldsymbol{Y}^\top \boldsymbol{Y}\boldsymbol{A}^\top) \boldsymbol{\Lambda}_{(t)}[j,]^\top
\end{align*}.
$$ {#eq-psiupdate}

But notice that the $Q$-function is not concave globally, therefore we may update $\boldsymbol{\Psi}$ selectively. More specifically, we only update $\psi_{jj}$ when it satisfies

$$
\frac{\partial^2}{\partial\psi_{jj}^2}Q((\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(t)})\leq 0,
$$

i.e.

$$
\begin{align*}
\psi_{jj} &\leq \frac{2}{n}[\sum_{i=1}^n(y_{ij}^2-2y_{ij}\boldsymbol{\Lambda}_{(t)}(j,)\boldsymbol{A}\boldsymbol{y}_i)+\sum_{i=1}^n\boldsymbol{\Lambda}_{(t)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(t)}(j,)^\top]\\
&= \frac{2}{n}( \boldsymbol{Y}[,j]^\top \boldsymbol{Y}[,j] -  \boldsymbol{\Lambda}_{(t)}[j,] \boldsymbol{A} \boldsymbol{Y}^\top \boldsymbol{Y}[\cdot,j] + \boldsymbol{\Lambda}_{(t)}[j,] (n\boldsymbol{B} + \boldsymbol{A} \boldsymbol{Y}^\top \boldsymbol{Y}\boldsymbol{A}^\top) \boldsymbol{\Lambda}_{(t)}[j,]^\top)
\end{align*}
$$ {#eq-psiselect}

For step 2, let us revise the update formula, we have 

$$
\begin{align*}
\boldsymbol{\Lambda}_{(t+1)}=&\arg \max_{\boldsymbol{\Lambda}} (Q(\boldsymbol{\Lambda,\boldsymbol{\Psi}})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(t+1)})\\
=&\arg \max_{\boldsymbol{\Lambda}} \{\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(t+1)}}\\
&-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(t+1)}}\\
&-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj,(t+1)}}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2}{\psi_{jj,(t+1)}}+\text{constant}\}.\\
\end{align*}
$$

Since the last three terms do not contain any $\boldsymbol{\Lambda}$, so they can be eliminated. Therefore we have

$$
\begin{align*}
\boldsymbol{\Lambda}_{(t+1)}

=& \arg \min_{\boldsymbol{\Lambda}}\{-\sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(t+1)}}\\

& + \sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(t+1)}} \}
\end{align*} 
$$

The objective function, denoted as $G(\boldsymbol{\Lambda})$ enjoys a nice convexity property given by the following lemma:

::: {#lem-mstep-covexity}

The objective function is convex with respective to $\boldsymbol{\Lambda}_{q\cdot}$ for any given $q=1,2,\dots,p$.

:::

::: proof
To show @lem-mstep-covexity, we only need to show $\sum_{i=1}^n (\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)=n\boldsymbol{B}+\boldsymbol{AY}^\top\boldsymbol{YA}^\top$ is positive semi-definite. Consider an arbitrary vector $\boldsymbol{\eta}\in\mathbb{R}^k$, we have
$$
\begin{align*}
\boldsymbol{\eta}^\top (n \boldsymbol{B} + \boldsymbol{AY}^\top \boldsymbol{YA}^\top) \boldsymbol{\eta}& = n \boldsymbol{\eta}^\top \boldsymbol{B} \boldsymbol{\eta} + \boldsymbol{\eta}^\top \boldsymbol{AY}^\top  \boldsymbol{YA}^\top \boldsymbol{\eta}\\
& = n \boldsymbol{\eta}^\top \boldsymbol{B} \boldsymbol{\eta} + || \boldsymbol{YA}^\top \boldsymbol{\eta} ||_2^2
\end{align*}.
$$
The second term is greater or equal to zero and for the first term, consider the following block-structured matrix

$$
\boldsymbol{P}=
\begin{pmatrix}
\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi} & \boldsymbol{\Lambda}\\
\boldsymbol{\Lambda}^\top & \boldsymbol{I}_k\\
\end{pmatrix}
$$

We can verify that the top-left matrix, denoted as $\boldsymbol{P}_{11}$ is positive semi-definite since $\boldsymbol{\eta}^\top (\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}) \boldsymbol{\eta} = ||\boldsymbol{\Lambda}^\top \boldsymbol{\eta}||_2^2 + \boldsymbol{\eta}^\top \boldsymbol{\Psi} \boldsymbol{\eta}$ and the $\boldsymbol{\Psi}$ is a diagonal matrix with all the non-zero elements positive as defined. Therefore the matrix $\boldsymbol{P}$ is positive semi-definite and hence its Schur complementary is also positive semi-definite. The Schur complementary with respect to $\boldsymbol{P}_{11}$ is given by $\boldsymbol{I}_k - \boldsymbol{\Lambda}^\top (\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi})$, which is $\boldsymbol{B}$ we previously defined. Therefore $n \boldsymbol{\eta}^\top \boldsymbol{B} \boldsymbol{\eta} \geq 0$, which leads to $\boldsymbol{\eta}^\top (n \boldsymbol{B} + \boldsymbol{AY}^\top \boldsymbol{YA}^\top) \boldsymbol{\eta} \geq 0$ for any $\boldsymbol{\eta}\in\mathbb{R}^k$.
:::

Therefore we can update $\boldsymbol{\Lambda}$ rowwisely using the critical points. Mathematically, the gradient of the object function with respect of the $q$-th row is given by 

$$
\begin{align*}
\nabla G(\boldsymbol{\Lambda}(q,)) &= \sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(q,)^\top}{\psi_{qq,(t+1)}}-\sum_{i=1}^n\frac{2y_{iq}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{qq,(t+1)}}\\
&= \frac{2}{\psi_{qq}}(n\boldsymbol{B}+\boldsymbol{A}\boldsymbol{Y}^\top\boldsymbol{Y}\boldsymbol{A}^\top)\boldsymbol{\Lambda}[q,]^\top - \boldsymbol{Y}[,q]^\top\boldsymbol{Y}\boldsymbol{A}^\top)
\end{align*}.
$$ {#eq-gradient}
Let $\nabla G(\boldsymbol{\Lambda}(q,))=0$, we have the formula to update $\boldsymbol{\Lambda}$ as

$$
\boldsymbol{\Lambda}_{(t+1)}(q,) = \boldsymbol{Y}[,q]^\top \boldsymbol{Y} \boldsymbol{A} (n\boldsymbol{B} + \boldsymbol{A}\boldsymbol{Y}^\top\boldsymbol{Y}\boldsymbol{A}^\top)^{-1}.
$$ {#eq-nplambdaupdate}


## M-step: Penalized EM-algorithm

The $Q$-function of the penalized EM-algorithm is the penalized expectation, i.e.
$$
Q(\boldsymbol{\Lambda},\boldsymbol{\Psi}) = \mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})|_{(t)}\right) - \frac{1}{2}P_{\rho}(\boldsymbol{\Lambda})
$$
We add a coefficient $\frac{1}{2}$ before $P_{\rho}(\boldsymbol{\Lambda})$ for simplification since we notice that the same coefficient occurs in each term of conditional expectation $\mathbb{E}\left(\ell(\boldsymbol{\theta}_{(t)})|_{(t)}\right)$.

The maximization towards the $Q$-function with respect to $\boldsymbol{\Psi}$ is exactly the same with @eq-psiupdate, since we did not add any penalty to matrix $\boldsymbol{\Psi}$.

Now consider the update for $boldsymbol{\Lambda}$, we have

$$
\begin{align*}
\boldsymbol{\Lambda}_{(t+1)} &= \arg \max_{\boldsymbol{\Lambda}}\{Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})| \boldsymbol{\Psi} = \boldsymbol{\Psi}_{t+1} \} \\

=& \arg \min_{\boldsymbol{\Lambda}}\{-\sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(t+1)}}\\

& + \sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(t+1)}} + \rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1
  \}
\end{align*} 
$$


Due to the convexity proved before and non-differentiablity of the $L_1$-norm at certain points, a proximal gradient method can be necessary to optimize $\boldsymbol{\Lambda}$ row by row.

Denote the differentiable part (w.r.t. the $q$-th row of $\boldsymbol{\Lambda}$) of the objective function as 
$$
G(\boldsymbol{\Lambda}):= \sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(t+1)}} -\sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(t+1)}},
$$
while the non-differentiable part is denoted as
$$
H(\boldsymbol{\Lambda}) := \rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1
$$
We will have a nice form for the objective function, under which a proximal gradient method can be applied for rowwisely upgrade the loading matrix.

:::{#lem-proximalmethod}
Consider the optimization problem
$$
\min_{\boldsymbol{x}\in\mathbb{R}^n}\{G(\boldsymbol{x})+H(\boldsymbol{x})\},
$$
where 

1. $G(\boldsymbol{x}): \mathbb{R}^n \to \mathbb{R}$ is convex, differentiable, and $L$-Lipschitz continuous,
2. $H(\boldsymbol{x}): \mathbb{R}^n \to (-\infty,\infty]$ is proper, lower semi-continuous and convex.

The proximal gradient method is that pick an initial guess $\boldsymbol{x}_0$, for $k=0,1,2,\dots$, repeat
$$
\boldsymbol{\xi}_k := \boldsymbol{x}_k - s_k \nabla G(\boldsymbol{x})
$$
$$
\boldsymbol{x}_{k+1} := \arg \min_{\boldsymbol{x}\in\mathbb{R}^n}\{H(\boldsymbol{x})+\frac{1}{2s_k}||\boldsymbol{x}-\boldsymbol{\xi}_k||^2_2\}
$$
with a properly chosen step size $0<s_k<\frac{1}{L}$.

:::
One should notice that in practice, we may choose a sufficiently small constant step size $s_k=s$. In our case, the strategy to update the loading matrix is

:::{.callout-note}
## Proximal method for updating loading matrix 

For each row of the $\boldsymbol{\Lambda}$, say, $\boldsymbol{\Lambda}(q,)$ where $q=1,2,\dots,p$, initialize it as the updated row of loading matrix in the last M-step respectively. Then for $k=1,2,\dots,$, update the $q$-th row of $\boldsymbol{\Lambda}$ by repeating
$$
\boldsymbol{\xi}_k := \boldsymbol{\Lambda}(q,)_{(t)} - s_k \nabla G(\boldsymbol{\Lambda}(q,)_{(t)})
$$ {#eq-xi}
$$
\boldsymbol{\Lambda}(q,)_{(t+1)}:=\text{sign}(\boldsymbol{\xi}_k) \cdot \max(|\boldsymbol{\xi}_k|-\rho,0)
$$ {#eq-lambdaupdate}
where $|\cdot|$ is the elementwise absolute value and $\nabla G(\boldsymbol{\Lambda}(q,))$ can be calculated by
$$
\nabla G(\boldsymbol{\Lambda}(q,)) = \sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(q,)^\top}{\psi_{qq,(k+1)}}-\sum_{i=1}^n\frac{2y_{iq}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{qq,(k+1)}}.
$$ {#eq-gradient}
:::



## Summary of algorithms: EM and penalized EM

:::{.callout-note}
## Algorithm: Standard EM-algorithm


:::

:::{.callout-note}

## Algorithm: Penalized EM-algorithm

  1. Set reasonable initial value $\boldsymbol{\Psi}_{(0)}$ and $\boldsymbol{\Lambda}_{(0)}$.
  2. Calculate the expectation using formula @eq-expectation.
  3. Update $\boldsymbol{\Psi}$ using formulae @eq-psiupdate and @eq-psiselect.
  4. Update $\boldsymbol{\Lambda}$ using proximal method @eq-lambdaupdate, @eq-xi, and @eq-gradient
  5. Repeat 2,3, and 4 until convergence.

:::


## Selecting initial values 

The initialization strategy of the EM algorithm is of interest to discuss. Although the update of $\boldsymbol{\Lambda}$ is a convex optimization problem, an appropriate initial value can still make the convergence faster. Here are some ways to find a satisfying initial value:

1. randomly chosen values,
2. use the result of MLE as the initial value,
3. use the result of traditional two-step method as the initial value,
4. start from a very small order, like $k=1$. Then use the result from the smaller order model as the first $k$ columns of the loading matrix.


The comparison of these strategies will be demonstrated in the simulation part.

## Simulation setting 

Criteria? Judge based on estimation? Sparsity structure.


### Setting 1

- Hirose + other existing simulation settings in published papers
- Different FA order (k = 2, 4, 6, 8 x n = 200, 400, 800, 1600 x sparse vs non-sparse).



### Setting 2



