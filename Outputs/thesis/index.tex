% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
  oneside,
  openany,
  12pt,
  onecolumn]{book}

\usepackage{amsmath,amssymb}
\usepackage{setspace}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
    \setmainfont[]{Public Sans}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[top=30mm,left=25mm,right=25mm,bottom=30mm]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{2}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{remark}
\AtBeginDocument{\renewcommand*{\proofname}{Proof}}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\newtheorem{refremark}{Remark}[chapter]
\newtheorem{refsolution}{Solution}[chapter]
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Model Selection in Factor Analysis},
  pdfauthor={Zhining Wang},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{amsmath}	% Advanced maths commands
\usepackage{amssymb}	% Extra maths symbols
\usepackage{rotating} % Rotating table
\usepackage[toc,page]{appendix}
\usepackage{setspace}
\usepackage{rotating}
\usepackage{longtable}
\usepackage{natbib}
\usepackage{multirow}


\def\TODO#1{{\color{red}{\bf [TODO:} {\it{#1}}{\bf ]}}}

%%%%%% AUTHORS - PLACE YOUR OWN PACKAGES HERE %%%%%

\usepackage{color}
\usepackage[normalem]{ulem}

% These are autoref macros
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{equation}

% There are editing macros
\newcommand{\red}[1]{{\textcolor{red}{#1}}}
\newcommand{\blue}[1]{{\textcolor{blue}{#1}}}
\newcommand{\cyan}[1]{{\textcolor{cyan}{#1}}}
\newcommand{\cut}[1]{{\red{\sout{#1}}}}
\newcommand{\add}[1]{{\cyan{#1}}}

% Autoref macros
\def\sectionautorefname{Section}
\def\subsectionautorefname{Section}
\def\subsubsectionautorefname{Section}
\def\figureautorefname{Figure}
\def\tableautorefname{Table}
\def\equationautorefname{equation}
\newcommand{\aref}[1]{\hyperref[#1]{Appendix~\ref{#1}}}

% Only include extra packages if you really need them. Common packages are:
%\usepackage{caption}
%\usepackage{subcaption}
%\captionsetup{compatibility=false} %https://tex.stackexchange.com/questions/31906/subcaption-package-compatibility-issue
\usepackage{multicol}        % Multi-column entries in tables
\usepackage{booktabs} %to include pandas latex tables
\usepackage{float}


\title{Model Selection in Factor Analysis}
\author{Zhining Wang}
\date{27th June 2024}

\begin{document}
  \begin{frontmatter}
  \begin{titlepage}
  %%% TITLE PAGE:
  \pagenumbering{roman}  % first use Roman numerals for page numbers

  \begin{titlepage}
    \begin{flushright}%
      \vspace{50mm}
      {\small A thesis submitted for the degree of {\it Master of
  Mathematical Science}}
      \rule[1ex]{\textwidth}{1pt}\\
      {\fontsize{9}{0} 27th June 2024}\\
      \vspace{25mm}
      {\fontsize{40}{44}\bfseries Model Selection in Factor
  Analysis\par}
        \vspace{12mm}
        \vfill
      {\fontsize{20}{0}\bfseries Zhining Wang}\\
      \vspace{2mm}
      {\fontsize{8}{0} Mathematical Sciences Institute}\\
      \vspace{35mm}
      {\fontsize{10}{0}\bfseries supervised by}\\
      Dr.~Emi Tanaka, Dr.~Qinian Jin
      
      \vspace{2.0cm}
  		\includegraphics[width=0.4\textwidth]{\_extensions/anu-thesis/assets/latex/ANU\_Primary\_Horizontal\_GoldBlack.eps}\\
   \end{flushright}%

   \clearpage\thispagestyle{empty}
   \normalfont
   \vspace*{\fill}
   \noindent
   \begin{tabular}{lp{10cm}}
     {\bf Master of Mathematical Science Thesis} & \\[2mm]
     {\bf Author:} & Zhining Wang\\[2mm]
     {\bf Supervisors:} & Dr.~Emi Tanaka, Dr.~Qinian Jin\\[2mm]
     
     {\bf Project period:} & 2024-03-01 -- 2024-10-01 \\[2mm]
   \end{tabular}\\[2mm]

  \noindent Mathematical Sciences Institute\\
  \noindent Australian National University

  \end{titlepage}
  \setlength{\parindent}{0pt}
  \setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}








  \end{titlepage}
  \end{frontmatter}


\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\phantomsection\addcontentsline{toc}{section}{List of Figures}
\listoffigures
\phantomsection\addcontentsline{toc}{section}{List of Tables}
\listoftables
\setstretch{1.2}
\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

Notation is presented in Table~\ref{tbl-notation}.

\section{What is factor analysis and why is it
important?}\label{what-is-factor-analysis-and-why-is-it-important}

Factor analysis is a mathematical model which tries to use fewer
underlying factors to explain the correlation between a large set of
observed variables \citep{mardiaMultivariateAnalysis1979}. It provides a
useful tool for exploring the covariance structure among observable
variables \citep{hiroseSparseEstimationNonconcave2015}. One of the major
assumptions that factor analytic model stands on is that it is
impossible for us to observe those underlying factors directly. This
assumption is especially suited to subjects like psychology where we
cannot observe exactly some concept like how intelligent our subjects
are \citep{mardiaMultivariateAnalysis1979}.

Suppose we have a observable random vector
\(\boldsymbol{y}\in \mathbb{R}^p\) with mean
\(\mathbb{E}[\boldsymbol{y}]=\boldsymbol{\mu}\) and variance
\(\mathbb{V}[\boldsymbol{y}]=\boldsymbol{\Sigma}\). Then a \(k\)-order
factor analysis model for \(\boldsymbol{y}\) can be given by
\begin{equation}
\boldsymbol{y}=\boldsymbol{\Lambda} \boldsymbol{f}+\boldsymbol{\mu}+\boldsymbol{\epsilon},
\end{equation} where
\(\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}\) is called
\emph{loading matrix}, we call \(\boldsymbol{f} \in \mathbb{R}^{k}\) as
\emph{common factors} and \(\boldsymbol{\epsilon} \in \mathbb{R}^{p}\)
is \emph{unique factors}. To make the model well-defined, we may assume
\[\mathbb{E}[\boldsymbol{f}]=\boldsymbol{0}_k, \mathbb{V}[\boldsymbol{f}]=\boldsymbol{I}_{k\times k}, \mathbb{E}[\boldsymbol{\epsilon}]=\boldsymbol{0}_p, \mathbb{V}[\boldsymbol{\epsilon}]=:\boldsymbol{\Psi}=\text{diag}(\Psi_{11},\dots,\Psi_{pp})\]
and also the independence between any elements from \(\boldsymbol{f}\)
and \(\boldsymbol{\epsilon}\) separately, i.e.
\[Cov[f_i,\epsilon_j]=0, \text{for all } i\in\{1,2,\dots,k\} \text{ and } j \in \{1,2,\dots,p\}\]
Straightforwardly, the covariance of observable vector
\(\boldsymbol{y}\) can be modelled by\\
\begin{equation}
\mathbb{V}[\boldsymbol{y}]=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}
\end{equation}

\section{Indeterminacy of the loading
matrix}\label{indeterminacy-of-the-loading-matrix}

One can easily see that if our factor analytic model is given by (1),
then it can also be modelled as
\[\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}\]
where the matrix \(\boldsymbol{M}\) is orthogonal and simultaneously the
variance of \(\boldsymbol{y}\) given by (2) still holds, since
\[\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]
Therefore a rotated loading matrix
\(\boldsymbol{\Lambda}\boldsymbol{M}\) is still a valid loading matrix
for a factor analytic model. Sometimes we resolve this problem by making
the loading matrix to satisfy some constraints like
\citep{mardiaMultivariateAnalysis1979}
\[\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}\]

\section{Traditional Estimation of Parameters in Factor Analytic
Models}\label{traditional-estimation-of-parameters-in-factor-analytic-models}

We denote the set of parameters by
\(\beta := \{\text{vec}(\boldsymbol{\Lambda}),\text{vec}(\boldsymbol{\Psi})\}\)
where \(\text{vec}(\cdot)\) is the vectorisation of the input.

Traditionally, a two-step procedure is used to construct a factor
analytic model: estimate parameters by maximum likelihood estimation
(aka, MLE) and then use rotation techniques to find an interpretable
model.

\subsection{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}

Suppose we have \(n\) independent and identically distributed
observations
\(\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_N\) from a
p-dimensional multi-variate normal distribution
\(N_p(\boldsymbol{\mu},\boldsymbol{\Sigma})\) and by our hypothesis, we
have
\(\boldsymbol{\Sigma}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\).
Then the likelihood function is given by
\[L(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].\]
and hence the log-likelihood is given by \begin{align*}
l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=& \sum^n_{i=1}[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})]\\
&= -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})],
\end{align*} where \(\boldsymbol{S}\) is the sample covariance defined
as
\(\boldsymbol{S}:=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\).
To get the MLE of parameters, we seek the roots of equation system

\begin{cases}
\frac{\partial}{\partial \boldsymbol{\Lambda}}l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=0.
\end{cases}

However, there is no closed form of the roots. Many iterative algorithms
are developed to get the roots of the equation, like EM algorithm
developed by Rubin\citep{Rubin1982EMAlgorithms}.

\subsection{Rotation techniques}\label{rotation-techniques}

After estimation, we want to rotate the loading matrix to possess a
sparse matrix in order to interpret the observable variables by
underlying factors better. Also there are many method to achieve
rotation as well such as the varimax method and the promax method
\citep{hiroseSparseEstimationNonconcave2015}.

Suppose \(Q(\boldsymbol{\Lambda})\) is an criterion for
\(\boldsymbol{\Lambda}\) in the rotation procedure, and we may express
it as
\(Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})\)
where \(P(\cdot)\) is some loss
function\citep{hiroseSparseEstimationNonconcave2015}. Specifically, if
we set \(P(\cdot)=|\cdot|\), we have LASSO to generate a theoretically
sparse loading matrix. If we rewrite this in a optimization problem, it
can be given as \citep{Jennrich2004Rotation} \begin{align*}
&\min_\boldsymbol{\Lambda} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*} where \(\boldsymbol{\Lambda}_0\) is an initial guess. Since
we execute this technique after obtaining the MLE of parameters,
therefore what we want is \begin{align*}
&\min_\boldsymbol{\Lambda} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*} where \(\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\) is the
maximum likelihood estimator.

\subsection{Discussion about two-step
method}\label{discussion-about-two-step-method}

The traditional two-step method faces significant shortcomings,
primarily its unsuitability\citep{hiroseSparseEstimationNonconcave2015}.
Similar to challenges in regression models, MLE can result in
overfitting, and rotation techniques might not yield a sufficiently
sparse loading matrix. See this as an example: Suppose the true loading
matrix is given by \[
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 & 0 \\
0 & 0.8 \\
0 & 0\\
0 & 0\\
0 & 0\\
\end{bmatrix}.
\] We generate \(\boldsymbol{Y}\in \mathbb{R}^{{100}\times 5}\) using
common factor \(\boldsymbol{f}\in \mathbb{R}^{{100}\times 2}\) where
each factor is generated randomly from standard normal distribution
\(N(0,1)\). We utilize the R-function factanal() to produce the loading
matrix via MLE and then call varimax() to rotate our loading matrix.
What we finally get is \[
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
-0.158 & 0.985\\
0 & 0\\
0.997 & 0\\
0.207 & 0\\
0 & 0\\
\end{bmatrix},
\] which is neither precise nor sparse.

Simulation code:

\begin{verbatim}
set.seed(123)
n <- 100 # Number of observations
p <- 5   # Number of features

factor1 <- rnorm(n, 0, 1) # set factors
factor2 <- rnorm(n, 0, 1)

X <- matrix(NA, nrow=n, ncol=p)
X[,1] <- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading
X[,2] <- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading
X[,3] <- rnorm(n, 0, 1)  # Noise, no strong loading on any factor
X[,4] <- rnorm(n, 0, 1)  
X[,5] <- rnorm(n, 0, 1) 

fa_result <- factanal(factors = 2, covmat = cor(X))
rotated_fa <- varimax(fa_result$loadings)
\end{verbatim}

Another problem is that the absence of model selection complicates
efficient modeling by making it difficult to determine the precise
number of underlying factors, that is, the order of the factor analytic
model. A usual but troublesome method is conducting a hypothesis test
for the model\citep{mardiaMultivariateAnalysis1979}. We have
\begin{align*}
  &&&H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &&&H_a: \text{ Otherwise}.
\end{align*} The test statistics is given by
\(\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})\)
which has an asymptotic \(\chi^2_s\) distribution with
\(s=\frac{1}{2}(p-k)^2-(p+k)\) by the property of MLE, where \(F\) is
given by (\citet{mardiaMultivariateAnalysis1979}) \[
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
\] Typically, the test is conducted at a \(5\%\) significant level. We
can start the procedure from a very small \(k\), say, \(k=1\) or \(k=2\)
and then increase \(k\) until the null hypothesis is not rejected.

\section{Penalized Likelihood Method}\label{penalized-likelihood-method}

Penalized likelihood method can be viewed as a generalization of
two-step method mentioned
above\citep{hiroseSparseEstimationNonconcave2015}. A penalized factor
analytic model can be obtained by solving following optimization problem
\[
 \max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}l_p := l(\boldsymbol{\Lambda},\boldsymbol{\Psi})-\rho\sum^p_{i=1}\sum^k_{j=1}P(|\boldsymbol{\Lambda}_{ij}|).
\] where we call \(l_p\) as the penalized likelihood, \(\rho\) is called
regularization parameter and we can treat \(P(\cdot)\) as a penalized
function. There are many types of penalized functions developed, such as
LASSO (\(P(\cdot)=|\cdot|\)) and MC+
(\(P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|<\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)\))\citep{hiroseSparseEstimationNonconcave2015}.
In this article, we will mainly focus on the LASSO penalty.

\section{LASSO penalty and LASSO
estimator}\label{lasso-penalty-and-lasso-estimator}

Again, recall that the LASSO penalized likelihood is given by
\[l_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\]
and the LASSO estimator, denoted as
\((\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)\), can be obtained via
\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}

An E-M algorithm can be applied for evaluating the LASSO estimator.

\section{Appendix}\label{appendix}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.1600}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8400}}@{}}
\caption{List of notations used in this
report.}\label{tbl-notation}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Notation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Notation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(\boldsymbol{A}^\top\) & the transpose of the matrix (or vector)
\(A\) \\
\(\mathbb{R}^{p}\) & the space of all p-dimensional real column vectors
like \([a_1,a_2,\dots,a_p]^\top\) \\
\(\mathbb{R}^{p\times q}\) & the space of all real matrices with size
\(p\times q\) \\
\(\mathbb{E}[\cdot]\) & the expectation, or mean of a random variable \\
\(\mathbb{V}[\cdot]\) & the variance, or covariance of a random
variable \\
\(\boldsymbol{Cov}[\cdot , \cdot]\) & the covariance of two random
variables \\
\(\boldsymbol{0}_{p}\) and \(\boldsymbol{0}_{p\times q}\) & the
p-dimensional \(0\) vector or \(0\) matrix in size \(p\times q\)
respectively \\
\(\boldsymbol{I}_p\) & the identity matrix in size \(p\times p\) \\
\(\det(\cdot)\) & the determinant of a matrix \\
\(M(i,)\) & the \(i^{th}\) row of the matrix \(M\) \\
\(M(,j)\) & the \(j^{th}\) row of the matrix \(M\) \\
\end{longtable}

\bookmarksetup{startatroot}

\chapter{References}\label{references}

\bookmarksetup{startatroot}

\chapter{The EM Algorithm}\label{sec-EM}

\section{A brief introduction about the EM
algorithm}\label{a-brief-introduction-about-the-em-algorithm}

The Expectation-Maximization (EM) algorithm is a widely used iterative
method to compute the maximum likelihood estimation, especially when we
have some unobserved data\citep{Ng2012EMAlgorithm}. As we mentioned, the
key of maximum likelihood estimation is solving equation \[
\frac{\partial}{\partial \beta}l=0.
\] However, challenges often arise from the complex nature of the
log-likelihood function, especially with data that is grouped, censored,
or truncated. To navigate these difficulties, the EM algorithm
introduces an ingenious approach by conceptualizing an equivalent
statistical problem that incorporates both observed and unobserved data.
Here, \emph{augmented data}(or complete) refers to the integration of
this unobserved component, enhancing the algorithm's ability to
iteratively estimate through two distinct phases: the Expectation step
(E-step) and the Maximization step (M-step). The iteration between these
steps facilitates the efficient of parameter estimates, making the EM
algorithm an essential tool for handling incomplete data sets
effectively.

\section{The E-step and M-step}\label{the-e-step-and-m-step}

Let \(\boldsymbol{x}\) denote the vector containing complete data,
\(\boldsymbol{y}\) denote the observed incomplete data and
\(\boldsymbol{z}\) denote the vector containing the missing data. Here
`missing data' is not necessarily missing, even if it does not at first
appear to be missed, we can formulating it to be as such to facilitate
the computation (we may see this later)\citep{Ng2012EMAlgorithm}. Also
we assume \(\boldsymbol{\beta}\) as the parameter we want to estimate
over the parameter space \(\boldsymbol{\Omega}\).

Now denote \(f_\boldsymbol{X}(\boldsymbol{x};\boldsymbol{\beta})\) as
the probability density function (p.d.f.) of the random vector
\(\boldsymbol{X}\) corresponding to \(\boldsymbol{x}\). Then the
complete-data log-likelihood function when complete data is fully
observed can be given by

\[\log L_\boldsymbol{X}(\boldsymbol{\beta})=\log f_\boldsymbol{X}(\boldsymbol{x};\boldsymbol{\beta}).\]

The EM algorithm approaches the problem of solving the incomplete-data
likelihood equation indirectly by proceeding iteratively in terms of
\(\log L_\boldsymbol{X}(\boldsymbol{\beta})\). But it is unobservable
since it includes missing part of the data, then we use the conditional
expectation given \(\boldsymbol{y}\) and current fit for
\(\boldsymbol{\beta}\).

On the \((k+1)^th\) iteration, we have \[
\begin{align*}
&\text{E-step: Compute } Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}):=\mathbb{E}_\boldsymbol{X}[\log L_\boldsymbol{X}(\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
&\text{M-step: Update }\boldsymbol{\beta}^{(k+1)} \text{ as }\boldsymbol{{\beta}}^{(k+1)}:=\text{arg}\max_\boldsymbol{\beta} Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) \text{ (Original EM)}\\
& \text{ Or update }\boldsymbol{{\beta}}^{(k+1)} \text{ such that } Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\text{ (Generalized EM)}.
\end{align*} 
\] We keep iterating between E-step and M-step until convergence, which
may be determined by a criteria such as
\(||\boldsymbol{\beta}^{(k+1)}-\boldsymbol{\beta}^{(k)}||_p\leq \epsilon\)
for some p-norm \(||\cdot||_p\) and positive \(\epsilon\).

Meanwhile, the M-step in both original and generalized algorithms
defines a mapping from the parameter space \(\boldsymbol{\Omega}\) to
itself by \[
\begin{align*}
M: \boldsymbol{\Omega} &\to \boldsymbol{\Omega}\\
   \boldsymbol{\beta}^{(k)} &\mapsto \boldsymbol{\beta}^{(k+1)}.
\end{align*}
\]

\section{Convergence of the EM
Algorithm}\label{convergence-of-the-em-algorithm}

By the definition of conditional likelihood, our likelihood of complete
data can be expressed by \[
L_\boldsymbol{X}(\boldsymbol{\beta}) = f_\boldsymbol{X}(\boldsymbol{x};\boldsymbol{\beta})=L_\boldsymbol{Y}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
\] and hence the log-likelihood is given by \[
\log L_\boldsymbol{X}(\boldsymbol{\beta}) = \log L_\boldsymbol{Y}(\boldsymbol{\beta})+ \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
\] Take expectation to both sides of the equation with respect to
\(\boldsymbol{x|y}\) and replace \(\boldsymbol{\beta}\) by
\(\boldsymbol{\beta}^{(k)}\), we will have \[
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_\boldsymbol{Y}(\boldsymbol{\beta}) + \mathbb{E}_\boldsymbol{X}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
\] Now consider the difference of log-likelihood of \(\boldsymbol{Y}\)
function between two iterations, we have \[
\begin{align*}
  \log L_\boldsymbol{Y}(\boldsymbol{\beta}^{(k+1)})-\log L_\boldsymbol{Y}(\boldsymbol{\beta}^{(k)}) =
  &\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &-\{\mathbb{E}_\boldsymbol{X}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &-\mathbb{E}_\boldsymbol{X}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
\] By the procedure of EM-algorithm, we always have
\(Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\).
By the Jensen's inequality, we have
\(\mathbb{E}_\boldsymbol{X}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_\boldsymbol{X}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].\)
Therefore during iterations, the log-likelihood of observed data
\(\boldsymbol{Y}\) keeps increasing. If the log-likelihood is bounded,
then we can promise that it must converge to some maximum.

\bookmarksetup{startatroot}

\chapter{The EM Algorithm in LASSO Factor Analytic
Models}\label{the-em-algorithm-in-lasso-factor-analytic-models}

\section{Model Setup}\label{model-setup}

Suppose we have the \(n\) centralized observations,
\(\{\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\}\), where
\(\boldsymbol{y}_j\in \mathbb{R}^p\) and hence the mean,
\(\boldsymbol{\mu}_j=\boldsymbol{0}_p\) (\(j=1,2,\dots,n\)). For each of
the observation \(\boldsymbol{y}_j\), the common factor and unique
factor are \(\boldsymbol{f}_j\) and \(\boldsymbol{\epsilon}_j\)
respectively. Denote the response matrix as
\(\boldsymbol{Y}=[\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n]^\top\),
the common factor matrix as
\(\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_n]^\top\),
and the unique factor matrix as
\(\boldsymbol{\hat\epsilon}=[\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2,\dots,\boldsymbol{\epsilon}_n]^\top\).
Then the model can be written as
\[\boldsymbol{Y}=\boldsymbol{F}\boldsymbol{\Lambda}^\top+\boldsymbol{\hat\epsilon}.\]\\
We also assume

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\boldsymbol{\epsilon}_j\) follows a normal distribution with mean
  \(0\) and variance \(\boldsymbol{\Psi}\) for \(j=1,2,\dots,n\), where
  \(\boldsymbol{\Psi}\) is a diagonal matrix defined by
  \(\boldsymbol{\Psi}=diag(\psi_{11},\psi_{22},\dots,\psi_{pp})\).
\item
  \(\boldsymbol{f}_j\) follows a normal distribution with mean \(0\) and
  variance \(\boldsymbol{I}_k\) for \(j=1,2,\dots,n\).
\item
  \(\boldsymbol{y}_j\) follows a normal distribution with mean \(0\) and
  variance
  \(\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\)
  for \(j=1,2,\dots,n\). Those \(\boldsymbol{y}_j\) are pairwisely
  independent.
\end{enumerate}

\section{The EM Algorithm}\label{the-em-algorithm}

We treat the common factor matrix \(\boldsymbol{F}\) as the latent
variable. In E-step, we need to compute the conditional expectation of
the joint likelihood of \((\boldsymbol{Y},\boldsymbol{F})\) given
\(\boldsymbol{Y}\) and
\((\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})\), where
\((\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})\) is the
parameter we got in the \(k\)-th iteration (\(k>1\)) and
\((\boldsymbol{\Lambda}_{(0)},\boldsymbol{\Psi}_{(0)})\) is the initial
value we set. In M-step, we maximize the conditional expectation over
parameters and get
\((\boldsymbol{\Lambda}_{(k+1)},\boldsymbol{\Psi}_{(k+1)})\).

\subsection{E-Step}\label{e-step}

First, the joint likelihood of \((\boldsymbol{Y},\boldsymbol{F})\),
denoted as
\(L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\),
is given by

\[
\begin{align*}
L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&\prod_{i=1}^nf(\boldsymbol{y}_i,\boldsymbol{f}_i)\\
=&\prod_{i=1}^nf(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\\
=&\prod_{i=1}^nN(\boldsymbol{y}_i;\boldsymbol{\Lambda}\boldsymbol{f}_i,\boldsymbol{\Psi})N(\boldsymbol{f}_i;\boldsymbol{0}_k,\boldsymbol{I}_k)\\
=&\prod_{i=1}^n[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Psi})^{-\frac{1}{2}}\exp\{-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}\\
&(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\}][(2\pi)^{-\frac{k}{2}}\det({\boldsymbol{I}_k})^{-\frac{1}{2}}\exp\{-\frac{1}{2}\boldsymbol{f}_i^\top\boldsymbol{I}_k^{-1}\boldsymbol{f}_i\}]\\
= &(2\pi)^{-\frac{n}{2}(p+k)}(\prod_{i=1}^p\psi_{jj})^{-\frac{n}{2}}\exp\{-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}\}\\
&\exp\{-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\}.
\end{align*}
\]

Furthermore, the log-likelihood, denoted as
\(l_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\),
is given by

\begin{equation}\phantomsection\label{eq-loglikelihood}{
l_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i+\text{constant}.
}\end{equation}

Now let us deduce the conditional expectation to \(\boldsymbol{F}\)
given
\(\boldsymbol{Y},\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)}\),
denoted as \(El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\). In
Equation~\ref{eq-loglikelihood}, the first term is independent on
\(\boldsymbol{F}\), hence stay the same under conditional expectation.
The last term is independent on
\((\boldsymbol{\Lambda},\boldsymbol{\Psi})\), therefore we can regard it
as a constant in \(El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\).
We only need to consider the term inside the sum in the second term,
more specifically, the numerator, we have

\[
(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2 = y_{ij}^2 - 2 y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i + \boldsymbol{\Lambda}(j,)\boldsymbol{f}_i\boldsymbol{f}_i^\top \boldsymbol{\Lambda}^\top(,j).
\]

Without ambiguity, denote \(\mathbb{E}[\boldsymbol{f}_i|_{(k)}]\) to be
the conditional expectation
\(\mathbb{E}[\boldsymbol{f}_i|\boldsymbol{Y},\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)}]\)
for simplification. Then the conditional expectation
\(El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\) is given by

\[
\begin{align*}
El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=& \ \text{constant}-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}-\\
&\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2 - 2 y_{ij}{\boldsymbol{\Lambda}}(j,)\mathbb{E}[\boldsymbol{f}_i|_{(k)}]\boldsymbol+\boldsymbol{\Lambda}(j,) \mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}^\top_i|_{(k)}]\boldsymbol{\Lambda}^\top(,j)}{\psi_{jj}}\\
\end{align*}
\]

To deal with \(\mathbb{E}[\boldsymbol{f}_i|_{(k)}]\) and
\(\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]\), we only
need to know the mean and variance of conditional distribution
\(\boldsymbol{f}_i|\boldsymbol{Y},\boldsymbol{\Lambda}^\top_{(k)},\boldsymbol{\Psi}_{(k)}\),
or equivalently
\(\boldsymbol{f}_i|\boldsymbol{y}_i,\boldsymbol{\Lambda}^\top_{(k)},\boldsymbol{\Psi}_{(k)}\)
because of the independency of \(\boldsymbol{f}_i\) to
\(\boldsymbol{y}_j\) for (\(i\neq j\)). This is because we can always
treat \(\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]\) as

\[
\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]= \mathbb{V}[\boldsymbol{f}_i|_{(k)}]+ \mathbb{E}[\boldsymbol{f}_i|_{(k)}]\mathbb{E}[\boldsymbol{f}_i|_{(k)}]^\top.
\] where \(\mathbb{V}[\boldsymbol{f}_i|_{(k)}]\) is the variance of
conditional distribution. We have

\begin{lemma}[]\protect\hypertarget{lem-1}{}\label{lem-1}

If
\(\boldsymbol{\alpha}\sim N(\boldsymbol{\mu}_\boldsymbol{\alpha},\boldsymbol{\Sigma_\boldsymbol{\alpha}})\)
and
\(\boldsymbol{\beta}\sim N(\boldsymbol{\mu}_\boldsymbol{\beta},\boldsymbol{\Sigma}_\boldsymbol{\beta})\),
then we have
\(\boldsymbol{\alpha}|\boldsymbol{\beta}\sim N(\boldsymbol{\mu}_{\boldsymbol{\alpha}|\boldsymbol{\beta}},\boldsymbol{\Sigma}_{\boldsymbol{\alpha}|\boldsymbol{\beta}})\),
where \[
\begin{align*}
& \boldsymbol{\mu}_{\boldsymbol{\alpha}|\boldsymbol{\beta}}=\boldsymbol{\mu}_\boldsymbol{\alpha}+\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]\boldsymbol{\Sigma}_\boldsymbol{\beta}^{-1}\boldsymbol{\beta}\\
& \boldsymbol{\Sigma}_{\boldsymbol{\alpha}|\boldsymbol{\beta}}= \boldsymbol{\Sigma}_\boldsymbol{\alpha}-\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]\boldsymbol{\Sigma}_\boldsymbol{\beta}^{-1}\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]^\top.
\end{align*}
\]

\end{lemma}

In our scenario, we have

\[
\begin{align*}
& \boldsymbol{\mu}_{\boldsymbol{f}_i}= \boldsymbol{0}_k, \boldsymbol{\Sigma}_{\boldsymbol{f}_{i}}=\boldsymbol{I}_k\\
& \boldsymbol{\mu}_{\boldsymbol{y}_i}= \boldsymbol{0}_p, \boldsymbol{\Sigma}_{\boldsymbol{y}_i}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}
\end{align*} 
\]

and also

\[
\boldsymbol{Cov}(\boldsymbol{y_i},\boldsymbol{f_i})=\boldsymbol{Cov}(\boldsymbol{\Lambda f}_i+\boldsymbol{\epsilon}_i,\boldsymbol{f}_i)=\boldsymbol{\Lambda}^\top.
\]

Therefore we have

\[
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(k)})=\boldsymbol{\mu}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{y}_i\\
&\mathbb{V}(\boldsymbol{f}_i|_{(k)})=\boldsymbol{\Sigma}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{\Lambda}_{(k)}.\\
\end{align*} 
\]

For simplification, let us denote

\[
\begin{align*}
&\boldsymbol{A}:=\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\\
&\boldsymbol{B}:=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{\Lambda}_{(k)},\\
\end{align*} 
\]

we will get

\[
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(k)})= \boldsymbol{A}\boldsymbol{y}_i\\
&\mathbb{E}(\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)})= \boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top.
\end{align*}
\]

Our expectation will finally be confirmed by

\[
\begin{align*}
El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})= & -\frac{n}{2}\sum_{j=1}^p\log{\Psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2-2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\Psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\Psi_{jj}}+\text{constant}.
\end{align*}
\]

\subsection{M-step}\label{m-step}

In M-step, we need to maximize so called \(Q\)-function with respect to
parameters where \(Q\)-function is penalized conditional expectation of
the log-likelihood, i.e. \[
Q(\boldsymbol{\Lambda},\boldsymbol{\Psi}) = El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi}) - \frac{1}{2}P_{\rho}(\boldsymbol{\Lambda})
\] We add a coefficient \(\frac{1}{2}\) before
\(P_{\rho}(\boldsymbol{\Lambda})\) for simplification since we notice
that the same coefficient occurs in each term of conditional expectation
\(El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})\). When execute
M-step, we use the following strategy\citep{Ng2012EMAlgorithm}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find \(\boldsymbol{\Psi}_{(k+1)}\) using current
  \(\boldsymbol{\Lambda}_{(k)}\), i.e. \[
    \boldsymbol{\Psi}_{(k+1)} = \arg \max_{\boldsymbol{\Psi}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)}).
  \]
\item
  Find \(\boldsymbol{\Lambda}_{(k+1)}\) using
  \(\boldsymbol{\Psi}_{(k+1)}\) we got in previous step, i.e.~ \[
    \boldsymbol{\Lambda}_{(k+1)} = \arg \max_{\boldsymbol{\Lambda}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(k+1)}).
  \]
\end{enumerate}

For step 1, take partial derivative with respect to each \(\psi_{jj}\)
and let it equal to zero to find the local maximizer of \(\psi_{jj}\),
i.e.~

\[
\frac{\partial}{\partial \psi_{jj}}Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)})=0.
\] By simple calculation, we will have

\[
\begin{align*}
\frac{\partial}{\partial \psi_{jj}}Q((\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)})=
&-\frac{n}{2}\frac{1}{\psi_{jj}}+\frac{1}{2}\sum_{i=1}^n
\frac{y_{ij}^2-2y_{ij}\boldsymbol{\Lambda}_{(k)}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj}^2} \\
&+ \frac{1}{2}\sum_{i=1}^n\frac{\boldsymbol{\Lambda}_{(k)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(k)}(j,)^\top}{\psi_{jj}^2}.\\
\end{align*}
\]

Thus the update of \(\boldsymbol{\Psi}\) will be elementwisely given by

\[
\begin{equation}
\psi_{jj,(k+1)}=\frac{1}{n}\sum_{i=1}^n y_{ij}^2-\frac{2}{n}\sum_{i=1}^ny_{ij}\boldsymbol{\Lambda}_{(k)}(j,)\boldsymbol{A}\boldsymbol{y}_i + \frac{1}{n}\sum_{i=1}^n \boldsymbol{\Lambda}_{(k)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(k)}(j,)^\top.
\end{equation}
\]

For step 2, let us revise the update formula, we have

\[
\begin{align*}
\boldsymbol{\Lambda}_{(k+1)}=&\arg \max_\boldsymbol{\Lambda} (Q(\boldsymbol{\Lambda,\boldsymbol{\Psi}})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(k+1)})\\
=&\arg \max_\boldsymbol{\Lambda} \{\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}\\
&-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}-\frac{1}{2}P_{\rho}(\boldsymbol{\Lambda})\\
&-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj,(k+1)}}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2}{\psi_{jj,(k+1)}}+\text{constant}\}.\\
\end{align*}
\]

Since the last three terms do not contain any \(\boldsymbol{\Lambda}\),
so they can be eliminated. After letting
\(P_\rho(\boldsymbol{\Lambda}):=\rho\sum_{j=1}^p\sum_{i=1}^k|\lambda_{ji}|=\rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1\)
as LASSO, we can rewrite it as

\$\$ \begin{align*}
\boldsymbol{\Lambda}_{(k+1)}

=& \arg \min_{\boldsymbol{\Lambda}}\{-\sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}\\

& + \sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}} + \rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1
  \}
\end{align*} \$\$

Notice that the objective function with respect to
\(\boldsymbol{\Lambda}(j,)\) for any given \(j=1,2,\dots,p\) is convex,
and due to non-differentiablity of the \(L_1\)-norm at certain points, a
subgradient approach can be necessary to optimize
\(\boldsymbol{\Lambda}\) row by row.

Denote the objective function as \(g(\boldsymbol{\Lambda})\), the
subdifferential of \(g(\boldsymbol{\Lambda})\) with respect to some
given \(j=1,2,\dots,p\) is given by \[
\begin{align*}
\partial_{(j)}g(\boldsymbol{\Lambda})&=-\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top+\boldsymbol{B}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \partial||\boldsymbol{\Lambda}(j,)^\top||_1\\
&=\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \partial||\boldsymbol{\Lambda}(j,)^\top||_1\\
\end{align*}
\] We have

\begin{lemma}[]\protect\hypertarget{lem-subd}{}\label{lem-subd}

For the set \(\partial||\boldsymbol{\Lambda}(j,)||_1\), we have \[
\text{sign}(\boldsymbol{\Lambda}(j,)^\top)\in \partial||\boldsymbol{\Lambda}(j,)^\top||_1,
\] where the sign function is given by \(\text{sign}(x)=
\begin{cases}
1 &\text{if } x>0\\
0 &\text{if } x=0\\
-1 &\text{if } x<0\\
\end{cases}\) elementwisely.

\end{lemma}

Therefore a iterative subgradient method for finding
\(\boldsymbol{\Lambda}_{(k+1)}\) rowwisely given by
\(\boldsymbol{\Lambda}_{(k)}\) is given by

\begin{tcolorbox}[enhanced jigsaw, bottomrule=.15mm, rightrule=.15mm, toprule=.15mm, bottomtitle=1mm, arc=.35mm, opacitybacktitle=0.6, leftrule=.75mm, colframe=quarto-callout-note-color-frame, titlerule=0mm, coltitle=black, opacityback=0, toptitle=1mm, breakable, colback=white, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{algorithm}, left=2mm]

For each \(j=1,2,...,p\), denote
\(\boldsymbol{\Lambda}_{(k+1)}^{(l)}(j,)\) as the \(l^{th}\) iteration
when executing the subgradient method to find the \(j^{th}\) row of
\(\boldsymbol{\Lambda}_{(k+1)}\), we iterate as following

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Set
  \(\boldsymbol{\Lambda}_{(k+1)}^{(0)}(j,):=\boldsymbol{\Lambda}_{(k)}(j,)\).
\item
  For \(l\geq 1\), calculate \[
  \partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})=-\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}^{(l)}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \text{ sign}(\boldsymbol{\Lambda}^{(l)}(j,)^\top) .
  \]
\item
  Update \(\boldsymbol{\Lambda}_{(k+1)}^{(l+1)}(j,)\) as \[
  \boldsymbol{\Lambda}_{(k+1)}^{(l+1)}(j,)=\boldsymbol{\Lambda}_{(k+1)}^{(l)}(j,)-t^{(l)}[\partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})]^\top
  \] where \(t^{(l)}\) is the step size in the \(l^{th}\) iteration and
  a widely used choice is letting
  \(t^{(l)}=\frac{1}{(l+1)||[\partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})]^\top||_2}\),
  where \(||\cdot||_2\) is the \(L_2\)
  norm.(\citet{boyd2003subgradient})
\item
  Repeat step 3 and 4 until converge.
\end{enumerate}

\end{tcolorbox}

the EM-Algorithm for LASSO FA

\bookmarksetup{startatroot}

\chapter*{References}\label{references-1}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\renewcommand{\bibsection}{}
\bibliography{ref.bib}


\backmatter


\end{document}
