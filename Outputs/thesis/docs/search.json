[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model Selection in Factor Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "3  Introduction",
    "section": "",
    "text": "3.1 What is factor analysis and why is it important?\nFactor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables (Mardia, Kent, and Bibby 1979). It provides a useful tool for exploring the covariance structure among observable variables (Hirose and Yamamoto 2015). One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are (Mardia, Kent, and Bibby 1979).\nSuppose we have a observable random vector \\boldsymbol{y}\\in \\mathbb{R}^p with mean \\mathbb{E}[\\boldsymbol{y}]=\\boldsymbol{\\mu} and variance \\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Sigma}. Then a k-order factor analysis model for \\boldsymbol{y} can be given by \\begin{equation}\n\\boldsymbol{y}=\\boldsymbol{\\Lambda} \\boldsymbol{f}+\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon},\n\\end{equation} where \\boldsymbol{\\Lambda} \\in \\mathbb{R}^{p \\times k} is called loading matrix, we call \\boldsymbol{f} \\in \\mathbb{R}^{k} as common factors and \\boldsymbol{\\epsilon} \\in \\mathbb{R}^{p} is unique factors. To make the model well-defined, we may assume \\mathbb{E}[\\boldsymbol{f}]=\\boldsymbol{0}_k, \\mathbb{V}[\\boldsymbol{f}]=\\boldsymbol{I}_{k\\times k}, \\mathbb{E}[\\boldsymbol{\\epsilon}]=\\boldsymbol{0}_p, \\mathbb{V}[\\boldsymbol{\\epsilon}]=:\\boldsymbol{\\Psi}=\\text{diag}(\\Psi_{11},\\dots,\\Psi_{pp}) and also the independence between any elements from \\boldsymbol{f} and \\boldsymbol{\\epsilon} separately, i.e. Cov[f_i,\\epsilon_j]=0, \\text{for all } i\\in\\{1,2,\\dots,k\\} \\text{ and } j \\in \\{1,2,\\dots,p\\} Straightforwardly, the covariance of observable vector \\boldsymbol{y} can be modelled by\n\\begin{equation}\n\\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{equation}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "href": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "title": "3  Introduction",
    "section": "3.2 Indeterminacy of the loading matrix",
    "text": "3.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\boldsymbol{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon} where the matrix \\boldsymbol{M} is orthogonal and simultaneously the variance of \\boldsymbol{y} given by (2) still holds, since \\mathbb{V}[\\boldsymbol{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Therefore a rotated loading matrix \\boldsymbol{\\Lambda}\\boldsymbol{M} is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "href": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "title": "3  Introduction",
    "section": "3.3 Traditional Estimation of Parameters in Factor Analytic Models",
    "text": "3.3 Traditional Estimation of Parameters in Factor Analytic Models\nWe denote the set of parameters by \\beta := \\{\\text{vec}(\\boldsymbol{\\Lambda}),\\text{vec}(\\boldsymbol{\\Psi})\\} where \\text{vec}(\\cdot) is the vectorisation of the input.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n3.3.1 Maximum Likelihood Estimation\nSuppose we have n independent and identically distributed observations \\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_N from a p-dimensional multi-variate normal distribution N_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) and by our hypothesis, we have \\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Then the likelihood function is given by L(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu}))\\right]. and hence the log-likelihood is given by\n\\begin{align*}\nl(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})]\\\\\n=& -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})],\n\\end{align*}\nwhere \\boldsymbol{S} is the sample covariance defined as \\boldsymbol{S}:=\\frac{1}{n}\\sum^n_{i=1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top. To get the MLE of parameters, we seek the roots of equation system\n\\begin{cases}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0.\n\\end{cases}\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin (Rubin and Thayer 1982).\n\n\n3.3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose Q(\\boldsymbol{\\Lambda}) is an criterion for \\boldsymbol{\\Lambda} in the rotation procedure, and we may express it as Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij}) where P(\\cdot) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set P(\\cdot)=|\\cdot|, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004)\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\boldsymbol{\\Lambda}_0 is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}} is the maximum likelihood estimator.\n\n\n3.3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability (Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n We generate \\boldsymbol{Y}\\in \\mathbb{R}^{{100}\\times 5} using common factor \\boldsymbol{f}\\in \\mathbb{R}^{{100}\\times 2} where each factor is generated randomly from standard normal distribution N(0,1). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is\n\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n which is neither precise nor sparse.\nSimulation code:\n\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model (Mardia, Kent, and Bibby 1979). We have \\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*} The test statistics is given by \\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}}) which has an asymptotic \\chi^2_s distribution with s=\\frac{1}{2}(p-k)^2-(p+k) by the property of MLE, where F is given by (Mardia, Kent, and Bibby (1979)) \nF(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})-\\log(\\det(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}))-p.\n Typically, the test is conducted at a 5\\% significant level. We can start the procedure from a very small k, say, k=1 or k=2 and then increase k until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#penalized-likelihood-method",
    "href": "chapter1.html#penalized-likelihood-method",
    "title": "3  Introduction",
    "section": "3.4 Penalized Likelihood Method",
    "text": "3.4 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above (Hirose and Yamamoto 2015). A penalized factor analytic model can be obtained by solving following optimization problem \n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}l_p := l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})-\\rho\\sum^p_{i=1}\\sum^k_{j=1}P(|\\boldsymbol{\\Lambda}_{ij}|).\n where we call l_p as the penalized likelihood, \\rho is called regularization parameter and we can treat P(\\cdot) as a penalized function. There are many types of penalized functions developed, such as LASSO (P(\\cdot)=|\\cdot|) and MC+ (P(|\\theta|;\\rho;\\gamma)=n(|\\theta|-\\frac{\\theta^2}{2\\rho\\gamma})I(|\\theta|&lt;\\rho\\gamma)+\\frac{\\rho^2\\gamma}{2}I(|\\theta|\\geq\\rho\\gamma)) (Hirose and Yamamoto 2015). In this article, we will mainly focus on the LASSO penalty.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "href": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "title": "3  Introduction",
    "section": "3.5 LASSO penalty and LASSO estimator",
    "text": "3.5 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}| and the LASSO estimator, denoted as (\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*), can be obtained via\n\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|.\n\\end{align*}\nAn E-M algorithm can be applied for evaluating the LASSO estimator.\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix",
    "href": "chapter1.html#appendix",
    "title": "1  Introduction",
    "section": "1.6 Appendix",
    "text": "1.6 Appendix\n\n\n\nTable 1.1: List of notations used in this report.\n\n\n\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\boldsymbol{A}^\\top\nthe transpose of the matrix (or vector) A\n\n\n\\mathbb{R}^{p}\nthe space of all p-dimensional real column vectors like [a_1,a_2,\\dots,a_p]^\\top\n\n\n\\mathbb{R}^{p\\times q}\nthe space of all real matrices with size p\\times q\n\n\n\\mathbb{E}[\\cdot]\nthe expectation, or mean of a random variable\n\n\n\\mathbb{V}[\\cdot]\nthe variance, or covariance of a random variable\n\n\n\\boldsymbol{Cov}[\\cdot , \\cdot]\nthe covariance of two random variables\n\n\n\\boldsymbol{0}_{p} and \\boldsymbol{0}_{p\\times q}\nthe p-dimensional 0 vector or 0 matrix in size p\\times q respectively\n\n\n\\boldsymbol{I}_p\nthe identity matrix in size p\\times p\n\n\n\\det(\\cdot)\nthe determinant of a matrix\n\n\nM(i,)\nthe i^{th} row of the matrix M\n\n\nM(,j)\nthe j^{th} row of the matrix M\n\n\n\n\n\n\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  The EM Algorithm",
    "section": "",
    "text": "2.1 A brief introduction about the EM algorithm\nThe Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data (Ng, Krishnan, and McLachlan 2012). As we mentioned, the key of maximum likelihood estimation is solving equation \n\\frac{\\partial}{\\partial \\beta}l=0.\n However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, augmented data(or complete) refers to the integration of this unobserved component, enhancing the algorithm’s ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete data sets effectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The EM Algorithm</span>"
    ]
  },
  {
    "objectID": "chapter2.html#the-e-step-and-m-step",
    "href": "chapter2.html#the-e-step-and-m-step",
    "title": "2  The EM Algorithm",
    "section": "2.2 The E-step and M-step",
    "text": "2.2 The E-step and M-step\nLet \\boldsymbol{x} denote the vector containing complete data, \\boldsymbol{y} denote the observed incomplete data and \\boldsymbol{z} denote the vector containing the missing data. Here ‘missing data’ is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later) (Ng, Krishnan, and McLachlan 2012). Also we assume \\boldsymbol{\\beta} as the parameter we want to estimate over the parameter space \\boldsymbol{\\Omega}.\nNow denote f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta}) as the probability density function (p.d.f.) of the random vector \\boldsymbol{X} corresponding to \\boldsymbol{x}. Then the complete-data log-likelihood function when complete data is fully observed can be given by\n\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta})=\\log f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta}).\nThe EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of \\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta}). But it is unobservable since it includes missing part of the data, then we use the conditional expectation given \\boldsymbol{y} and current fit for \\boldsymbol{\\beta}.\nOn the (k+1)^th iteration, we have \n\\begin{align*}\n&\\text{E-step: Compute } Q(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}):=\\mathbb{E}_{\\boldsymbol{X}}[\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\\\\n&\\text{M-step: Update }\\boldsymbol{\\beta}^{(k+1)} \\text{ as }\\boldsymbol{{\\beta}}^{(k+1)}:=\\text{arg}\\max_{\\boldsymbol{\\beta}} Q(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}) \\text{ (Original EM)}\\\\\n& \\text{ Or update }\\boldsymbol{{\\beta}}^{(k+1)} \\text{ such that } Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})\\geq Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)})\\text{ (Generalized EM)}.\n\\end{align*}\n We keep iterating between E-step and M-step until convergence, which may be determined by a criteria such as ||\\boldsymbol{\\beta}^{(k+1)}-\\boldsymbol{\\beta}^{(k)}||_p\\leq \\epsilon for some p-norm ||\\cdot||_p and positive \\epsilon.\nMeanwhile, the M-step in both original and generalized algorithms defines a mapping from the parameter space \\boldsymbol{\\Omega} to itself by \n\\begin{align*}\nM: \\boldsymbol{\\Omega} &\\to \\boldsymbol{\\Omega}\\\\\n   \\boldsymbol{\\beta}^{(k)} &\\mapsto \\boldsymbol{\\beta}^{(k+1)}.\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The EM Algorithm</span>"
    ]
  },
  {
    "objectID": "chapter2.html#convergence-of-the-em-algorithm",
    "href": "chapter2.html#convergence-of-the-em-algorithm",
    "title": "2  The EM Algorithm",
    "section": "2.3 Convergence of the EM Algorithm",
    "text": "2.3 Convergence of the EM Algorithm\nBy the definition of conditional likelihood, our likelihood of complete data can be expressed by \nL_{\\boldsymbol{X}}(\\boldsymbol{\\beta}) = f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta})=L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta})f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}),\n and hence the log-likelihood is given by \n\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta}) = \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}) + \\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}).\n Take expectation to both sides of the equation with respect to \\boldsymbol{x|y} and replace \\boldsymbol{\\beta} by \\boldsymbol{\\beta}^{(k)}, we will have \nQ(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}) = \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}) + \\mathbb{E}_{\\boldsymbol{X}}[\\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}].\n Now consider the difference of log-likelihood of \\boldsymbol{Y} function between two iterations, we have \n\\begin{align*}\n  \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}^{(k+1)})-\\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}^{(k)}) =\n  &\\{Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})-Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)})\\}\\\\\n  &-\\{\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k+1)})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\\\\n  &-\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k)}|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\}.\n\\end{align*}\n By the procedure of EM-algorithm, we always have Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})\\geq Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)}). By the Jensen’s inequality, we have \\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k+1)})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}] \\leq \\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k)}|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]. Therefore during iterations, the log-likelihood of observed data \\boldsymbol{Y} keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.\n\n\n\n\nNg, S. K., T. Krishnan, and G. J. McLachlan. 2012. “The EM Algorithm.” In Handbook of Computational Statistics: Concepts and Methods, 139–72. Springer.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The EM Algorithm</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "",
    "text": "3.1 Model Setup\nSuppose we have the n centralized observations, \\{\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n\\}, where \\boldsymbol{y}_j\\in \\mathbb{R}^p and hence the mean, \\boldsymbol{\\mu}_j=\\boldsymbol{0}_p (j=1,2,\\dots,n). For each of the observation \\boldsymbol{y}_j, the common factor and unique factor are \\boldsymbol{f}_j and \\boldsymbol{\\epsilon}_j respectively. Denote the response matrix as \\boldsymbol{Y}=[\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n]^\\top, the common factor matrix as \\boldsymbol{F}=[\\boldsymbol{f}_1,\\boldsymbol{f}_2,\\dots,\\boldsymbol{f}_n]^\\top, and the unique factor matrix as \\boldsymbol{\\hat\\epsilon}=[\\boldsymbol{\\epsilon}_1,\\boldsymbol{\\epsilon}_2,\\dots,\\boldsymbol{\\epsilon}_n]^\\top. Then the model can be written as \\boldsymbol{Y}=\\boldsymbol{F}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\hat\\epsilon}.\nWe also assume",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "chapter3.html#model-setup",
    "href": "chapter3.html#model-setup",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "",
    "text": "\\boldsymbol{\\epsilon}_j follows a normal distribution with mean 0 and variance \\boldsymbol{\\Psi} for j=1,2,\\dots,n, where \\boldsymbol{\\Psi} is a diagonal matrix defined by \\boldsymbol{\\Psi}=diag(\\psi_{11},\\psi_{22},\\dots,\\psi_{pp}).\n\\boldsymbol{f}_j follows a normal distribution with mean 0 and variance \\boldsymbol{I}_k for j=1,2,\\dots,n.\n\\boldsymbol{y}_j follows a normal distribution with mean 0 and variance \\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi} for j=1,2,\\dots,n. Those \\boldsymbol{y}_j are pairwisely independent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "chapter3.html#the-em-algorithm",
    "href": "chapter3.html#the-em-algorithm",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "3.2 The EM Algorithm",
    "text": "3.2 The EM Algorithm\nWe treat the common factor matrix \\boldsymbol{F} as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of (\\boldsymbol{Y},\\boldsymbol{F}) given \\boldsymbol{Y} and (\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}), where (\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}) is the parameter we got in the k-th iteration (k&gt;1) and (\\boldsymbol{\\Lambda}_{(0)},\\boldsymbol{\\Psi}_{(0)}) is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get (\\boldsymbol{\\Lambda}_{(k+1)},\\boldsymbol{\\Psi}_{(k+1)}).\n\n3.2.1 E-Step\nFirst, the joint likelihood of (\\boldsymbol{Y},\\boldsymbol{F}), denoted as L_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}), is given by\n\n\\begin{align*}\nL_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=&\\prod_{i=1}^nf(\\boldsymbol{y}_i,\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nf(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nN(\\boldsymbol{y}_i;\\boldsymbol{\\Lambda}\\boldsymbol{f}_i,\\boldsymbol{\\Psi})N(\\boldsymbol{f}_i;\\boldsymbol{0}_k,\\boldsymbol{I}_k)\\\\\n=&\\prod_{i=1}^n[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Psi})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}\\\\\n&(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\}][(2\\pi)^{-\\frac{k}{2}}\\det({\\boldsymbol{I}_k})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}\\boldsymbol{f}_i^\\top\\boldsymbol{I}_k^{-1}\\boldsymbol{f}_i\\}]\\\\\n= &(2\\pi)^{-\\frac{n}{2}(p+k)}(\\prod_{i=1}^p\\psi_{jj})^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(y_{ij}-\\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i)^2}{\\psi_{jj}}\\}\\\\\n&\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\}.\n\\end{align*}\n\nFurthermore, the log-likelihood, denoted as l_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}), is given by\n\nl_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(y_{ij}-\\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i)^2}{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i+\\text{constant}.\n\\tag{3.1}\nNow let us deduce the conditional expectation to \\boldsymbol{F} given \\boldsymbol{Y},\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}, denoted as El_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}). In Equation 3.1, the first term is independent on \\boldsymbol{F}, hence stay the same under conditional expectation. The last term is independent on (\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}), therefore we can regard it as a constant in El_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}). We only need to consider the term inside the sum in the second term, more specifically, the numerator, we have\n\n(y_{ij}-\\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i)^2 = y_{ij}^2 - 2 y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i + \\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top \\boldsymbol{\\Lambda}^\\top(,j).\n\nWithout ambiguity, denote \\mathbb{E}[\\boldsymbol{f}_i|_{(k)}] to be the conditional expectation \\mathbb{E}[\\boldsymbol{f}_i|\\boldsymbol{Y},\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}] for simplification. Then the conditional expectation El_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) is given by\n\n\\begin{align*}\nEl_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\ \\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2 - 2 y_{ij}{\\boldsymbol{\\Lambda}}(j,)\\mathbb{E}[\\boldsymbol{f}_i|_{(k)}]\\boldsymbol+\\boldsymbol{\\Lambda}(j,) \\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}^\\top_i|_{(k)}]\\boldsymbol{\\Lambda}^\\top(,j)}{\\psi_{jj}}\\\\\n\\end{align*}\n\nTo deal with \\mathbb{E}[\\boldsymbol{f}_i|_{(k)}] and \\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(k)}], we only need to know the mean and variance of conditional distribution \\boldsymbol{f}_i|\\boldsymbol{Y},\\boldsymbol{\\Lambda}^\\top_{(k)},\\boldsymbol{\\Psi}_{(k)}, or equivalently \\boldsymbol{f}_i|\\boldsymbol{y}_i,\\boldsymbol{\\Lambda}^\\top_{(k)},\\boldsymbol{\\Psi}_{(k)} because of the independency of \\boldsymbol{f}_i to \\boldsymbol{y}_j for (i\\neq j). This is because we can always treat \\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(k)}] as\n\n\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(k)}]= \\mathbb{V}[\\boldsymbol{f}_i|_{(k)}]+ \\mathbb{E}[\\boldsymbol{f}_i|_{(k)}]\\mathbb{E}[\\boldsymbol{f}_i|_{(k)}]^\\top.\n where \\mathbb{V}[\\boldsymbol{f}_i|_{(k)}] is the variance of conditional distribution. We have\n\nLemma 3.1 If \\boldsymbol{\\alpha}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}},\\boldsymbol{\\Sigma_{\\boldsymbol{\\alpha}}}) and \\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}), then we have \\boldsymbol{\\alpha}|\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}), where \n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}}|\\boldsymbol{\\beta}=\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}}+\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}\\boldsymbol{\\beta}\\\\\n& \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}= \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}}-\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]^\\top.\n\\end{align*}\n\n\nIn our scenario, we have\n\n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{f}_i}= \\boldsymbol{0}_k, \\boldsymbol{\\Sigma}_{\\boldsymbol{f}_{i}}=\\boldsymbol{I}_k\\\\\n& \\boldsymbol{\\mu}_{\\boldsymbol{y}_i}= \\boldsymbol{0}_p, \\boldsymbol{\\Sigma}_{\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{align*}\n\nand also\n\n\\boldsymbol{Cov}(\\boldsymbol{y_i},\\boldsymbol{f_i})=\\boldsymbol{Cov}(\\boldsymbol{\\Lambda f}_i+\\boldsymbol{\\epsilon}_i,\\boldsymbol{f}_i)=\\boldsymbol{\\Lambda}^\\top.\n\nTherefore we have\n\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(k)})=\\boldsymbol{\\mu}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}_{(k)}^\\top(\\boldsymbol{\\Lambda}_{(k)}\\boldsymbol{\\Lambda}_{(k)}^\\top+\\boldsymbol{\\Psi}_{(k)})^{-1}\\boldsymbol{y}_i\\\\\n&\\mathbb{V}(\\boldsymbol{f}_i|_{(k)})=\\boldsymbol{\\Sigma}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(k)}^\\top(\\boldsymbol{\\Lambda}_{(k)}\\boldsymbol{\\Lambda}_{(k)}^\\top+\\boldsymbol{\\Psi}_{(k)})^{-1}\\boldsymbol{\\Lambda}_{(k)}.\\\\\n\\end{align*}\n\nFor simplification, let us denote\n\n\\begin{align*}\n&\\boldsymbol{A}:=\\boldsymbol{\\Lambda}_{(k)}^\\top(\\boldsymbol{\\Lambda}_{(k)}\\boldsymbol{\\Lambda}_{(k)}^\\top+\\boldsymbol{\\Psi}_{(k)})^{-1}\\\\\n&\\boldsymbol{B}:=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(k)}^\\top(\\boldsymbol{\\Lambda}_{(k)}\\boldsymbol{\\Lambda}_{(k)}^\\top+\\boldsymbol{\\Psi}_{(k)})^{-1}\\boldsymbol{\\Lambda}_{(k)},\\\\\n\\end{align*}\n\nwe will get\n\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(k)})= \\boldsymbol{A}\\boldsymbol{y}_i\\\\\n&\\mathbb{E}(\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(k)})= \\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top.\n\\end{align*}\n\nOur expectation will finally be confirmed by\n\n\\begin{align*}\nEl_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})= & -\\frac{n}{2}\\sum_{j=1}^p\\log{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\Psi_{jj}}+\\text{constant}.\n\\end{align*}\n\n\n\n3.2.2 M-step\nIn M-step, we need to maximize so called Q-function with respect to parameters where Q-function is penalized conditional expectation of the log-likelihood, i.e. \nQ(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) = El_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) - \\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\n We add a coefficient \\frac{1}{2} before P_{\\rho}(\\boldsymbol{\\Lambda}) for simplification since we notice that the same coefficient occurs in each term of conditional expectation El_{(k)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}). When execute M-step, we use the following strategy(Ng, Krishnan, and McLachlan 2012):\n\nFind \\boldsymbol{\\Psi}_{(k+1)} using current \\boldsymbol{\\Lambda}_{(k)}, i.e. \n  \\boldsymbol{\\Psi}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Psi}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(k)}).\n\nFind \\boldsymbol{\\Lambda}_{(k+1)} using \\boldsymbol{\\Psi}_{(k+1)} we got in previous step, i.e.  \n  \\boldsymbol{\\Lambda}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(k+1)}).\n\n\nFor step 1, take partial derivative with respect to each \\psi_{jj} and let it equal to zero to find the local maximizer of \\psi_{jj}, i.e. \n\n\\frac{\\partial}{\\partial \\psi_{jj}}Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(k)})=0.\n By simple calculation, we will have\n\n\\begin{align*}\n\\frac{\\partial}{\\partial \\psi_{jj}}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(k)})=\n&-\\frac{n}{2}\\frac{1}{\\psi_{jj}}+\\frac{1}{2}\\sum_{i=1}^n\n\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}_{(k)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj}^2} \\\\\n&+ \\frac{1}{2}\\sum_{i=1}^n\\frac{\\boldsymbol{\\Lambda}_{(k)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(k)}(j,)^\\top}{\\psi_{jj}^2}.\\\\\n\\end{align*}\n\nThus the update of \\boldsymbol{\\Psi} will be elementwisely given by\n\n\\begin{equation}\n\\psi_{jj,(k+1)}=\\frac{1}{n}\\sum_{i=1}^n y_{ij}^2-\\frac{2}{n}\\sum_{i=1}^ny_{ij}\\boldsymbol{\\Lambda}_{(k)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i + \\frac{1}{n}\\sum_{i=1}^n \\boldsymbol{\\Lambda}_{(k)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(k)}(j,)^\\top.\n\\end{equation}\n\nFor step 2, let us revise the update formula, we have\n\\begin{align*}\n\\boldsymbol{\\Lambda}_{(k+1)}=&\\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda,\\boldsymbol{\\Psi}})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(k+1)})\\\\\n=&\\arg \\max_{\\boldsymbol{\\Lambda}} \\{\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}\\\\\n&-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}}-\\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\\\\\n&-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj,(k+1)}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2}{\\psi_{jj,(k+1)}}+\\text{constant}\\}.\\\\\n\\end{align*}\nSince the last three terms do not contain any \\boldsymbol{\\Lambda}, so they can be eliminated. After letting P_\\rho(\\boldsymbol{\\Lambda}):=\\rho\\sum_{j=1}^p\\sum_{i=1}^k|\\lambda_{ji}|=\\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1 as LASSO, we can rewrite it as\n$$ \\begin{align*}\n\\boldsymbol{\\Lambda}_{(k+1)}\n\n=& \\arg \\min_{\\boldsymbol{\\Lambda}}\\{-\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}\\\\\n\n& + \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}} + \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n  \\}\n\\end{align*} $$\nNotice that the objective function with respect to \\boldsymbol{\\Lambda}(j,) for any given j=1,2,\\dots,p is convex, and due to non-differentiablity of the L_1-norm at certain points, a subgradient approach can be necessary to optimize \\boldsymbol{\\Lambda} row by row.\nDenote the objective function as g(\\boldsymbol{\\Lambda}), the subdifferential of g(\\boldsymbol{\\Lambda}) with respect to some given j=1,2,\\dots,p is given by \n\\begin{align*}\n\\partial_{(j)}g(\\boldsymbol{\\Lambda})&=-\\sum_{i=1}^n\\frac{2y_{ij}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}+\\sum_{i=1}^n\\frac{(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top+\\boldsymbol{B}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}}+\\rho \\partial||\\boldsymbol{\\Lambda}(j,)^\\top||_1\\\\\n&=\\sum_{i=1}^n\\frac{2y_{ij}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}+\\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}}+\\rho \\partial||\\boldsymbol{\\Lambda}(j,)^\\top||_1\\\\\n\\end{align*}\n We have\n\nLemma 3.2 For the set \\partial||\\boldsymbol{\\Lambda}(j,)||_1, we have \n\\text{sign}(\\boldsymbol{\\Lambda}(j,)^\\top)\\in \\partial||\\boldsymbol{\\Lambda}(j,)^\\top||_1,\n where the sign function is given by \\text{sign}(x)=\n\\begin{cases}\n1 &\\text{if } x&gt;0\\\\\n0 &\\text{if } x=0\\\\\n-1 &\\text{if } x&lt;0\\\\\n\\end{cases} elementwisely.\n\nTherefore a iterative subgradient method for finding \\boldsymbol{\\Lambda}_{(k+1)} rowwisely given by \\boldsymbol{\\Lambda}_{(k)} is given by\n\n\n\n\n\n\nalgorithm\n\n\n\nFor each j=1,2,...,p, denote \\boldsymbol{\\Lambda}_{(k+1)}^{(l)}(j,) as the l^{th} iteration when executing the subgradient method to find the j^{th} row of \\boldsymbol{\\Lambda}_{(k+1)}, we iterate as following\n\nSet \\boldsymbol{\\Lambda}_{(k+1)}^{(0)}(j,):=\\boldsymbol{\\Lambda}_{(k)}(j,).\nFor l\\geq 1, calculate \n\\partial_{(j)}g(\\boldsymbol{\\Lambda}^{(l)})=-\\sum_{i=1}^n\\frac{2y_{ij}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}+\\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}^{(l)}(j,)^\\top}{\\psi_{jj,(k+1)}}+\\rho \\text{ sign}(\\boldsymbol{\\Lambda}^{(l)}(j,)^\\top) .\n\nUpdate \\boldsymbol{\\Lambda}_{(k+1)}^{(l+1)}(j,) as \n\\boldsymbol{\\Lambda}_{(k+1)}^{(l+1)}(j,)=\\boldsymbol{\\Lambda}_{(k+1)}^{(l)}(j,)-t^{(l)}[\\partial_{(j)}g(\\boldsymbol{\\Lambda}^{(l)})]^\\top\n where t^{(l)} is the step size in the l^{th} iteration and a widely used choice is letting t^{(l)}=\\frac{1}{(l+1)||[\\partial_{(j)}g(\\boldsymbol{\\Lambda}^{(l)})]^\\top||_2}, where ||\\cdot||_2 is the L_2 norm.(Boyd, Xiao, and Mutapcic (2003))\nRepeat step 3 and 4 until converge.\n\n\n\nthe EM-Algorithm for LASSO FA\n\n\n\n\nBoyd, S., L. Xiao, and A. Mutapcic. 2003. “Subgradient Methods.” Lecture notes of EE392o, Stanford University, Autumn Quarter, 2004(01).\n\n\nNg, S. K., T. Krishnan, and G. J. McLachlan. 2012. “The EM Algorithm.” In Handbook of Computational Statistics: Concepts and Methods, 139–72. Springer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Multivariate data, where each observational unit consists of multiple outcomes, are abundant across multitude of disciplines (e.g. in psychology, multiple test scores from different courses for a student; in agriculture, yield measurements at different environments for the same crop variety; and in biology, gene expressions from genes of the same organism). A key analysis of multivariate data include understanding or predicting the characteristics of observational units. A simple analysis may assess one outcome at a time, however, a more sophisticated form of an analysis would consider multiple outcomes simultaneously and exploit the correlated structure within the data. While the latter analysis would be more desirable to make the most out of the data, there are several challenges as explained next.\nWe represent multivariate data as a n \\times p matrix, \\textbf{Y}, where n \\in \\mathbb{Z}^+ is the number of individuals and p \\in \\mathbb{Z}^+ is the number of dependent variables.\nNotation used throughout this thesis is presented in Table 1.1.\n\n\n\nTable 1.1: List of notations used in this report.\n\n\n\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\boldsymbol{A}^\\top\nthe transpose of the matrix (or vector) A\n\n\n\\mathbb{R}^{p}\nthe space of all p-dimensional real column vectors like [a_1,a_2,\\dots,a_p]^\\top\n\n\n\\mathbb{R}^{p\\times q}\nthe space of all real matrices with size p\\times q\n\n\n\\mathbb{E}[\\cdot]\nthe expectation, or mean of a random variable\n\n\n\\mathbb{V}[\\cdot]\nthe variance, or covariance of a random variable\n\n\n\\boldsymbol{Cov}[\\cdot , \\cdot]\nthe covariance of two random variables\n\n\n\\boldsymbol{0}_{p} and \\boldsymbol{0}_{p\\times q}\nthe p-dimensional 0 vector or 0 matrix in size p\\times q respectively\n\n\n\\boldsymbol{I}_p\nthe identity matrix in size p\\times p\n\n\n\\det(\\cdot)\nthe determinant of a matrix\n\n\nM(i,)\nthe i^{th} row of the matrix M\n\n\nM(,j)\nthe j^{th} row of the matrix M\n\n\n\n\n\n\nThis thesis is structured as follows. Chapter 2 outlines the background information on factor analytics models, EM algorithm, and penalised likelihood estimation. Chapter 3 describe the methods and simulation settings. Chapter 4 presents the results from the simulation with discussion.\nThis thesis is written reproducibly using the Quarto system and all the code to reproduce the thesis and results can be found at https://github.com/ZhiningW/Master_Project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#indeterminacy-of-the-loading-matrix",
    "href": "intro.html#indeterminacy-of-the-loading-matrix",
    "title": "1  Introduction",
    "section": "1.2 Indeterminacy of the loading matrix",
    "text": "1.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\boldsymbol{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon} where the matrix \\boldsymbol{M} is orthogonal and simultaneously the variance of \\boldsymbol{y} given by (2) still holds, since \\mathbb{V}[\\boldsymbol{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Therefore a rotated loading matrix \\boldsymbol{\\Lambda}\\boldsymbol{M} is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "href": "intro.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "title": "1  Introduction",
    "section": "1.3 Traditional Estimation of Parameters in Factor Analytic Models",
    "text": "1.3 Traditional Estimation of Parameters in Factor Analytic Models\nWe denote the set of parameters by \\beta := \\{\\text{vec}(\\boldsymbol{\\Lambda}),\\text{vec}(\\boldsymbol{\\Psi})\\} where \\text{vec}(\\cdot) is the vectorisation of the input.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n1.3.1 Maximum Likelihood Estimation\nSuppose we have n independent and identically distributed observations \\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_N from a p-dimensional multi-variate normal distribution N_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) and by our hypothesis, we have \\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Then the likelihood function is given by L(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu}))\\right]. and hence the log-likelihood is given by\n\\begin{align*}\nl(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})]\\\\\n=& -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})],\n\\end{align*}\nwhere \\boldsymbol{S} is the sample covariance defined as \\boldsymbol{S}:=\\frac{1}{n}\\sum^n_{i=1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top. To get the MLE of parameters, we seek the roots of equation system\n\\begin{cases}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0.\n\\end{cases}\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin (Rubin and Thayer 1982).\n\n\n1.3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose Q(\\boldsymbol{\\Lambda}) is an criterion for \\boldsymbol{\\Lambda} in the rotation procedure, and we may express it as Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij}) where P(\\cdot) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set P(\\cdot)=|\\cdot|, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004)\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\boldsymbol{\\Lambda}_0 is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}} is the maximum likelihood estimator.\n\n\n1.3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability (Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n We generate \\boldsymbol{Y}\\in \\mathbb{R}^{{100}\\times 5} using common factor \\boldsymbol{f}\\in \\mathbb{R}^{{100}\\times 2} where each factor is generated randomly from standard normal distribution N(0,1). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is\n\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n which is neither precise nor sparse.\nSimulation code:\n\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model (Mardia, Kent, and Bibby 1979). We have \\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*} The test statistics is given by \\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}}) which has an asymptotic \\chi^2_s distribution with s=\\frac{1}{2}(p-k)^2-(p+k) by the property of MLE, where F is given by (Mardia, Kent, and Bibby (1979)) \nF(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})-\\log(\\det(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}))-p.\n Typically, the test is conducted at a 5\\% significant level. We can start the procedure from a very small k, say, k=1 or k=2 and then increase k until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#penalized-likelihood-method",
    "href": "intro.html#penalized-likelihood-method",
    "title": "1  Introduction",
    "section": "1.4 Penalized Likelihood Method",
    "text": "1.4 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above (Hirose and Yamamoto 2015). A penalized factor analytic model can be obtained by solving following optimization problem \n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}l_p := l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})-\\rho\\sum^p_{i=1}\\sum^k_{j=1}P(|\\boldsymbol{\\Lambda}_{ij}|).\n where we call l_p as the penalized likelihood, \\rho is called regularization parameter and we can treat P(\\cdot) as a penalized function. There are many types of penalized functions developed, such as LASSO (P(\\cdot)=|\\cdot|) and MC+ (P(|\\theta|;\\rho;\\gamma)=n(|\\theta|-\\frac{\\theta^2}{2\\rho\\gamma})I(|\\theta|&lt;\\rho\\gamma)+\\frac{\\rho^2\\gamma}{2}I(|\\theta|\\geq\\rho\\gamma)) (Hirose and Yamamoto 2015). In this article, we will mainly focus on the LASSO penalty.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#lasso-penalty-and-lasso-estimator",
    "href": "intro.html#lasso-penalty-and-lasso-estimator",
    "title": "1  Introduction",
    "section": "1.5 LASSO penalty and LASSO estimator",
    "text": "1.5 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}| and the LASSO estimator, denoted as (\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*), can be obtained via\n\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|.\n\\end{align*}\nAn E-M algorithm can be applied for evaluating the LASSO estimator.\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Factor analytic model\nFactor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables (Mardia, Kent, and Bibby 1979). It provides a useful tool for exploring the covariance structure among observable variables (Hirose and Yamamoto 2015). One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are (Mardia, Kent, and Bibby 1979).\nSuppose we have a observable random vector \\boldsymbol{y}\\in \\mathbb{R}^p with mean \\mathbb{E}[\\boldsymbol{y}]=\\boldsymbol{\\mu} and variance \\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Sigma}. Then a k-order factor analysis model for \\boldsymbol{y} can be given by \\begin{equation}\n\\boldsymbol{y}=\\boldsymbol{\\Lambda} \\boldsymbol{f}+\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon},\n\\end{equation} where \\boldsymbol{\\Lambda} \\in \\mathbb{R}^{p \\times k} is called loading matrix, we call \\boldsymbol{f} \\in \\mathbb{R}^{k} as common factors and \\boldsymbol{\\epsilon} \\in \\mathbb{R}^{p} is unique factors. To make the model well-defined, we may assume \\mathbb{E}[\\boldsymbol{f}]=\\boldsymbol{0}_k, \\mathbb{V}[\\boldsymbol{f}]=\\boldsymbol{I}_{k\\times k}, \\mathbb{E}[\\boldsymbol{\\epsilon}]=\\boldsymbol{0}_p, \\mathbb{V}[\\boldsymbol{\\epsilon}]=:\\boldsymbol{\\Psi}=\\text{diag}(\\Psi_{11},\\dots,\\Psi_{pp}) and also the independence between any elements from \\boldsymbol{f} and \\boldsymbol{\\epsilon} separately, i.e. Cov[f_i,\\epsilon_j]=0, \\text{for all } i\\in\\{1,2,\\dots,k\\} \\text{ and } j \\in \\{1,2,\\dots,p\\} Straightforwardly, the covariance of observable vector \\boldsymbol{y} can be modelled by\n\\begin{equation}\n\\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{equation}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#indeterminacy-of-the-loading-matrix",
    "href": "background.html#indeterminacy-of-the-loading-matrix",
    "title": "2  Background",
    "section": "2.2 Indeterminacy of the loading matrix",
    "text": "2.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\boldsymbol{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon} where the matrix \\boldsymbol{M} is orthogonal and simultaneously the variance of \\boldsymbol{y} given by (2) still holds, since \\mathbb{V}[\\boldsymbol{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Therefore a rotated loading matrix \\boldsymbol{\\Lambda}\\boldsymbol{M} is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "href": "background.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "title": "2  Background",
    "section": "2.3 Traditional Estimation of Parameters in Factor Analytic Models",
    "text": "2.3 Traditional Estimation of Parameters in Factor Analytic Models\nWe denote the set of parameters by \\beta := \\{\\text{vec}(\\boldsymbol{\\Lambda}),\\text{vec}(\\boldsymbol{\\Psi})\\} where \\text{vec}(\\cdot) is the vectorisation of the input.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n2.3.1 Maximum Likelihood Estimation\nSuppose we have n independent and identically distributed observations \\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_N from a p-dimensional multi-variate normal distribution N_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) and by our hypothesis, we have \\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Then the likelihood function is given by L(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu}))\\right]. and hence the log-likelihood is given by\n\\begin{align*}\nl(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})]\\\\\n=& -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})],\n\\end{align*}\nwhere \\boldsymbol{S} is the sample covariance defined as \\boldsymbol{S}:=\\frac{1}{n}\\sum^n_{i=1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top. To get the MLE of parameters, we seek the roots of equation system\n\\begin{cases}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0.\n\\end{cases}\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin (Rubin and Thayer 1982).\n\n\n2.3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose Q(\\boldsymbol{\\Lambda}) is an criterion for \\boldsymbol{\\Lambda} in the rotation procedure, and we may express it as Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij}) where P(\\cdot) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set P(\\cdot)=|\\cdot|, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004)\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\boldsymbol{\\Lambda}_0 is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}} is the maximum likelihood estimator.\n\n\n2.3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability (Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n We generate \\boldsymbol{Y}\\in \\mathbb{R}^{{100}\\times 5} using common factor \\boldsymbol{f}\\in \\mathbb{R}^{{100}\\times 2} where each factor is generated randomly from standard normal distribution N(0,1). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is\n\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n which is neither precise nor sparse.\nSimulation code:\n\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model (Mardia, Kent, and Bibby 1979). We have \\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*} The test statistics is given by \\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}}) which has an asymptotic \\chi^2_s distribution with s=\\frac{1}{2}(p-k)^2-(p+k) by the property of MLE, where F is given by (Mardia, Kent, and Bibby (1979)) \nF(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})-\\log(\\det(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}))-p.\n Typically, the test is conducted at a 5\\% significant level. We can start the procedure from a very small k, say, k=1 or k=2 and then increase k until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#penalized-likelihood-method",
    "href": "background.html#penalized-likelihood-method",
    "title": "2  Background",
    "section": "2.4 Penalized Likelihood Method",
    "text": "2.4 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above (Hirose and Yamamoto 2015). A penalized factor analytic model can be obtained by solving following optimization problem \n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}l_p := l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})-\\rho\\sum^p_{i=1}\\sum^k_{j=1}P(|\\boldsymbol{\\Lambda}_{ij}|).\n where we call l_p as the penalized likelihood, \\rho is called regularization parameter and we can treat P(\\cdot) as a penalized function. There are many types of penalized functions developed, such as LASSO (P(\\cdot)=|\\cdot|) and MC+ (P(|\\theta|;\\rho;\\gamma)=n(|\\theta|-\\frac{\\theta^2}{2\\rho\\gamma})I(|\\theta|&lt;\\rho\\gamma)+\\frac{\\rho^2\\gamma}{2}I(|\\theta|\\geq\\rho\\gamma)) (Hirose and Yamamoto 2015). In this article, we will mainly focus on the LASSO penalty.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#lasso-penalty-and-lasso-estimator",
    "href": "background.html#lasso-penalty-and-lasso-estimator",
    "title": "2  Background",
    "section": "2.5 LASSO penalty and LASSO estimator",
    "text": "2.5 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}| and the LASSO estimator, denoted as (\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*), can be obtained via\n\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|.\n\\end{align*}\nAn E-M algorithm can be applied for evaluating the LASSO estimator.\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#parameter-estimation",
    "href": "background.html#parameter-estimation",
    "title": "2  Background",
    "section": "2.2 Parameter Estimation",
    "text": "2.2 Parameter Estimation\nWe denote the set of parameters by \\beta := \\{\\text{vec}(\\boldsymbol{\\Lambda}),\\text{vec}(\\boldsymbol{\\Psi})\\} where \\text{vec}(\\cdot) is the vectorisation of the input.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n2.2.1 Maximum Likelihood Estimation\nSuppose we have n independent and identically distributed observations \\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_N from a p-dimensional multi-variate normal distribution N_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) and by our hypothesis, we have \\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Then the likelihood function is given by L(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu}))\\right]. and hence the log-likelihood is given by\n\\begin{align*}\nl(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})]\\\\\n=& -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})],\n\\end{align*}\nwhere \\boldsymbol{S} is the sample covariance defined as \\boldsymbol{S}:=\\frac{1}{n}\\sum^n_{i=1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top. To get the MLE of parameters, we seek the roots of equation system\n\\begin{cases}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0.\n\\end{cases}\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin (Rubin and Thayer 1982).\n\n\n2.2.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose Q(\\boldsymbol{\\Lambda}) is an criterion for \\boldsymbol{\\Lambda} in the rotation procedure, and we may express it as Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij}) where P(\\cdot) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set P(\\cdot)=|\\cdot|, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004)\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\boldsymbol{\\Lambda}_0 is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\nwhere \\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}} is the maximum likelihood estimator.\n\n\n2.2.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability (Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n We generate \\boldsymbol{Y}\\in \\mathbb{R}^{{100}\\times 5} using common factor \\boldsymbol{f}\\in \\mathbb{R}^{{100}\\times 2} where each factor is generated randomly from standard normal distribution N(0,1). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is\n\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n which is neither precise nor sparse.\nSimulation code:\n\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model (Mardia, Kent, and Bibby 1979). We have \\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*} The test statistics is given by \\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}}) which has an asymptotic \\chi^2_s distribution with s=\\frac{1}{2}(p-k)^2-(p+k) by the property of MLE, where F is given by (Mardia, Kent, and Bibby (1979)) \nF(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})-\\log(\\det(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}))-p.\n Typically, the test is conducted at a 5\\% significant level. We can start the procedure from a very small k, say, k=1 or k=2 and then increase k until the null hypothesis is not rejected.\n\n\n2.2.4 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above (Hirose and Yamamoto 2015). A penalized factor analytic model can be obtained by solving following optimization problem \n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}l_p := l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})-\\rho\\sum^p_{i=1}\\sum^k_{j=1}P(|\\boldsymbol{\\Lambda}_{ij}|).\n where we call l_p as the penalized likelihood, \\rho is called regularization parameter and we can treat P(\\cdot) as a penalized function. There are many types of penalized functions developed, such as LASSO (P(\\cdot)=|\\cdot|) and MC+ (P(|\\theta|;\\rho;\\gamma)=n(|\\theta|-\\frac{\\theta^2}{2\\rho\\gamma})I(|\\theta|&lt;\\rho\\gamma)+\\frac{\\rho^2\\gamma}{2}I(|\\theta|\\geq\\rho\\gamma)) (Hirose and Yamamoto 2015). In this article, we will mainly focus on the LASSO penalty.\n\n\n2.2.5 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}| and the LASSO estimator, denoted as (\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*), can be obtained via\n\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|.\n\\end{align*}\nAn E-M algorithm can be applied for evaluating the LASSO estimator.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#factor-analytic-model",
    "href": "background.html#factor-analytic-model",
    "title": "2  Background",
    "section": "",
    "text": "Indeterminacy of the loading matrix\n\n\n\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\boldsymbol{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon} where the matrix \\boldsymbol{M} is orthogonal and simultaneously the variance of \\boldsymbol{y} given by (2) still holds, since \\mathbb{V}[\\boldsymbol{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}. Therefore a rotated loading matrix \\boldsymbol{\\Lambda}\\boldsymbol{M} is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-EM",
    "href": "background.html#sec-EM",
    "title": "2  Background",
    "section": "2.3 The EM Algorithm",
    "text": "2.3 The EM Algorithm\nThe Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data (Ng, Krishnan, and McLachlan 2012). As we mentioned, the key of maximum likelihood estimation is solving equation \n\\frac{\\partial}{\\partial \\beta}l=0.\n However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, augmented data(or complete) refers to the integration of this unobserved component, enhancing the algorithm’s ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete data sets effectively.\n\n2.3.1 The E-step and M-step\nLet \\boldsymbol{x} denote the vector containing complete data, \\boldsymbol{y} denote the observed incomplete data and \\boldsymbol{z} denote the vector containing the missing data. Here ‘missing data’ is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later) (Ng, Krishnan, and McLachlan 2012). Also we assume \\boldsymbol{\\beta} as the parameter we want to estimate over the parameter space \\boldsymbol{\\Omega}.\nNow denote f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta}) as the probability density function (p.d.f.) of the random vector \\boldsymbol{X} corresponding to \\boldsymbol{x}. Then the complete-data log-likelihood function when complete data is fully observed can be given by\n\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta})=\\log f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta}).\nThe EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of \\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta}). But it is unobservable since it includes missing part of the data, then we use the conditional expectation given \\boldsymbol{y} and current fit for \\boldsymbol{\\beta}.\nOn the (k+1)^th iteration, we have \n\\begin{align*}\n&\\text{E-step: Compute } Q(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}):=\\mathbb{E}_{\\boldsymbol{X}}[\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\\\\n&\\text{M-step: Update }\\boldsymbol{\\beta}^{(k+1)} \\text{ as }\\boldsymbol{{\\beta}}^{(k+1)}:=\\text{arg}\\max_{\\boldsymbol{\\beta}} Q(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}) \\text{ (Original EM)}\\\\\n& \\text{ Or update }\\boldsymbol{{\\beta}}^{(k+1)} \\text{ such that } Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})\\geq Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)})\\text{ (Generalized EM)}.\n\\end{align*}\n We keep iterating between E-step and M-step until convergence, which may be determined by a criteria such as ||\\boldsymbol{\\beta}^{(k+1)}-\\boldsymbol{\\beta}^{(k)}||_p\\leq \\epsilon for some p-norm ||\\cdot||_p and positive \\epsilon.\nMeanwhile, the M-step in both original and generalized algorithms defines a mapping from the parameter space \\boldsymbol{\\Omega} to itself by \n\\begin{align*}\nM: \\boldsymbol{\\Omega} &\\to \\boldsymbol{\\Omega}\\\\\n   \\boldsymbol{\\beta}^{(k)} &\\mapsto \\boldsymbol{\\beta}^{(k+1)}.\n\\end{align*}\n\n\n\n2.3.2 Convergence of the EM Algorithm\nBy the definition of conditional likelihood, our likelihood of complete data can be expressed by \nL_{\\boldsymbol{X}}(\\boldsymbol{\\beta}) = f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\beta})=L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta})f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}),\n and hence the log-likelihood is given by \n\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\beta}) = \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}) + \\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}).\n Take expectation to both sides of the equation with respect to \\boldsymbol{x|y} and replace \\boldsymbol{\\beta} by \\boldsymbol{\\beta}^{(k)}, we will have \nQ(\\boldsymbol{\\beta};\\boldsymbol{\\beta}^{(k)}) = \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}) + \\mathbb{E}_{\\boldsymbol{X}}[\\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}].\n Now consider the difference of log-likelihood of \\boldsymbol{Y} function between two iterations, we have \n\\begin{align*}\n  \\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}^{(k+1)})-\\log L_{\\boldsymbol{Y}}(\\boldsymbol{\\beta}^{(k)}) =\n  &\\{Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})-Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)})\\}\\\\\n  &-\\{\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k+1)})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\\\\n  &-\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k)}|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]\\}.\n\\end{align*}\n By the procedure of EM-algorithm, we always have Q(\\boldsymbol{\\beta}^{(k+1)};\\boldsymbol{\\beta}^{(k)})\\geq Q(\\boldsymbol{\\beta}^{(k)};\\boldsymbol{\\beta}^{(k)}). By the Jensen’s inequality, we have \\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k+1)})|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}] \\leq \\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\boldsymbol{y};\\boldsymbol{\\beta}^{(k)}|\\boldsymbol{y},\\boldsymbol{\\beta}^{(k)}]. Therefore during iterations, the log-likelihood of observed data \\boldsymbol{Y} keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-penEM",
    "href": "background.html#sec-penEM",
    "title": "2  Background",
    "section": "2.4 The EM Algorithm in LASSO Factor Analytic Models",
    "text": "2.4 The EM Algorithm in LASSO Factor Analytic Models\n\n2.4.1 Model Setup\nSuppose we have the n centralized observations, \\{\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n\\}, where \\boldsymbol{y}_j\\in \\mathbb{R}^p and hence the mean, \\boldsymbol{\\mu}_j=\\boldsymbol{0}_p (j=1,2,\\dots,n). For each of the observation \\boldsymbol{y}_j, the common factor and unique factor are \\boldsymbol{f}_j and \\boldsymbol{\\epsilon}_j respectively. Denote the response matrix as \\boldsymbol{Y}=[\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n]^\\top, the common factor matrix as \\boldsymbol{F}=[\\boldsymbol{f}_1,\\boldsymbol{f}_2,\\dots,\\boldsymbol{f}_n]^\\top, and the unique factor matrix as \\boldsymbol{\\hat\\epsilon}=[\\boldsymbol{\\epsilon}_1,\\boldsymbol{\\epsilon}_2,\\dots,\\boldsymbol{\\epsilon}_n]^\\top. Then the model can be written as \\boldsymbol{Y}=\\boldsymbol{F}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\hat\\epsilon}.\nWe also assume\n\n\\boldsymbol{\\epsilon}_j follows a normal distribution with mean 0 and variance \\boldsymbol{\\Psi} for j=1,2,\\dots,n, where \\boldsymbol{\\Psi} is a diagonal matrix defined by \\boldsymbol{\\Psi}=diag(\\psi_{11},\\psi_{22},\\dots,\\psi_{pp}).\n\\boldsymbol{f}_j follows a normal distribution with mean 0 and variance \\boldsymbol{I}_k for j=1,2,\\dots,n.\n\\boldsymbol{y}_j follows a normal distribution with mean 0 and variance \\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi} for j=1,2,\\dots,n. Those \\boldsymbol{y}_j are pairwisely independent.\n\n\n\n2.4.2 The EM Algorithm\nWe treat the common factor matrix \\boldsymbol{F} as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of (\\boldsymbol{Y},\\boldsymbol{F}) given \\boldsymbol{Y} and (\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}), where (\\boldsymbol{\\Lambda}_{(k)},\\boldsymbol{\\Psi}_{(k)}) is the parameter we got in the k-th iteration (k&gt;1) and (\\boldsymbol{\\Lambda}_{(0)},\\boldsymbol{\\Psi}_{(0)}) is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get (\\boldsymbol{\\Lambda}_{(k+1)},\\boldsymbol{\\Psi}_{(k+1)}).\n\n\n2.4.3 E-Step\nFirst, the joint likelihood of (\\boldsymbol{Y},\\boldsymbol{F}), denoted as L_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}), is given by\n\n\\begin{align*}\nL_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=&\\prod_{i=1}^nf(\\boldsymbol{y}_i,\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nf(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nN(\\boldsymbol{y}_i;\\boldsymbol{\\Lambda}\\boldsymbol{f}_i,\\boldsymbol{\\Psi})N(\\boldsymbol{f}_i;\\boldsymbol{0}_k,\\boldsymbol{I}_k)\\\\\n=&\\prod_{i=1}^n[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Psi})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}\\\\\n&(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\}][(2\\pi)^{-\\frac{k}{2}}\\det({\\boldsymbol{I}_k})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}\\boldsymbol{f}_i^\\top\\boldsymbol{I}_k^{-1}\\boldsymbol{f}_i\\}]\\\\\n= &(2\\pi)^{-\\frac{n}{2}(p+k)}(\\prod_{i=1}^p\\psi_{jj})^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(y_{ij}-\\boldsymbol{\\Lambda}(j,)\\boldsymbol{f}_i)^2}{\\psi_{jj}}\\}\\\\\n&\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\}.\n\\end{align*}\n\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nNg, S. K., T. Krishnan, and G. J. McLachlan. 2012. “The EM Algorithm.” In Handbook of Computational Statistics: Concepts and Methods, 139–72. Springer.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "method.html",
    "href": "method.html",
    "title": "3  Method",
    "section": "",
    "text": "3.1 Existing R packages",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#simulation-setting",
    "href": "method.html#simulation-setting",
    "title": "3  Method",
    "section": "3.3 Simulation setting",
    "text": "3.3 Simulation setting\n\n3.3.1 Setting 1\n\n\n3.3.2 Setting 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#initial-values",
    "href": "method.html#initial-values",
    "title": "3  Method",
    "section": "3.2 Initial values",
    "text": "3.2 Initial values",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#selecting-initial-values",
    "href": "method.html#selecting-initial-values",
    "title": "3  Method",
    "section": "3.2 Selecting initial values",
    "text": "3.2 Selecting initial values",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#setting-1",
    "href": "method.html#setting-1",
    "title": "3  Method",
    "section": "3.4 Setting 1",
    "text": "3.4 Setting 1",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#setting-2",
    "href": "method.html#setting-2",
    "title": "3  Method",
    "section": "3.5 Setting 2",
    "text": "3.5 Setting 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  }
]