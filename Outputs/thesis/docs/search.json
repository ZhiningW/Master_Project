[
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is factor analysis and why is it important?\nFactor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables [@mardiaMultivariateAnalysis1979]. It provides a useful tool for exploring the covariance structure among observable variables [@hiroseSparseEstimationNonconcave2015]. One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are [@mardiaMultivariateAnalysis1979].\nSuppose we have a observable random vector \\(\\boldsymbol{y}\\in \\mathbb{R}^p\\) with mean \\(\\mathbb{E}[\\boldsymbol{y}]=\\boldsymbol{\\mu}\\) and variance \\(\\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Sigma}\\). Then a \\(k\\)-order factor analysis model for \\(\\boldsymbol{y}\\) can be given by \\[\\begin{equation}\n\\boldsymbol{y}=\\boldsymbol{\\Lambda} \\boldsymbol{f}+\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon},\n\\end{equation}\\] where \\(\\boldsymbol{\\Lambda} \\in \\mathbb{R}^{p \\times k}\\) is called loading matrix, we call \\(\\boldsymbol{f} \\in \\mathbb{R}^{k}\\) as common factors and \\(\\boldsymbol{\\epsilon} \\in \\mathbb{R}^{p}\\) is unique factors. To make the model well-defined, we may assume \\[\\mathbb{E}[\\boldsymbol{f}]=\\boldsymbol{0}_k, \\mathbb{V}[\\boldsymbol{f}]=\\boldsymbol{I}_{k\\times k}, \\mathbb{E}[\\boldsymbol{\\epsilon}]=\\boldsymbol{0}_p, \\mathbb{V}[\\boldsymbol{\\epsilon}]=:\\boldsymbol{\\Psi}=\\text{diag}(\\Psi_{11},\\dots,\\Psi_{pp})\\] and also the independence between any elements from \\(\\boldsymbol{f}\\) and \\(\\boldsymbol{\\epsilon}\\) separately, i.e. \\[Cov[f_i,\\epsilon_j]=0, \\text{for all } i\\in\\{1,2,\\dots,k\\} \\text{ and } j \\in \\{1,2,\\dots,p\\}\\] Straightforwardly, the covariance of observable vector \\(\\boldsymbol{y}\\) can be modelled by\n\\[\\begin{equation}\n\\mathbb{V}[\\boldsymbol{y}]=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "href": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "title": "1  Introduction",
    "section": "1.2 Indeterminacy of the loading matrix",
    "text": "1.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\[\\boldsymbol{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon}\\] where the matrix \\(\\boldsymbol{M}\\) is orthogonal and simultaneously the variance of \\(\\boldsymbol{y}\\) given by (2) still holds, since \\[\\mathbb{V}[\\boldsymbol{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}.\\] Therefore a rotated loading matrix \\(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\) is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like [@mardiaMultivariateAnalysis1979] \\[\\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "href": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "title": "1  Introduction",
    "section": "1.3 Traditional Estimation of Parameters in Factor Analytic Models",
    "text": "1.3 Traditional Estimation of Parameters in Factor Analytic Models\nWe denote the set of parameters by \\(\\beta := \\{\\text{vec}(\\boldsymbol{\\Lambda}),\\text{vec}(\\boldsymbol{\\Psi})\\}\\) where \\(\\text{vec}(\\cdot)\\) is the vectorisation of the input.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n1.3.1 Maximum Likelihood Estimation\nSuppose we have \\(n\\) independent and identically distributed observations \\(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_N\\) from a p-dimensional multi-variate normal distribution \\(N_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\) and by our hypothesis, we have \\(\\boldsymbol{\\Sigma}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\\). Then the likelihood function is given by \\[L(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu}))\\right].\\] and hence the log-likelihood is given by \\[\\begin{align*}\nl(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})]\\\\\n&= -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}S)],\n\\end{align*}\\] where \\(\\boldsymbol{S}\\) is the sample covariance defined as \\(\\boldsymbol{S}:=\\frac{1}{n}\\sum^n_{i=1}(\\boldsymbol{y}_i-\\boldsymbol{\\mu})(\\boldsymbol{y}_i-\\boldsymbol{\\mu})^\\top\\). To get the MLE of parameters, we seek the roots of equation system\n\\[\\begin{cases}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=0.\n\\end{cases}\\]\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin[@Rubin1982EMAlgorithms].\n\n\n1.3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method [@hiroseSparseEstimationNonconcave2015].\nSuppose \\(Q(\\boldsymbol{\\Lambda})\\) is an criterion for \\(\\boldsymbol{\\Lambda}\\) in the rotation procedure, and we may express it as \\(Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\) where \\(P(\\cdot)\\) is some loss function[@hiroseSparseEstimationNonconcave2015]. Specifically, if we set \\(P(\\cdot)=|\\cdot|\\), we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as [@Jennrich2004Rotation] \\[\\begin{align*}\n&\\min_\\boldsymbol{\\Lambda} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\\] where \\(\\boldsymbol{\\Lambda}_0\\) is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is \\[\\begin{align*}\n&\\min_\\boldsymbol{\\Lambda} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\\] where \\(\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\) is the maximum likelihood estimator.\n\n\n1.3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability[@hiroseSparseEstimationNonconcave2015]. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \\[\n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n\\] We generate \\(\\boldsymbol{Y}\\in \\mathbb{R}^{{100}\\times 5}\\) using common factor \\(\\boldsymbol{f}\\in \\mathbb{R}^{{100}\\times 2}\\) where each factor is generated randomly from standard normal distribution \\(N(0,1)\\). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is \\[\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n\\] which is neither precise nor sparse.\nSimulation code:\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model[@mardiaMultivariateAnalysis1979]. We have \\[\\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*}\\] The test statistics is given by \\(\\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}})\\) which has an asymptotic \\(\\chi^2_s\\) distribution with \\(s=\\frac{1}{2}(p-k)^2-(p+k)\\) by the property of MLE. Typically, the test is conducted at a \\(5\\%\\) significant level. We can start the procedure from a very small \\(k\\), say, \\(k=1\\) or \\(k=2\\) and then increase \\(k\\) until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#penalized-likelihood-method",
    "href": "chapter1.html#penalized-likelihood-method",
    "title": "1  Introduction",
    "section": "1.4 Penalized Likelihood Method",
    "text": "1.4 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above[@hiroseSparseEstimationNonconcave2015]. A penalized factor analytic model can be obtained by solving following optimization problem \\[\n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}l_p := l(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})-\\rho\\sum^p_{i=1}\\sum^k_{j=1}P(|\\boldsymbol{\\Lambda}_{ij}|).\n\\] where we call \\(l_p\\) as the penalized likelihood, \\(\\rho\\) is called regularization parameter and we can treat \\(P(\\cdot)\\) as a penalized function. There are many types of penalized functions developed, such as LASSO (\\(P(\\cdot)=|\\cdot|\\)) and MC+ (\\(P(|\\theta|;\\rho;\\gamma)=n(|\\theta|-\\frac{\\theta^2}{2\\rho\\gamma})I(|\\theta|&lt;\\rho\\gamma)+\\frac{\\rho^2\\gamma}{2}I(|\\theta|\\geq\\rho\\gamma)\\))[@hiroseSparseEstimationNonconcave2015]. In this article, we will mainly focus on the LASSO penalty.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "href": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "title": "1  Introduction",
    "section": "1.5 LASSO penalty and LASSO estimator",
    "text": "1.5 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by \\[l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\] and the LASSO estimator, denoted as \\((\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*)\\), can be obtained via \\[\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\Lambda_{ij}|.\n\\end{align*}\\]\nAn E-M algorithm can be applied for evaluating the LASSO estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix",
    "href": "chapter1.html#appendix",
    "title": "1  Introduction",
    "section": "1.6 Appendix",
    "text": "1.6 Appendix\n\n\n\nTable 1.1: List of notations used in this report.\n\n\n\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\(\\boldsymbol{A}^\\top\\)\nthe transpose of the matrix (or vector) \\(A\\)\n\n\n\\(\\mathbb{R}^{p}\\)\nthe space of all p-dimensional real column vectors like \\([a_1,a_2,\\dots,a_p]^\\top\\)\n\n\n\\(\\mathbb{R}^{p\\times q}\\)\nthe space of all real matrices with size \\(p\\times q\\)\n\n\n\\(\\mathbb{E}[\\cdot]\\)\nthe expectation, or mean of a random variable\n\n\n\\(\\mathbb{V}[\\cdot]\\)\nthe variance, or covariance of a random variable\n\n\n\\(\\boldsymbol{Cov}[\\cdot , \\cdot]\\)\nthe covariance of two random variables\n\n\n\\(\\boldsymbol{0}_{p}\\) and \\(\\boldsymbol{0}_{p\\times q}\\)\nthe p-dimensional \\(0\\) vector or \\(0\\) matrix in size \\(p\\times q\\) respectively\n\n\n\\(\\boldsymbol{I}_p\\)\nthe identity matrix in size \\(p\\times p\\)\n\n\n\\(\\det(\\cdot)\\)\nthe determinant of a matrix\n\n\n\\(M(i,)\\)\nthe \\(i^{th}\\) row of the matrix \\(M\\)\n\n\n\\(M(,j)\\)\nthe \\(j^{th}\\) row of the matrix \\(M\\)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]