[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model Selection in Factor Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#benefits",
    "href": "index.html#benefits",
    "title": "Model Selection in Factor Analysis",
    "section": "Benefits",
    "text": "Benefits\nThe benefits of using Quarto document include:\n\nIt allows you to write your thesis in a simple markup language called Markdown. This means that you can focus on writing your thesis without having to worry about formatting.\nThe document can be output to a variety of formats including PDF, HTML, Word, and LaTeX.\nCode can be easily embedded in the document and executed. This means that you can include the results of your analysis in your thesis without having to manually copy and paste them. This is a good reproducible and scientific practice.\nYou can easily integrate with aspects of GitHub (edit, reporting an issue, etc).\n\nThe above outlined benefits can also be considered as best practice. Version controlling and collaborative writing (via Git and GitHub) are important in managing multiple versions of your thesis and in collaborating with your supervisory team. Embedding code in your thesis is a good practice in reproducible research. Making your thesis in HTML format can allow for interactive web elements to be embedded while PDF format can be for general distribution and printing.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Model Selection in Factor Analysis",
    "section": "Getting started",
    "text": "Getting started\nThere are several systems that you are expected to know to use this template. These include:\n\nMarkdown syntax for writing\nQuarto or R Markdown syntax (note that these works for Python or Julia too) for embedding code\n(Optional) Git and GitHub for hosting",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#frequently-asked-questions",
    "href": "index.html#frequently-asked-questions",
    "title": "Model Selection in Factor Analysis",
    "section": "Frequently asked questions",
    "text": "Frequently asked questions\n\nWhat about Overleaf?\nANU has a professional account for Overleaf, which is great for those that use LaTeX regularly. Unfortunately, there is no equivalent system with track changes in Quarto. You can output the tex file from Quarto document and use this in Overleaf. The changes made in this tex document however has to be manually transferred back to the Quarto document. If your main output is mainly mathematical and you have little to no code outputs, Overleaf is probably a better choice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What is factor analysis and why is it important?\nFactor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables (Mardia, Kent, and Bibby 1979). It provides a useful tool for exploring the covariance structure among observable variables (Hirose and Yamamoto 2015). One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are (Mardia, Kent, and Bibby 1979).\nSuppose we have a observable random vector \\(\\mathbf{y}\\in \\mathbb{R}^p\\) with mean \\(\\mathbb{E}[\\mathbf{y}]=\\mu\\) and variance \\(\\mathbb{V}[\\mathbf{y}]=\\Sigma\\). Then a \\(k\\)-order factor analysis model for \\(\\mathbf{y}\\) can be given by \\[\\begin{equation}\n\\mathbf{y}=\\Lambda \\mathbf{f}+\\mu+\\epsilon,\n\\end{equation}\\] where \\(\\Lambda \\in \\mathbb{R}^{p \\times k}\\) is called loading matrix, we call \\(\\mathbf{f} \\in \\mathbb{R}^{k}\\) as common factors and \\(\\epsilon \\in \\mathbb{R}^{p}\\) is unique factors. To make the model well-defined, we may assume \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}_k, \\mathbb{V}[\\mathbf{f}]=\\mathbf{I}_{k\\times k}, \\mathbb{E}[\\epsilon]=\\mathbf{0}_p, \\mathbb{V}[\\epsilon]=:\\Psi=\\text{diag}(\\psi_{11},\\dots,\\psi_{pp})\\] and also the independence between any elements from \\(\\mathbf{f}\\) and \\(\\epsilon\\) separately, i.e. \\[Cov[\\mathbf{f}_i,\\epsilon_j]=0, \\text{for all } i\\in\\{1,2,\\dots,k\\} \\text{ and } j \\in \\{1,2,\\dots,p\\}\\] Straightforwardly, the covariance of observable vector \\(\\mathbf{y}\\) can be modelled by\n\\[\\begin{equation}\n\\mathbb{V}[\\mathbf{y}]=\\Lambda\\Lambda^\\top+\\Psi\n\\end{equation}\\] If we assemble the \\(n\\) observation, \\(\\{\\mathbf{y}_1,\\mathbf{y}_2,\\dots,\\mathbf{y}_n\\}\\), into a matrix \\(\\mathbf{Y}\\in \\mathbb{R}^{n\\times p}\\). The model can be rewritten as \\[\\mathbf{Y}=\\mathbf{F}\\Lambda^\\top+\\mu+\\epsilon\\] where common factors matrix \\(\\mathbf{F}\\in \\mathbb{R}^{n\\times k}\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-factor-analysis-and-why-is-it-important",
    "href": "chapter1.html#what-is-factor-analysis-and-why-is-it-important",
    "title": "1  Model Selection in Factor Analysis",
    "section": "2.1 What is factor analysis and why is it important?",
    "text": "2.1 What is factor analysis and why is it important?\nFactor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables (Mardia, Kent, and Bibby 1979). It provides a useful tool for exploring the covariance structure among observable variables (Hirose and Yamamoto 2015). One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are (Mardia, Kent, and Bibby 1979).\nSuppose we have a observable random vector \\(\\mathbf{y}\\in \\mathbb{R}^p\\) with mean \\(\\mathbb{E}[\\mathbf{y}]=\\mu\\) and variance \\(\\mathbb{V}[\\mathbf{y}]=\\Sigma\\). Then a \\(k\\)-order factor analysis model for \\(\\mathbf{y}\\) can be given by \\[\\begin{equation}\n\\mathbf{y}=\\Lambda \\mathbf{f}+\\mu+\\epsilon,\n\\end{equation}\\] where \\(\\Lambda \\in \\mathbb{R}^{p \\times k}\\) is called loading matrix, we call \\(\\mathbf{f} \\in \\mathbb{R}^{k}\\) as common factors and \\(\\epsilon \\in \\mathbb{R}^{p}\\) is unique factors. To make the model well-defined, we may assume \\[\\mathbb{E}[\\mathbf{f}]=\\mathbf{0}_k, \\mathbb{V}[\\mathbf{f}]=\\mathbf{I}_{k\\times k}, \\mathbb{E}[\\epsilon]=\\mathbf{0}_p, \\mathbb{V}[\\epsilon]=:\\Psi=\\text{diag}(\\psi_{11},\\dots,\\psi_{pp})\\] and also the independence between any elements from \\(\\mathbf{f}\\) and \\(\\epsilon\\) separately, i.e. \\[Cov[\\mathbf{f}_i,\\epsilon_j]=0, \\text{for all } i\\in\\{1,2,\\dots,k\\} \\text{ and } j \\in \\{1,2,\\dots,p\\}\\] Straightforwardly, the covariance of observable vector \\(\\mathbf{y}\\) can be modelled by\n\\[\\begin{equation}\n\\mathbb{V}[\\mathbf{y}]=\\Lambda\\Lambda^\\top+\\Psi\n\\end{equation}\\] If we assemble the \\(n\\) observation, \\(\\{\\mathbf{y}_1,\\mathbf{y}_2,\\dots,\\mathbf{y}_n\\}\\), into a matrix \\(\\mathbf{Y}\\in \\mathbb{R}^{n\\times p}\\). The model can be rewritten as \\[\\mathbf{Y}=\\mathbf{F}\\Lambda^\\top+\\mu+\\epsilon\\] where common factors matrix \\(\\mathbf{F}\\in \\mathbb{R}^{n\\times k}\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Model Selection in Factor Analysis</span>"
    ]
  },
  {
    "objectID": "chapter1.html#maximum-likelihood-estimation",
    "href": "chapter1.html#maximum-likelihood-estimation",
    "title": "1  Introduction",
    "section": "3.1 Maximum Likelihood Estimation",
    "text": "3.1 Maximum Likelihood Estimation\nSuppose we have \\(n\\) independent and identically distributed observations \\(\\mathbf{y}_1,\\mathbf{y}_2,\\dots,\\mathbf{y}_N\\) from a p-dimensional multi-variate normal distribution \\(N_p(\\mu,\\Sigma)\\) and by our hypothesis, we have \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\). Then the likelihood function is given by \\[L(\\Lambda,\\Psi)=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\Sigma)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\mathbf{y}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{y}_i-\\mu))\\right].\\] and hence the log-likelihood is given by \\[\\begin{align*}\nl(\\Lambda,\\Psi)=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\Sigma))-\\frac{1}{2}(\\mathbf{y}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{y}_i-\\mu)]\\\\\n&= -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\Sigma))+\\text{tr}(\\Sigma^{-1}S)],\n\\end{align*}\\] where \\(S\\) is the sample covariance defined as \\(S:=\\frac{1}{n}\\sum^n_{i=1}(\\mathbf{y}_i-\\mu)(\\mathbf{y}_i-\\mu)^\\top\\). To get the MLE of parameters, we seek the roots of equation system\n\\[\\begin{cases}\n\\frac{\\partial}{\\partial \\Lambda}l(\\Lambda,\\Psi)=0 \\\\\n\\frac{\\partial}{\\partial \\Psi}l(\\Lambda,\\Psi)=0.\n\\end{cases}\\]\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin(Rubin and Thayer 1982).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#rotation-techniques",
    "href": "chapter1.html#rotation-techniques",
    "title": "1  Introduction",
    "section": "3.2 Rotation techniques",
    "text": "3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose \\(Q(\\Lambda)\\) is an criterion for \\(\\Lambda\\) in the rotation procedure, and we may express it as \\(Q(\\Lambda):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\lambda_{ij})\\) where \\(P(\\cdot)\\) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set \\(P(\\cdot)=|\\cdot|\\), we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004) \\[\\begin{align*}\n&\\min_\\Lambda \\sum^p_{i=1}\\sum^k_{j=1}P(\\lambda_{ij})\\\\\n&\\text{subject to } \\Lambda=\\Lambda_0\\mathbf{M} \\text{ and } \\mathbf{M}^\\top\\mathbf{M}=\\mathbf{I}_k,\n\\end{align*}\\] where \\(\\Lambda_0\\) is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is \\[\\begin{align*}\n&\\min_\\Lambda \\sum^p_{i=1}\\sum^k_{j=1}P(\\lambda_{ij})\\\\\n&\\text{subject to } \\Lambda=\\hat{\\Lambda}_{\\text{MLE}}\\mathbf{M} \\text{ and } \\mathbf{M}^\\top\\mathbf{M}=\\mathbf{I}_k,\n\\end{align*}\\] where \\(\\hat{\\Lambda}_{\\text{MLE}}\\) is the maximum likelihood estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#discussion-about-two-step-method",
    "href": "chapter1.html#discussion-about-two-step-method",
    "title": "1  Introduction",
    "section": "3.3 Discussion about two-step method",
    "text": "3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability(Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \\[\n\\Lambda=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n\\] We generate \\(\\mathbf{Y}\\in \\mathbb{R}^{{100}\\times 5}\\) using common factor \\(\\mathbf{f}\\in \\mathbb{R}^{{100}\\times 2}\\) where each factor is generated randomly from standard normal distribution \\(N(0,1)\\). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is \\[\n\\hat{\\Lambda}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n\\] which is neither precise nor sparse.\nSimulation code:\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model(Mardia, Kent, and Bibby 1979). We have \\[\\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*}\\] The test statistics is given by \\(\\text{TS}=nF(\\hat{\\Lambda},\\hat{\\Psi})\\) which has an asymptotic \\(\\chi^2_s\\) distribution with \\(s=\\frac{1}{2}(p-k)^2-(p+k)\\) by the property of MLE. Typically, the test is conducted at a \\(5\\%\\) significant level. We can start the procedure from a very small \\(k\\), say, \\(k=1\\) or \\(k=2\\) and then increase \\(k\\) until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "href": "chapter1.html#lasso-penalty-and-lasso-estimator",
    "title": "1  Introduction",
    "section": "2.1 LASSO penalty and LASSO estimator",
    "text": "2.1 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by \\[l_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\Sigma))+\\text{tr}(\\Sigma^{-1}S)]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|\\] and the LASSO estimator, denoted as \\((\\Lambda^*,\\Psi^*)\\), can be obtained via \\[\\begin{align*}\n(\\Lambda^*,\\Psi^*) &= \\text{arg}\\max_{\\Lambda,\\Psi} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\Sigma))+\\text{tr}(\\Sigma^{-1}S)]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\Lambda,\\Psi} \\quad \\log(\\det(\\Sigma))+\\text{tr}(\\Sigma^{-1}S)+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|.\n\\end{align*}\\]\nAn E-M algorithm can be applied for evaluating the LASSO estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix",
    "href": "chapter1.html#appendix",
    "title": "1  Introduction",
    "section": "2.2 Appendix",
    "text": "2.2 Appendix\n\n\n\nTable 2.1: List of notations used in this report.\n\n\n\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\(A^\\top\\)\nthe transpose of the matrix (or vector) \\(A\\)\n\n\n\\(\\mathbb{R}^{p}\\)\nthe space of all p-dimensional real column vectors like \\([a_1,a_2,\\dots,a_p]^\\top\\)\n\n\n\\(\\mathbb{R}^{p\\times q}\\)\nthe space of all real matrices with size \\(p\\times q\\)\n\n\n\\(\\mathbb{E}[\\cdot]\\)\nthe expectation, or mean of a random variable\n\n\n\\(\\mathbb{V}[\\cdot]\\)\nthe variance, or covariance of a random variable\n\n\n\\(\\mathbf{Cov}[\\cdot , \\cdot]\\)\nthe covariance of two random variables\n\n\n\\(\\mathbf{0}_{p}\\) and \\(\\mathbf{0}_{p\\times q}\\)\nthe p-dimensional \\(0\\) vector or \\(0\\) matrix in size \\(p\\times q\\) respectively\n\n\n\\(\\mathbf{I}_p\\)\nthe identity matrix in size \\(p\\times p\\)\n\n\n\\(\\det(\\cdot)\\)\nthe determinant of a matrix",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "disclaimer.html",
    "href": "disclaimer.html",
    "title": "Disclaimer",
    "section": "",
    "text": "This thesis is composed of my original work, and contains no material previously published or written by another person except where due reference has been made in the text. I have clearly stated the contribution by others to jointly-authored works that I have included in my thesis.\nI have clearly stated the contribution of others to my thesis as a whole, including statistical assistance, study design, data analysis, significant technical procedures, professional editorial advice, financial support and any other original research work used or reported in my thesis. The content of my thesis is the result of work I have carried out since the commencement of my higher degree by research candidature and does not include a substantial part of work that has been submitted to qualify for the award of any other degree or diploma in any university or other tertiary institution. I have clearly stated which parts of my thesis, if any, have been submitted to qualify for another award.\n\nZhining Wang\n2024-04-17\n\n\nPublications\nAccepted or in-press publication in this thesis.\nSubmitted manuscripts included in this thesis.\nOther publications during candidature.",
    "crumbs": [
      "Disclaimer"
    ]
  },
  {
    "objectID": "acknowledgements.html",
    "href": "acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "I would like to express my sincere gratitude to my dog, Chuckles, for eating my research notes multiple times. If it wasn’t for you, I would have finished this thesis earlier.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  The EM Algorithm",
    "section": "",
    "text": "2.1 A brief introduction about the EM algorithm\nThe Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data(Ng2012EMAlgorithm?). As we mentioned, the key of maximum likelihood estimation is solving equation \\[\\begin{equation}\n\\frac{\\partial}{\\partial \\beta}l=0.\n\\end{equation}\\] However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, augmented data(or complete) refers to the integration of this unobserved component, enhancing the algorithm’s ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete datasets effectively.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The EM Algorithm</span>"
    ]
  },
  {
    "objectID": "chapter2.html#the-e-step-and-m-step",
    "href": "chapter2.html#the-e-step-and-m-step",
    "title": "2  The EM Algorithm",
    "section": "2.2 The E-step and M-step",
    "text": "2.2 The E-step and M-step\nLet \\(\\boldsymbol{x}\\) denote the vector containing complete data, \\(\\boldsymbol{y}\\) denote the observed incomplete data and \\(\\boldsymbol{z}\\) denote the vector containing the missing data. Here ‘missing data’ is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later)(Ng2012EMAlgorithm?).\nNow denote \\(g_c(\\boldsymbol{x};\\boldsymbol{\\Psi})\\) as the probability density function (p.d.f.) of the random vector \\(\\boldsymbol{X}\\) corresponding to \\(\\boldsymbol{x}\\). Then the complete-data log-likelihood function when complete data is fully observed can be given by \\[\\log L_c(\\boldsymbol{\\Psi})=\\log g_c(\\boldsymbol{x};\\boldsymbol{\\Psi}).\\] The EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of \\(\\log L_c(\\boldsymbol{\\Psi})\\). But it is unobservable since it includes missing part of the data, then we use the conditional expectation given \\(\\boldsymbol{y}\\) and current fit for \\(\\boldsymbol{\\Psi}\\).\nOn the \\((k+1)^th\\) iteration, we have \\[\\begin{align*}\n&\\text{E-step: Compute } Q(\\boldsymbol{\\Psi};\\boldsymbol{\\Psi}^{(k)}):=\\mathbb{E}[\\log L_c(\\boldsymbol{\\Psi})|y]\\\\\n&\\text{M-step: Update }\\boldsymbol{\\Psi}^{(k+1)} \\text{ as }\\boldsymbol{{\\Psi}}^{(k+1)}:=\\text{arg}\\max_\\boldsymbol{\\Psi} Q(\\boldsymbol{\\Psi};\\boldsymbol{\\Psi}^{(k)})\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The EM Algorithm</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via\nNonconcave Penalized Likelihood in Factor Analysis Model.”\nStatistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component\nLoss Functions: The Orthogonal Case.” Psychometrika 69:\n257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate\nAnalysis. Probability and Mathematical Statistics. London ; New\nYork: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for\nML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "tools.html",
    "href": "tools.html",
    "title": "Appendix A — Tools",
    "section": "",
    "text": "This thesis was written using Quarto 1.4.551 (quarto?) and the following system and R packages:\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.3 (2024-02-29)\n os       macOS Sonoma 14.4.1\n system   aarch64, darwin20\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Australia/Sydney\n date     2024-04-17\n pandoc   3.1.1 @ /Applications/RStudio.app/Contents/Resources/app/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cli           3.6.2   2023-12-11 [1] CRAN (R 4.3.1)\n digest        0.6.35  2024-03-11 [1] CRAN (R 4.3.1)\n evaluate      0.23    2023-11-01 [1] CRAN (R 4.3.1)\n fastmap       1.1.1   2023-02-24 [1] CRAN (R 4.3.0)\n htmltools     0.5.7   2023-11-03 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.1)\n knitr         1.45    2023-10-30 [1] CRAN (R 4.3.1)\n later         1.3.2   2023-12-06 [1] CRAN (R 4.3.1)\n processx      3.8.3   2023-12-10 [1] CRAN (R 4.3.1)\n ps            1.7.6   2024-01-18 [1] CRAN (R 4.3.1)\n quarto        1.4     2024-03-06 [1] CRAN (R 4.3.1)\n Rcpp          1.0.12  2024-01-09 [1] CRAN (R 4.3.1)\n rlang         1.1.3   2024-01-10 [1] CRAN (R 4.3.1)\n rmarkdown     2.26    2024-03-05 [1] CRAN (R 4.3.1)\n rstudioapi    0.15.0  2023-07-07 [1] CRAN (R 4.3.0)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.0)\n xfun          0.42    2024-02-08 [1] CRAN (R 4.3.1)\n yaml          2.3.8   2023-12-11 [1] CRAN (R 4.3.1)\n\n [1] /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Tools</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "",
    "text": "3.1 Model Setup\nSuppose we have the \\(n\\) centralized observations, \\(\\{\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n\\}\\), where \\(\\boldsymbol{y}_j\\in \\mathbb{R}^p\\) for \\(j=1,2,\\dots,n\\). For each of the observation \\(\\boldsymbol{y}_j\\), the common factor and unique factor are \\(\\boldsymbol{f}_j\\) and \\(\\boldsymbol{\\epsilon}_j\\) respectively. Denote the response matrix as \\(\\boldsymbol{Y}=[\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\dots,\\boldsymbol{y}_n]^\\top\\), the common factor matrix as \\(\\boldsymbol{F}=[\\boldsymbol{f}_1,\\boldsymbol{f}_2,\\dots,\\boldsymbol{f}_n]^\\top\\), and the unique factor matrix as \\(\\boldsymbol{\\hat\\epsilon}=[\\boldsymbol{\\epsilon}_1,\\boldsymbol{\\epsilon}_2,\\dots,\\boldsymbol{\\epsilon}_n]^\\top\\). Then the model can be written as \\(\\boldsymbol{Y}=\\boldsymbol{F}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\mu}+\\boldsymbol{\\hat\\epsilon}.\\) Moreover, \\(\\boldsymbol{\\mu}\\) vanishes because of centralization and denote \\(\\boldsymbol{\\theta}:=\\boldsymbol{\\Lambda}^\\top\\), we have our model \\[\n\\boldsymbol{Y}=\\boldsymbol{F}\\boldsymbol{\\theta}+\\boldsymbol{\\hat\\epsilon}.\n\\] We also assume",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "chapter3.html#model-setup",
    "href": "chapter3.html#model-setup",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "",
    "text": "\\(\\boldsymbol{\\epsilon}_j\\) follows a normal distribution with mean \\(0\\) and variance \\(\\boldsymbol{\\Psi}\\), where \\(\\boldsymbol{\\Psi}\\) is a diagonal matrix defined by \\(\\boldsymbol{\\Psi}=diag(\\psi_{11},\\psi_{22},\\dots,\\psi_{pp})\\).\n\\(\\boldsymbol{f}_j\\) follows a normal distribution with mean \\(0\\) and variance \\(\\boldsymbol{I}_k\\) for \\(j=1,2,\\dots,n\\).\n\\(\\boldsymbol{y}_j\\) follows a normal distribution with mean \\(0\\) and variance \\(\\boldsymbol{\\theta}\\boldsymbol{\\theta}^\\top+\\boldsymbol{\\Psi}\\) for \\(j=1,2,\\dots,n\\). Those \\(\\boldsymbol{y}_j\\) are pairwisely independent.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "chapter3.html#e-step",
    "href": "chapter3.html#e-step",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "4.1 E-Step",
    "text": "4.1 E-Step\nFirst, the joint likelihood of \\((\\boldsymbol{Y},\\boldsymbol{F})\\), denoted as \\(L_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), is given by\n\\[\\begin{align*}\nL_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=&\\prod_{i=1}^nf(\\boldsymbol{y}_i,\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nf(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nN(\\boldsymbol{y}_i;\\boldsymbol{\\Lambda}\\boldsymbol{f}_i,\\boldsymbol{\\Psi})N(\\boldsymbol{f}_i;\\boldsymbol{0}_k,\\boldsymbol{I}_k)\\\\\n=&\\prod_{i=1}^n[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Psi})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}\\\\\n&(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\}][(2\\pi)^{-\\frac{k}{2}}\\det({\\boldsymbol{I}_k})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}\\boldsymbol{f}_i^\\top\\boldsymbol{I}_k^{-1}\\boldsymbol{f}_i\\}]\\\\\n= &(2\\pi)^{-\\frac{n}{2}(p+k)}(\\prod_{i=1}^p\\psi_{jj})^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2}{\\psi_{jj}}\\}\\\\\n&\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{F}(i,)\\boldsymbol{F}(i,)^\\top\\}.\n\\end{align*}\\]\nFurthermore, the log-likelihood, denoted as \\(l_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), is given by\n\\[\\begin{equation}\nl_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2}{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{F}(i,)\\boldsymbol{F}(i,)^\\top+\\text{constant}.\n\\label{ll}\n\\end{equation}\\]\nNow let us deduce the conditional expectation to \\(\\boldsymbol{F}\\) given \\(\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}\\), denoted as \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\). In \\(\\ref{ll}\\), the first term is independent on \\(\\boldsymbol{F}\\), hence stay the same under conditional expectation. The last term is independent on \\((\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), therefore we can regard it as a constant in \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\). We only need to consider the term inside the sum in the second term, more specifically, the numerator, we have\n\\[\n(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2 = \\boldsymbol{Y}_{ij}^2 - 2 \\boldsymbol{Y}_{ij}\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j)+\\boldsymbol{\\theta}(,j)^\\top \\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j)\n\\]\nWithout ambiguity, denote \\(\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\) to be the conditional expectation \\(\\mathbb{E}[\\boldsymbol{F}(i,)|\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}]\\) for simplification. Then the conditional expectation \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\) is given by \\[\\begin{align*}\nEl_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=&\\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{Y}_{ij}^2 - 2 \\boldsymbol{Y}_{ij}\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\boldsymbol{\\theta}(,j)+\\boldsymbol{\\theta}(,j)^\\top \\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\boldsymbol{\\theta}(,j)}{\\psi_{jj}}\\\\\n\\end{align*}\\]\nTo deal with \\(\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\) and \\(\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\), we only need to know the mean and variance of conditional distribution \\(\\boldsymbol{F}(i,)|\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}\\), where the latter one is denoted as \\(\\mathbb{V}[\\boldsymbol{F}(i,)|_{(k)}]\\). This is because we can always treat \\(\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\) as\n\\[\n\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]= \\mathbb{V}[\\boldsymbol{F}(i,)|_{(k)}]+ \\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]^\\top\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}].\n\\]\nWe have\nTheorem: If \\(\\boldsymbol{\\alpha}\\sim N(\\boldsymbol{\\mu}_\\boldsymbol{\\alpha},\\boldsymbol{\\Sigma_\\boldsymbol{\\alpha}})\\) and \\(\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta})\\), then we have \\(\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}})\\), where \\[\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}=\\boldsymbol{\\mu}_\\boldsymbol{\\alpha}+\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta}^{-1}\\boldsymbol{\\beta}\\\\\n& \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}= \\boldsymbol{\\Sigma}_\\boldsymbol{\\alpha}-\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta}^{-1}\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]^\\top.\n\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  },
  {
    "objectID": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "href": "chapter1.html#indeterminacy-of-the-loading-matrix",
    "title": "1  Introduction",
    "section": "1.2 Indeterminacy of the loading matrix",
    "text": "1.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\[\\mathbf{y}=(\\Lambda\\mathbf{M})(\\mathbf{M}^\\top\\mathbf{f}) +\\mu+\\epsilon\\] where the matrix \\(\\mathbf{M}\\) is orthogonal and simultaneously the variance of \\(\\mathbf{y}\\) given by (2) still holds, since \\[\\mathbb{V}[\\mathbf{y}]=(\\Lambda\\mathbf{M}\\mathbf{M}^\\top)\\mathbb{V}[\\mathbf{f}](\\Lambda\\mathbf{M}\\mathbf{M}^\\top)^\\top+\\Psi=\\Lambda\\Lambda^\\top+\\Psi.\\] Therefore a rotated loading matrix \\(\\Lambda\\mathbf{M}\\) is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\[\\Lambda^\\top \\Psi^{-1} \\Lambda \\text{ is diagonal.}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "href": "chapter1.html#traditional-estimation-of-parameters-in-factor-analytic-models",
    "title": "1  Introduction",
    "section": "1.3 Traditional Estimation of Parameters in Factor Analytic Models",
    "text": "1.3 Traditional Estimation of Parameters in Factor Analytic Models\nWe denote the set of parameters by \\(\\beta := \\{\\text{vec}(\\Lambda),\\text{vec}(\\Psi)\\}\\) where \\(\\text{vec}(\\cdot)\\) is the vectorisation of the input. Traditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n1.3.1 Maximum Likelihood Estimation\nSuppose we have \\(n\\) independent and identically distributed observations \\(\\mathbf{y}_1,\\mathbf{y}_2,\\dots,\\mathbf{y}_N\\) from a p-dimensional multi-variate normal distribution \\(N_p(\\mu,\\Sigma)\\) and by our hypothesis, we have \\(\\Sigma=\\Lambda\\Lambda^\\top+\\Psi\\). Then the likelihood function is given by \\[L(\\Lambda,\\Psi)=\\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\Sigma)^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\mathbf{y}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{y}_i-\\mu))\\right].\\] and hence the log-likelihood is given by \\[\\begin{align*}\nl(\\Lambda,\\Psi)=& \\sum^n_{i=1}[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\Sigma))-\\frac{1}{2}(\\mathbf{y}_i-\\mu)^\\top\\Sigma^{-1}(\\mathbf{y}_i-\\mu)]\\\\\n&= -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\Sigma))+\\text{tr}(\\Sigma^{-1}S)],\n\\end{align*}\\] where \\(S\\) is the sample covariance defined as \\(S:=\\frac{1}{n}\\sum^n_{i=1}(\\mathbf{y}_i-\\mu)(\\mathbf{y}_i-\\mu)^\\top\\). To get the MLE of parameters, we seek the roots of equation system\n\\[\\begin{cases}\n\\frac{\\partial}{\\partial \\Lambda}l(\\Lambda,\\Psi)=0 \\\\\n\\frac{\\partial}{\\partial \\Psi}l(\\Lambda,\\Psi)=0.\n\\end{cases}\\]\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin(Rubin and Thayer 1982).\n\n\n1.3.2 Rotation techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose \\(Q(\\Lambda)\\) is an criterion for \\(\\Lambda\\) in the rotation procedure, and we may express it as \\(Q(\\Lambda):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\lambda_{ij})\\) where \\(P(\\cdot)\\) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set \\(P(\\cdot)=|\\cdot|\\), we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004) \\[\\begin{align*}\n&\\min_\\Lambda \\sum^p_{i=1}\\sum^k_{j=1}P(\\lambda_{ij})\\\\\n&\\text{subject to } \\Lambda=\\Lambda_0\\mathbf{M} \\text{ and } \\mathbf{M}^\\top\\mathbf{M}=\\mathbf{I}_k,\n\\end{align*}\\] where \\(\\Lambda_0\\) is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is \\[\\begin{align*}\n&\\min_\\Lambda \\sum^p_{i=1}\\sum^k_{j=1}P(\\lambda_{ij})\\\\\n&\\text{subject to } \\Lambda=\\hat{\\Lambda}_{\\text{MLE}}\\mathbf{M} \\text{ and } \\mathbf{M}^\\top\\mathbf{M}=\\mathbf{I}_k,\n\\end{align*}\\] where \\(\\hat{\\Lambda}_{\\text{MLE}}\\) is the maximum likelihood estimator.\n\n\n1.3.3 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability(Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \\[\n\\Lambda=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0 & 0\\\\\n0 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix}.\n\\] We generate \\(\\mathbf{Y}\\in \\mathbb{R}^{{100}\\times 5}\\) using common factor \\(\\mathbf{f}\\in \\mathbb{R}^{{100}\\times 2}\\) where each factor is generated randomly from standard normal distribution \\(N(0,1)\\). We utilize the R-function factanal() to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is \\[\n\\hat{\\Lambda}=\n\\begin{bmatrix}\n-0.158 & 0.985\\\\\n0 & 0\\\\\n0.997 & 0\\\\\n0.207 & 0\\\\\n0 & 0\\\\\n\\end{bmatrix},\n\\] which is neither precise nor sparse.\nSimulation code:\nset.seed(123)\nn &lt;- 100 # Number of observations\np &lt;- 5   # Number of features\n\nfactor1 &lt;- rnorm(n, 0, 1) # set factors\nfactor2 &lt;- rnorm(n, 0, 1)\n\nX &lt;- matrix(NA, nrow=n, ncol=p)\nX[,1] &lt;- 0.8*factor1 + rnorm(n, 0, 1)  # Strong loading\nX[,2] &lt;- 0.8*factor2 + rnorm(n, 0, 1)  # Strong loading\nX[,3] &lt;- rnorm(n, 0, 1)  # Noise, no strong loading on any factor\nX[,4] &lt;- rnorm(n, 0, 1)  \nX[,5] &lt;- rnorm(n, 0, 1) \n\nfa_result &lt;- factanal(factors = 2, covmat = cor(X))\nrotated_fa &lt;- varimax(fa_result$loadings)\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model(Mardia, Kent, and Bibby 1979). We have \\[\\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*}\\] The test statistics is given by \\(\\text{TS}=nF(\\hat{\\Lambda},\\hat{\\Psi})\\) which has an asymptotic \\(\\chi^2_s\\) distribution with \\(s=\\frac{1}{2}(p-k)^2-(p+k)\\) by the property of MLE. Typically, the test is conducted at a \\(5\\%\\) significant level. We can start the procedure from a very small \\(k\\), say, \\(k=1\\) or \\(k=2\\) and then increase \\(k\\) until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapter3.html#the-em-algorithm",
    "href": "chapter3.html#the-em-algorithm",
    "title": "3  The EM Algorithm in LASSO Factor Analytic Models",
    "section": "3.2 The EM Algorithm",
    "text": "3.2 The EM Algorithm\nWe treat the common factor matrix \\(\\boldsymbol{F}\\) as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of \\((\\boldsymbol{Y},\\boldsymbol{F})\\) given \\(\\boldsymbol{Y}\\) and \\((\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)})\\), where \\((\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)})\\) is the parameter we got in the \\(k\\)-th iteration (\\(k&gt;1\\)) and \\((\\boldsymbol{\\theta}_{(0)},\\boldsymbol{\\Psi}_{(0)})\\) is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get \\((\\boldsymbol{\\theta}_{(k+1)},\\boldsymbol{\\Psi}_{(k+1)})\\).\n\n3.2.1 E-Step\nFirst, the joint likelihood of \\((\\boldsymbol{Y},\\boldsymbol{F})\\), denoted as \\(L_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), is given by\n\\[\\begin{align*}\nL_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=&\\prod_{i=1}^nf(\\boldsymbol{y}_i,\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nf(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\\\\n=&\\prod_{i=1}^nN(\\boldsymbol{y}_i;\\boldsymbol{\\Lambda}\\boldsymbol{f}_i,\\boldsymbol{\\Psi})N(\\boldsymbol{f}_i;\\boldsymbol{0}_k,\\boldsymbol{I}_k)\\\\\n=&\\prod_{i=1}^n[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Psi})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}\\\\\n&(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\}][(2\\pi)^{-\\frac{k}{2}}\\det({\\boldsymbol{I}_k})^{-\\frac{1}{2}}\\exp\\{-\\frac{1}{2}\\boldsymbol{f}_i^\\top\\boldsymbol{I}_k^{-1}\\boldsymbol{f}_i\\}]\\\\\n= &(2\\pi)^{-\\frac{n}{2}(p+k)}(\\prod_{i=1}^p\\psi_{jj})^{-\\frac{n}{2}}\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2}{\\psi_{jj}}\\}\\\\\n&\\exp\\{-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{F}(i,)\\boldsymbol{F}(i,)^\\top\\}.\n\\end{align*}\\]\nFurthermore, the log-likelihood, denoted as \\(l_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), is given by\n\\[\\begin{equation}\nl_{\\boldsymbol{Y},\\boldsymbol{F}}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2}{\\psi_{jj}}-\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{F}(i,)\\boldsymbol{F}(i,)^\\top+\\text{constant}.\n\\label{ll}\n\\end{equation}\\]\nNow let us deduce the conditional expectation to \\(\\boldsymbol{F}\\) given \\(\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}\\), denoted as \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\). In \\(\\ref{ll}\\), the first term is independent on \\(\\boldsymbol{F}\\), hence stay the same under conditional expectation. The last term is independent on \\((\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\), therefore we can regard it as a constant in \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\). We only need to consider the term inside the sum in the second term, more specifically, the numerator, we have\n\\[\n(\\boldsymbol{Y}_{ij}-\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j))^2 = \\boldsymbol{Y}_{ij}^2 - 2 \\boldsymbol{Y}_{ij}\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j)+\\boldsymbol{\\theta}(,j)^\\top \\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)\\boldsymbol{\\theta}(,j)\n\\]\nWithout ambiguity, denote \\(\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\) to be the conditional expectation \\(\\mathbb{E}[\\boldsymbol{F}(i,)|\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}]\\) for simplification. Then the conditional expectation \\(El_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})\\) is given by \\[\\begin{align*}\nEl_{(k)}(\\boldsymbol{\\theta},\\boldsymbol{\\Psi})=&\\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{Y}_{ij}^2 - 2 \\boldsymbol{Y}_{ij}\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\boldsymbol{\\theta}(,j)+\\boldsymbol{\\theta}(,j)^\\top \\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\boldsymbol{\\theta}(,j)}{\\psi_{jj}}\\\\\n\\end{align*}\\]\nTo deal with \\(\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]\\) and \\(\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\), we only need to know the mean and variance of conditional distribution \\(\\boldsymbol{F}(i,)|\\boldsymbol{Y},\\boldsymbol{\\theta}_{(k)},\\boldsymbol{\\Psi}_{(k)}\\), where the latter one is denoted as \\(\\mathbb{V}[\\boldsymbol{F}(i,)|_{(k)}]\\). This is because we can always treat \\(\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]\\) as\n\\[\n\\mathbb{E}[\\boldsymbol{F}(i,)^\\top\\boldsymbol{F}(i,)|_{(k)}]= \\mathbb{V}[\\boldsymbol{F}(i,)|_{(k)}]+ \\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}]^\\top\\mathbb{E}[\\boldsymbol{F}(i,)|_{(k)}].\n\\]\nWe have\nTheorem: If \\(\\boldsymbol{\\alpha}\\sim N(\\boldsymbol{\\mu}_\\boldsymbol{\\alpha},\\boldsymbol{\\Sigma_\\boldsymbol{\\alpha}})\\) and \\(\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_\\boldsymbol{\\beta},\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta})\\), then we have \\(\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}})\\), where \\[\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}=\\boldsymbol{\\mu}_\\boldsymbol{\\alpha}+\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta}^{-1}\\boldsymbol{\\beta}\\\\\n& \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}= \\boldsymbol{\\Sigma}_\\boldsymbol{\\alpha}-\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_\\boldsymbol{\\beta}^{-1}\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]^\\top.\n\n\\end{align*}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The EM Algorithm in LASSO Factor Analytic Models</span>"
    ]
  }
]