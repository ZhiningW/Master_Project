[
  {
    "objectID": "method.html",
    "href": "method.html",
    "title": "3  Method",
    "section": "",
    "text": "3.1 E-Step\nNow let us deduce the conditional expectation to \\(\\mathbf{f}\\) given \\(\\mathbf{y},\\boldsymbol{\\theta}_{(t)}\\), denoted as \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\). In Equation 2.3, the first term is independent of \\(\\mathbf{f}\\), hence stay the same under conditional expectation. The last term is independent of \\(\\boldsymbol{\\theta}\\), therefore we can regard it as a constant in \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\).\nWithout ambiguity, denote \\(\\mathbb{E}[f_{ik}|_{(t)}]\\) to be the conditional expectation \\(\\mathbb{E}[f_{ik}|\\mathbf{y},\\boldsymbol{\\theta}_{(t)}]\\) for simplification. Then the conditional expectation \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\) is given by\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)=& \\ \\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{\\mathbb{E}\\left[(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2|_{(t)}\\right]}{\\psi_{jj}} - \\frac{1}{2}\\sum_{i=1}^n\\mathbb{E}(\\mathbf{f}_i^\\top\\mathbf{f}_i|_{(t)})\\\\\n\\end{align*}\n\\]\nTo deal with \\(\\mathbb{E}[\\mathbf{f}_i|_{(t)}]\\) and \\(\\mathbb{E}[\\mathbf{f}_i\\mathbf{f}_i^\\top|_{(t)}]\\), we only need to know the mean and variance of conditional distribution \\(\\mathbf{f}_i|\\mathbf{y},\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\), or equivalently \\(\\mathbf{f}_i|\\mathbf{y}_i,\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\) because of the independency of \\(\\mathbf{f}_i\\) to \\(\\mathbf{y}_j\\) for \\(i\\neq j\\). This is because we can always treat \\(\\mathbb{E}[\\mathbf{f}_i\\mathbf{f}_i^\\top|_{(t)}]\\) as\n\\[\n\\mathbb{E}[\\mathbf{f}_i\\mathbf{f}_i^\\top|_{(t)}]= \\mathbb{V}[\\mathbf{f}_i|_{(t)}]+ \\mathbb{E}[\\mathbf{f}_i|_{(t)}]\\mathbb{E}[\\mathbf{f}_i|_{(t)}]^\\top.\n\\] where \\(\\mathbb{V}[\\mathbf{f}_i|_{(t)}]\\) is the variance of conditional distribution. To deal with this, wed need the following lemma.\nIn our scenario, using Lemma 5.1, we have\n\\[\n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\mathbf{f}_k}= \\boldsymbol{0}_k, \\boldsymbol{\\Sigma}_{\\mathbf{f}_{i}}=\\boldsymbol{I}_k\\\\\n& \\boldsymbol{\\mu}_{\\mathbf{y}_k}= \\boldsymbol{0}_p, \\boldsymbol{\\Sigma}_{\\mathbf{y}_k}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{align*}\n\\]\nand also\n\\[\n\\text{Cov}(\\mathbf{y}_i,\\mathbf{f}_i)=\\text{Cov}(\\boldsymbol{\\Lambda }\\mathbf{f}_i+\\boldsymbol{\\epsilon}_i,\\mathbf{f}_i)=\\boldsymbol{\\Lambda}^\\top.\n\\]\nTherefore we have\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\mathbf{f}_i|_{(t)})=\\boldsymbol{\\mu}_{\\mathbf{f}_i|\\mathbf{y}_i}=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\mathbf{y}_i\\\\\n&\\mathbb{V}(\\mathbf{f}_i|_{(t)})=\\boldsymbol{\\Sigma}_{\\mathbf{f}_i|\\mathbf{y}_i}=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)}.\\\\\n\\end{align*}\n\\]\nFor simplification, let us denote\n\\[\n\\begin{align*}\n&\\boldsymbol{A}:=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\\\\n&\\boldsymbol{B}:=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)},\\\\\n\\end{align*}\n\\]\nwe will get\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\mathbf{f}_i|_{(t)})= \\boldsymbol{A}\\mathbf{y}_i\\\\\n&\\mathbb{E}(\\mathbf{f}_i\\mathbf{f}_i^\\top|_{(t)})= \\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top.\n\\end{align*}\n\\]\nOur expectation will finally be confirmed by\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)= & -\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj}}+\\text{constant}\\\\\n=& -\\frac{n}{2}\\log(\\det(\\boldsymbol{\\Psi}))-\\frac{1}{2}\\text{tr}(\\mathbf{y}\\boldsymbol{\\Psi}^{-1}\\mathbf{y}^\\top)+\\text{tr}(\\boldsymbol{\\Lambda}\\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}\\boldsymbol{\\Psi}^{-1})-\\frac{1}{2}\\text{tr}(\\boldsymbol{\\Lambda}(\\boldsymbol{n\\boldsymbol{B}}+\\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}^\\top\\boldsymbol{\\Psi}^{-1})\n\\end{align*}\n\\tag{3.1}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#parameter-estimation",
    "href": "method.html#parameter-estimation",
    "title": "3  Method",
    "section": "",
    "text": "3.1.1 The EM Algorithm\nWe treat the common factor matrix \\(\\boldsymbol{f}\\) as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of \\((\\boldsymbol{y},\\boldsymbol{f})\\) given \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{\\theta}_{(t)}\\), where \\(\\boldsymbol{\\theta}_{(t)}\\) is the parameter we got in the \\(t\\)-th iteration (\\(t&gt;1\\)) and \\(\\boldsymbol{\\theta}_{(0)}\\) is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get \\(\\boldsymbol{\\theta}_{(t + 1)}\\).\n\n3.1.1.1 E-Step\n\nLemma 3.1 The joint log-likelihood of \\((\\boldsymbol{y},\\boldsymbol{f})\\) is given by \\[\\ell_{\\boldsymbol{y},\\boldsymbol{f}}(\\boldsymbol{\\theta})= -\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2}{\\psi_{jj}} - \\sum_{i=1}^n\\sum_{j=1}^pf_{ij}^2 + \\text{constant}.\\\\\\]\n\n\nProof. First, the joint log-likelihood of \\((\\boldsymbol{y},\\boldsymbol{f})\\) is given by \\[\\ell_{\\boldsymbol{y},\\boldsymbol{f}}(\\boldsymbol{\\theta})=\\sum_{i=1}^n\\log f(\\boldsymbol{y}_i,\\boldsymbol{f}_i) =\\sum_{i=1}^n\\log \\left(f(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\right)\\] where \\(\\boldsymbol{y}_i|\\boldsymbol{f}_i \\sim \\mathcal{N}(\\boldsymbol{\\Lambda}\\boldsymbol{f}_i, \\boldsymbol{\\Psi})\\) and \\(\\boldsymbol{f}_i \\sim \\mathcal{N}(\\boldsymbol{0}_m,\\boldsymbol{I}_m)\\). Therefore we have\n$$\n\\[\\begin{align*}\n\\ell(\\boldsymbol{\\theta})=& \\sum_{i=1}^n\\log f(\\boldsymbol{y}_i|\\boldsymbol{f}_i) + \\sum_{i=1}^n\\log f(\\boldsymbol{f}_i)\\\\\n=& -\\frac{np}{2}\\log(2\\pi)-\\frac{n}{2}\\log\\left[\\det(\\boldsymbol{\\Psi})\\right]-\\frac{1}{2}\\sum_{i=1}^n(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\\\\n&\\quad -\\frac{kp}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\left[\\det(\\boldsymbol{I}_m)\\right]-\\frac{1}{2}(\\boldsymbol{f}_i-\\boldsymbol{0}_k)^\\top\\boldsymbol{I}_m^{-1}(\\boldsymbol{f}_i-\\boldsymbol{0}_m)\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2}\\sum_{j=1}^p\\log \\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)- \\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} + \\frac{1}{2}\\sum_{i=1}^n \\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\boldsymbol{y}_i+ \\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\\text{tr}\\left(\n\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top\\right) -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n\\end{align*}\\]\n$$\n\nNow let us deduce the conditional expectation to \\(\\boldsymbol{f}\\) given \\(\\boldsymbol{y},\\boldsymbol{\\theta}_{(t)}\\), denoted as \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\). In Equation 2.3, the first term is independent of \\(\\boldsymbol{f}\\), hence stay the same under conditional expectation. The last term is independent of \\(\\boldsymbol{\\theta}\\), therefore we can regard it as a constant in \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\).\nWithout ambiguity, denote \\(\\mathbb{E}[f_{ik}|_{(t)}]\\) to be the conditional expectation \\(\\mathbb{E}[f_{ik}|\\boldsymbol{y},\\boldsymbol{\\theta}_{(t)}]\\) for simplification. Then the conditional expectation \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\) is given by\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)=& \\ \\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{\\mathbb{E}\\left[(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2|_{(t)}\\right]}{\\psi_{jj}} - \\frac{1}{2}\\sum_{i=1}^n\\mathbb{E}(\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i|_{(t)})\\\\\n\\end{align*}\n\\]\nTo deal with \\(\\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]\\) and \\(\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]\\), we only need to know the mean and variance of conditional distribution \\(\\boldsymbol{f}_i|\\boldsymbol{Y},\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\), or equivalently \\(\\boldsymbol{f}_i|\\boldsymbol{y}_i,\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\) because of the independency of \\(\\boldsymbol{f}_i\\) to \\(\\boldsymbol{y}_j\\) for \\(i\\neq j\\). This is because we can always treat \\(\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]\\) as\n\\[\n\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]= \\mathbb{V}[\\boldsymbol{f}_i|_{(t)}]+ \\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]\\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]^\\top.\n\\] where \\(\\mathbb{V}[\\boldsymbol{f}_i|_{(t)}]\\) is the variance of conditional distribution. To deal with this, wed need the following lemma.\nIn our scenario, using Lemma 5.1, we have\n\\[\n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{f}_i}= \\boldsymbol{0}_k, \\boldsymbol{\\Sigma}_{\\boldsymbol{f}_{i}}=\\boldsymbol{I}_k\\\\\n& \\boldsymbol{\\mu}_{\\boldsymbol{y}_i}= \\boldsymbol{0}_p, \\boldsymbol{\\Sigma}_{\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{align*}\n\\]\nand also\n\\[\n\\text{Cov}(\\boldsymbol{y}_i,\\boldsymbol{f}_i)=\\text{Cov}(\\boldsymbol{\\Lambda f}_i+\\boldsymbol{\\epsilon}_i,\\boldsymbol{f}_i)=\\boldsymbol{\\Lambda}^\\top.\n\\]\nTherefore we have\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(t)})=\\boldsymbol{\\mu}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{y}_i\\\\\n&\\mathbb{V}(\\boldsymbol{f}_i|_{(t)})=\\boldsymbol{\\Sigma}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)}.\\\\\n\\end{align*}\n\\]\nFor simplification, let us denote\n\\[\n\\begin{align*}\n&\\boldsymbol{A}:=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\\\\n&\\boldsymbol{B}:=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)},\\\\\n\\end{align*}\n\\]\nwe will get\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(t)})= \\boldsymbol{A}\\boldsymbol{y}_i\\\\\n&\\mathbb{E}(\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)})= \\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top.\n\\end{align*}\n\\]\nOur expectation will finally be confirmed by\n\\[\n\\begin{align*}\nEl_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})= & -\\frac{n}{2}\\sum_{j=1}^p\\log{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\Psi_{jj}}+\\text{constant}.\n\\end{align*}\n\\]\n\n\n3.1.1.2 M-step\nIn M-step, we need to maximize so called \\(Q\\)-function with respect to parameters where \\(Q\\)-function is penalized conditional expectation of the log-likelihood, i.e. \\[\nQ(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) = El_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) - \\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\n\\] We add a coefficient \\(\\frac{1}{2}\\) before \\(P_{\\rho}(\\boldsymbol{\\Lambda})\\) for simplification since we notice that the same coefficient occurs in each term of conditional expectation \\(El_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})\\). When execute M-step, we use the following strategy(Ng, Krishnan, and McLachlan 2012):\n\nFind \\(\\boldsymbol{\\Psi}_{(k+1)}\\) using current \\(\\boldsymbol{\\Lambda}_{(t)}\\), i.e. \\[\n  \\boldsymbol{\\Psi}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Psi}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)}).\n\\]\nFind \\(\\boldsymbol{\\Lambda}_{(k+1)}\\) using \\(\\boldsymbol{\\Psi}_{(k+1)}\\) we got in previous step, i.e.  \\[\n  \\boldsymbol{\\Lambda}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(k+1)}).\n\\]\n\nFor step 1, take partial derivative with respect to each \\(\\psi_{jj}\\) and let it equal to zero to find the local maximizer of \\(\\psi_{jj}\\), i.e. \n\\[\n\\frac{\\partial}{\\partial \\psi_{jj}}Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=0.\n\\] By simple calculation, we will have\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\psi_{jj}}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=\n&-\\frac{n}{2}\\frac{1}{\\psi_{jj}}+\\frac{1}{2}\\sum_{i=1}^n\n\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj}^2} \\\\\n&+ \\frac{1}{2}\\sum_{i=1}^n\\frac{\\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top}{\\psi_{jj}^2}.\\\\\n\\end{align*}\n\\]\nThus the update of \\(\\boldsymbol{\\Psi}\\) will be elementwisely given by\n\\[\n\\begin{equation}\n\\psi_{jj,(t+1)}=\\frac{1}{n}\\sum_{i=1}^n y_{ij}^2-\\frac{2}{n}\\sum_{i=1}^ny_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i + \\frac{1}{n}\\sum_{i=1}^n \\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top.\n\\end{equation}\n\\] But notice that the \\(Q\\)-function is not concave globally, therefore we may update \\(\\boldsymbol{\\Psi}\\) selectively. More specifically, we only update \\(\\psi_{jj}\\) when it satisfies \\[\n\\frac{\\partial^2}{\\partial\\psi_{jj}^2}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})\\leq 0,\n\\] i.e. \\[\n\\psi_{jj}\\leq \\frac{2}{n}[\\sum_{i=1}^n(y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i)+\\sum_{i=1}^n\\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top].\n\\] For step 2, let us revise the update formula, we have\n\\[\n\\begin{align*}\n\\boldsymbol{\\Lambda}_{(k+1)}=&\\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda,\\boldsymbol{\\Psi}})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(k+1)})\\\\\n=&\\arg \\max_{\\boldsymbol{\\Lambda}} \\{\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}\\\\\n&-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}}-\\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\\\\\n&-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj,(k+1)}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2}{\\psi_{jj,(k+1)}}+\\text{constant}\\}.\\\\\n\\end{align*}\n\\]\nSince the last three terms do not contain any \\(\\boldsymbol{\\Lambda}\\), so they can be eliminated. After letting \\(P_\\rho(\\boldsymbol{\\Lambda}):=\\rho\\sum_{j=1}^p\\sum_{i=1}^k|\\lambda_{ji}|=\\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\\) as LASSO, we can rewrite it as\n$$ \\[\\begin{align*}\n\\boldsymbol{\\Lambda}_{(k+1)}\n\n=& \\arg \\min_{\\boldsymbol{\\Lambda}}\\{-\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}}\\\\\n\n& + \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}} + \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n  \\}\n\\end{align*}\\] $$\nNotice that the objective function with respect to \\(\\boldsymbol{\\Lambda}(q,)\\) for any given \\(q=1,2,\\dots,p\\) is convex, and due to non-differentiablity of the \\(L_1\\)-norm at certain points, a proximal gradient method can be necessary to optimize \\(\\boldsymbol{\\Lambda}\\) row by row.\nDenote the differentiable part (w.r.t. the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\)) of the objective function as \\[\nG(\\boldsymbol{\\Lambda}):= \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(k+1)}} -\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(k+1)}},\n\\] while the non-differentiable part is denoted as \\[\nH(\\boldsymbol{\\Lambda}) := \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n\\] We will have a nice form for the objective function, under which a proximal gradient method can be applied for rowwisely upgrade the loading matrix.\n\nLemma 3.2 Consider the optimization problem \\[\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{G(\\boldsymbol{x})+H(\\boldsymbol{x})\\},\n\\] where\n\n\\(G(\\boldsymbol{x}): \\mathbb{R}^n \\to \\mathbb{R}\\) is convex, differentiable, and \\(L\\)-Lipschitz continuous,\n\\(H(\\boldsymbol{x}): \\mathbb{R}^n \\to (-\\infty,\\infty]\\) is proper, lower semi-continuous and convex.\n\nThe proximal gradient method is that pick an initial guess \\(\\boldsymbol{x}_0\\), for \\(k=0,1,2,\\dots\\), repeat \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{x}_k - s_k \\nabla G(\\boldsymbol{x})\n\\] \\[\n\\boldsymbol{x}_{k+1} := \\arg \\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{H(\\boldsymbol{x})+\\frac{1}{2s_k}||\\boldsymbol{x}-\\boldsymbol{\\xi}_k||^2_2\\}\n\\] with a properly chosen step size \\(0&lt;s_k&lt;\\frac{1}{L}\\).\n\nOne should notice that in practice, we may choose a sufficiently small constant step size \\(s_k=s\\). In our case, the strategy to update the loading matrix is\n\n\n\n\n\n\nAlgorithm\n\n\n\nFor each row of the \\(\\boldsymbol{\\Lambda}\\), say, \\(\\boldsymbol{\\Lambda}(q,)\\) where \\(q=1,2,\\dots,p\\), initialize it as the updated row of loading matrix in the last M-step respectively. Then for \\(k=1,2,\\dots,\\), update the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\) by repeating \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{\\Lambda}(q,)_k - s_k \\nabla G(\\boldsymbol{\\Lambda}(q,)_k)\n\\] \\[\n\\boldsymbol{\\Lambda}(q,)_{k+1}:=\\text{sign}(\\boldsymbol{\\xi}_k) \\cdot \\max(|\\boldsymbol{\\xi}_k|-\\rho,0)\n\\] where \\(|\\cdot|\\) is the elementwise absolute value and \\(\\nabla G(\\boldsymbol{\\Lambda}(q,))\\) can be calculated by \\[\n\\nabla G(\\boldsymbol{\\Lambda}(q,)) = \\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(q,)^\\top}{\\psi_{qq,(k+1)}}-\\sum_{i=1}^n\\frac{2y_{iq}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{qq,(k+1)}}.\n\\]\n\n\n\n\n\n3.1.2 Overall Algorithm for Fitting LASSO-penalized FA\n\n\n\n\n\n\nAlgorithm",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#existing-r-packages",
    "href": "method.html#existing-r-packages",
    "title": "3  Method",
    "section": "3.2 Existing R packages",
    "text": "3.2 Existing R packages",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#selecting-initial-values",
    "href": "method.html#selecting-initial-values",
    "title": "3  Method",
    "section": "3.5 Selecting initial values",
    "text": "3.5 Selecting initial values\nThe initialization strategy of the EM algorithm is of interest to discuss. Although the update of \\(\\boldsymbol{\\Lambda}\\) is a convex optimization problem, an appropriate initial value can still make the convergence faster. Here are some ways to find a satisfying initial value:\n\nrandomly chosen values,\nuse the result of MLE as the initial value,\nuse the result of traditional two-step method as the initial value,\nstart from a very small order, like \\(k=1\\). Then use the result from the smaller order model as the first \\(k\\) columns of the loading matrix.\n\nThe comparison of these strategies will be demonstrated in the simulation part.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#simulation-setting",
    "href": "method.html#simulation-setting",
    "title": "3  Method",
    "section": "3.6 Simulation setting",
    "text": "3.6 Simulation setting\nCriteria? Judge based on estimation? Sparsity structure.\n\n3.6.1 Setting 1\n\nHirose + other existing simulation settings in published papers\nDifferent FA order (k = 2, 4, 6, 8 x n = 200, 400, 800, 1600 x sparse vs non-sparse).\n\n\n\n3.6.2 Setting 2\n\n\n\n\nNg, S. K., T. Krishnan, and G. J. McLachlan. 2012. “The EM Algorithm.” In Handbook of Computational Statistics: Concepts and Methods, 139–72. Springer.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "2  Background",
    "section": "",
    "text": "2.1 Factor analytic model\nFactor analysis (FA) is a statistical method which attempts to use fewer underlying factors to explain the correlation between a large set of observed variables (Mardia, Kent, and Bibby 1979). FA provides a useful tool for exploring the covariance structure among observable variables (Hirose and Yamamoto 2015).\nAn FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g. in psychology where certain variables, like intelligence, cannot be directly measured (Mardia, Kent, and Bibby 1979).\nSuppose we have an observable random vector \\(\\mathbf{y}_i\\in \\mathbb{R}^p\\) for the \\(i\\)-th subject with mean \\(\\mathbb{E}(\\mathbf{y}_i)=\\boldsymbol{\\mu}\\) and variance \\(\\mathbb{V}(\\mathbf{y}_i)=\\boldsymbol{\\Sigma}\\). Then a \\(k\\)-order factor analysis model for \\(\\mathbf{y}_i\\) can be given by\n\\[\n\\mathbf{y}_i=\\boldsymbol{\\mu}+\\boldsymbol{\\Lambda} \\boldsymbol{f}_i+\\boldsymbol{\\epsilon}_i,\n\\tag{2.1}\\]\nwhere \\(\\boldsymbol{\\Lambda} \\in \\mathbb{R}^{p \\times k}\\), \\(\\boldsymbol{f}_i \\in \\mathbb{R}^{k}\\) is and \\(\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{p}\\) are called the loading matrix, common factors and unique (or specific) factors, respectively. The order \\(k\\) is usually much smaller than \\(p\\). For simplicity, we conduct a centralization to those \\(\\mathbf{y}_i\\) and assume that \\(\\boldsymbol{\\mu} = \\boldsymbol{0}_p\\).\nTo make the model well-defined, we may assume\n\\[\\mathbb{E}\\left(\\begin{bmatrix}\\boldsymbol{f}_i\\\\\\boldsymbol{\\epsilon}_i\\end{bmatrix}\\right) = \\begin{bmatrix}\\boldsymbol{0}_k\\\\\\boldsymbol{0}_p\\end{bmatrix}\\quad\\text{and}\\quad\\mathbb{V}\\left(\\begin{bmatrix}\\boldsymbol{f}_i\\\\\\boldsymbol{\\epsilon}_i\\end{bmatrix}\\right) =\\begin{bmatrix}\\mathbf{I}_k & \\mathbf{0}_{k\\times p}\\\\\\mathbf{0}_{p\\times k} & \\mathbf{\\Psi}\\end{bmatrix},\\]\nwhere \\(\\boldsymbol{\\Psi}\\) is a \\(p\\times p\\) diagonal matrix where we denote the \\(i\\)-th diagonal entry as \\(\\psi_{ii}\\). Based on this assumption, the covariance of observable vector \\(\\mathbf{y}_i\\) can be modelled by\n\\[\\mathbb{V}(\\mathbf{y}_i|\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi} )=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}.\\]\nAlternatively, we can write the model as\n\\[\\mathbf{y} = (\\mathbf{I}_n \\otimes \\boldsymbol{\\Lambda})\\boldsymbol{f} + \\boldsymbol{\\epsilon}\\] where \\(\\mathbf{y} =\n\\begin{pmatrix}\n\\mathbf{y}_1 \\\\ \\mathbf{y}_2 \\\\ \\vdots \\\\ \\mathbf{y}_n\n\\end{pmatrix}\\), \\(\\boldsymbol{f} =\n\\begin{pmatrix}\n\\boldsymbol{f}_1 \\\\ \\boldsymbol{f}_2 \\\\ \\vdots \\\\  \\boldsymbol{f}_n\n\\end{pmatrix}\\) and \\(\\boldsymbol{\\epsilon} =\n\\begin{pmatrix}\n\\boldsymbol{\\epsilon}_1 \\\\ \\boldsymbol{\\epsilon}_2 \\\\ \\vdots \\\\  \\boldsymbol{\\epsilon}_n\n\\end{pmatrix}\\). Thus we have\n\\[\\mathbb{E}\\left(\\begin{bmatrix}\\boldsymbol{f}\\\\\\boldsymbol{\\epsilon}\\end{bmatrix}\\right) = \\begin{bmatrix}\\boldsymbol{0}_{nk}\\\\\\boldsymbol{0}_{np}\\end{bmatrix}\\quad\\text{and}\\quad\\mathbb{V}\\left(\\begin{bmatrix}\\boldsymbol{f}\\\\\\boldsymbol{\\epsilon}\\end{bmatrix}\\right) =\\begin{bmatrix}\\mathbf{I}_{nk} & \\mathbf{0}_{nk\\times np}\\\\\\mathbf{0}_{np\\times nk} & \\mathbf{I}_n \\otimes \\mathbf{\\Psi}\\end{bmatrix}.\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#indeterminacy-of-the-loading-matrix",
    "href": "background.html#indeterminacy-of-the-loading-matrix",
    "title": "2  Background",
    "section": "2.2 Indeterminacy of the loading matrix",
    "text": "2.2 Indeterminacy of the loading matrix\nOne can easily see that if our factor analytic model is given by (1), then it can also be modelled as \\[\\mathbf{y}=(\\boldsymbol{\\Lambda}\\boldsymbol{M})(\\boldsymbol{M}^\\top\\boldsymbol{f}) +\\boldsymbol{\\mu}+\\boldsymbol{\\epsilon}\\] where the matrix \\(\\boldsymbol{M}\\) is orthogonal and simultaneously the variance of \\(\\mathbf{y}\\) given by (2) still holds, since \\[\\mathbb{V}[\\mathbf{y}]=(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)\\mathbb{V}[\\boldsymbol{f}](\\boldsymbol{\\Lambda}\\boldsymbol{M}\\boldsymbol{M}^\\top)^\\top+\\boldsymbol{\\Psi}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}.\\] Therefore a rotated loading matrix \\(\\boldsymbol{\\Lambda}\\boldsymbol{M}\\) is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like (Mardia, Kent, and Bibby 1979) \\[\\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\Psi}^{-1} \\boldsymbol{\\Lambda} \\text{ is diagonal.}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#parameter-estimation",
    "href": "background.html#parameter-estimation",
    "title": "2  Background",
    "section": "2.3 Parameter Estimation",
    "text": "2.3 Parameter Estimation\nWe denote the set of parameters by \\(\\boldsymbol{\\theta} = \\{\\text{vec}(\\boldsymbol{\\Lambda}), \\text{diag}(\\boldsymbol{\\Psi})\\}\\) where \\(\\text{vec}(\\cdot)\\) is the vectorisation of the input matrix and \\(\\text{diag}(\\cdot)\\) is the diagonal elements of the input matrix.\nTraditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.\n\n2.3.1 Maximum Likelihood Estimation\nSuppose we have \\(n\\) independent and identically distributed observations \\(\\mathbf{y}_1,\\mathbf{y}_2,\\dots,\\mathbf{y}_n\\) from a \\(p\\)-dimensional multi-variate normal distribution \\(\\mathcal{N}_p(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\). Now denote \\(f_{\\mathbf{y}}(\\mathbf{y};\\boldsymbol{\\theta})\\) as the probability density function of the random vector \\(\\mathbf{y}\\) corresponding to \\(\\mathbf{y}\\). Then the likelihood is given by\n\\[\nL(\\boldsymbol{\\theta};\\mathbf{y})=f_{\\mathbf{y}}(\\mathbf{y};\\boldsymbol{\\theta}) = \\prod^n_{i=1}f_{\\mathbf{y}_i}(\\mathbf{y}_i;\\boldsymbol{\\theta}) =  \\prod^n_{i=1}\\left[(2\\pi)^{-\\frac{p}{2}}\\det(\\boldsymbol{\\Sigma})^{-\\frac{1}{2}}\\exp(-\\frac{1}{2}(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu}))\\right].\n\\tag{2.2}\\]\nThe maximum likelihood estimate (MLE) of \\(\\boldsymbol{\\theta}\\), denoted as \\(\\hat{\\boldsymbol{\\theta}}\\), is found by finding \\(\\boldsymbol{\\theta}\\) that maximizes Equation 2.2. However, it is often more convenient to maximize the log likelihood function. To be more computational friendly, a better form of log likelihood is given by\n\nLemma 2.1 The log-likelihood is given by\n\\[\\ell(\\boldsymbol{\\theta}) =  -\\frac{n}{2}\\left[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})\\right] \\tag{2.3}\\]\nwhere \\(\\boldsymbol{S}\\) is the sample covariance defined as \\(\\boldsymbol{S}=\\frac{1}{n}\\sum^n_{i=1}(\\mathbf{y}_i-\\boldsymbol{\\mu})(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\).\n\n\nProof. \\[\n\\begin{align*}\n\\ell(\\boldsymbol{\\theta})=& \\sum^n_{i=1}\\left[-\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log(\\det(\\boldsymbol{\\Sigma}))-\\frac{1}{2}(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu})\\right]\\\\\n=& -\\frac{n}{2}\\left[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\frac{1}{n}\\sum_{i=1}^n(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu})\\right]\\\\\n=& -\\frac{n}{2}\\left[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})\\right].\n\\end{align*}\n\\]\nThe last equality holds from the following fact: \\[\n\\begin{align*}\n&\\boldsymbol{S}=\\frac{1}{n}\\sum^n_{i=1}(\\mathbf{y}_i-\\boldsymbol{\\mu})(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\\\\n\\Leftrightarrow \\ & \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}=\\frac{1}{n}\\sum^n_{i=1}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu})(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\\\\n\\Leftrightarrow \\ & \\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}) = \\frac{1}{n} \\sum^n_{i=1} \\text{tr} (\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu})(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top)\\\\\n\\Leftrightarrow \\ & \\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}) = \\frac{1}{n}\\sum_{i=1}^n(\\mathbf{y}_i-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\mu})\n\\end{align*}\n\\]\n\nThe MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\boldsymbol{\\Lambda}}\\ell(\\boldsymbol{\\theta})&=0 \\\\\n\\frac{\\partial}{\\partial \\boldsymbol{\\Psi}}\\ell(\\boldsymbol{\\theta})&= 0.\n\\end{align*}\n\\tag{2.4}\\]\nHowever, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximization (EM) algorithm (Rubin and Thayer 1982), we will discuss this algorithm in the last part of background.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#rotation-techniques",
    "href": "background.html#rotation-techniques",
    "title": "2  Background",
    "section": "2.4 Rotation Techniques",
    "text": "2.4 Rotation Techniques\nAfter estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method (Hirose and Yamamoto 2015).\nSuppose \\(Q(\\boldsymbol{\\Lambda})\\) is an criterion for \\(\\boldsymbol{\\Lambda}\\) in the rotation procedure, and we may express it as \\(Q(\\boldsymbol{\\Lambda}):= \\sum^p_{i=1}\\sum^d_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\) where \\(P(\\cdot)\\) is some loss function(Hirose and Yamamoto 2015). Specifically, if we set \\(P(\\cdot)=|\\cdot|\\), we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as (Jennrich 2004)\n\\[\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_0\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\n\\]\nwhere \\(\\boldsymbol{\\Lambda}_0\\) is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is\n\\[\n\\begin{align*}\n&\\min_{\\boldsymbol{\\Lambda}} \\sum^p_{i=1}\\sum^k_{j=1}P(\\boldsymbol{\\Lambda}_{ij})\\\\\n&\\text{subject to } \\boldsymbol{\\Lambda}=\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\boldsymbol{M} \\text{ and } \\boldsymbol{M}^\\top\\boldsymbol{M}=\\boldsymbol{I}_k,\n\\end{align*}\n\\]\nwhere \\(\\hat{\\boldsymbol{\\Lambda}}_{\\text{MLE}}\\) is the maximum likelihood estimator.\n\n2.4.1 Discussion about two-step method\nThe traditional two-step method faces significant shortcomings, primarily its unsuitability (Hirose and Yamamoto 2015). Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by \\[\n\\boldsymbol{\\Lambda}=\n\\begin{bmatrix}\n0.8 & 0 \\\\\n0 & 0.8 \\\\\n0.6 & 0\\\\\n0 & 0.7\\\\\n0 & 0.6\\\\\n\\end{bmatrix}.\n\\] and the real \\(\\boldsymbol{\\Psi}\\) is given by \\[\n\\boldsymbol{\\Psi}=\\text{diag}(0.1,0.2,0.2,0.1,0.1).\n\\] We generate \\(\\mathbf{y}\\in \\mathbb{R}^{{10000}\\times 5}\\) from a multivariate normal distribution with mean \\(\\mathbb{0}_{5}\\) and covariance \\(\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\\). We utilize the R-function factanal() to produce the loading matrix via MLE, where this R-function will unitize varimax method to achieve rotation automatically. What we finally get is\n\\[\n\\hat{\\boldsymbol{\\Lambda}}=\n\\begin{bmatrix}\n0 & 0.997\\\\\n0.872 & 0\\\\\n0 & 0.748\\\\\n0.915 & 0\\\\\n0.882 & 0\\\\\n\\end{bmatrix},\n\\hat{\\boldsymbol{\\Psi}}=\\text{diag}(0.005,0.240,0.440,0.163,0.223)\n\\] This result gives the same sparsity result. However, the result of loading matrix in the first and last two dimensions are much larger than the real one, which implies overfitness.\nSimulation code:\n\nset.seed(123)\nn &lt;- 10000 # Number of observations\n# True loading matrix\ntrue_loading &lt;- matrix(c(0.8,0,0.6,0,0,0,0.8,0,0.7,0.6),nrow=5,ncol=2) \n# True psi matrix\ntrue_psi &lt;- diag(c(0.1,0.2,0.2,0.1,0.1))\n# Generate samples from multivariate normal distribution\nsamples &lt;- MASS::mvrnorm(n = n, mu = rep(0, 5), \n                         Sigma = true_loading %*% t(true_loading) + true_psi)\n# Traditional two step procedure\n# the function factanal has already used varimax method to get the optimal loading\nfa_result &lt;- factanal(factors = 2, covmat = cor(samples))\n# Result\nfa_result\n\n\nCall:\nfactanal(factors = 2, covmat = cor(samples))\n\nUniquenesses:\n[1] 0.005 0.240 0.440 0.163 0.222\n\nLoadings:\n     Factor1 Factor2\n[1,]          0.997 \n[2,]  0.872         \n[3,]          0.748 \n[4,]  0.915         \n[5,]  0.882         \n\n               Factor1 Factor2\nSS loadings      2.375   1.555\nProportion Var   0.475   0.311\nCumulative Var   0.475   0.786\n\nThe degrees of freedom for the model is 1 and the fit was 2e-04 \n\n\nAnother problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model (Mardia, Kent, and Bibby 1979). We have \\[\\begin{align*}\n  &&&H_k: k \\text{ common factors are sufficient to describe the data } \\\\\n  \\longleftrightarrow &&&H_a: \\text{ Otherwise}.\n\\end{align*}\\] The test statistics is given by \\(\\text{TS}=nF(\\hat{\\boldsymbol{\\Lambda}},\\hat{\\boldsymbol{\\Psi}})\\) which has an asymptotic \\(\\chi^2_s\\) distribution with \\(s=\\frac{1}{2}(p-k)^2-(p+k)\\) by the property of MLE, where \\(F\\) is given by (Mardia, Kent, and Bibby (1979)) \\[\nF(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})=\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})-\\log(\\det(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S}))-p.\n\\] Typically, the test is conducted at a \\(5\\%\\) significant level. We can start the procedure from a very small \\(k\\), say, \\(k=1\\) or \\(k=2\\) and then increase \\(k\\) until the null hypothesis is not rejected.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#penalized-likelihood-method",
    "href": "background.html#penalized-likelihood-method",
    "title": "2  Background",
    "section": "2.5 Penalized Likelihood Method",
    "text": "2.5 Penalized Likelihood Method\nPenalized likelihood method can be viewed as a generalization of two-step method mentioned above (Hirose and Yamamoto 2015). A penalized factor analytic model can be obtained by solving following optimization problem \\[\n\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}}\\ell_p := \\ell(\\boldsymbol{\\theta})-P(\\boldsymbol{\\theta};\\rho).\n\\] where we call \\(\\ell_p\\) as the penalized likelihood, \\(\\rho\\) is called regularization parameter or tuning parameter, and we treat \\(P(\\boldsymbol{\\theta};\\rho)\\) as the penalty. There are many types of penalized functions developed, such as LASSO (\\(P(\\boldsymbol{\\Lambda})=\\sum_{i=1}^p\\sum_{j=1}^k|\\lambda_{ij}|\\)) and MC+ ($P(;;)={i=1}^p{j=1}^k { n(|{ij}|-)I(|{ij}|&lt;)+I(|_{ij}|)} $)(Hirose and Yamamoto 2015)，where \\(\\gamma\\) is another tuning parameter to keep a balance between unbiasedness and concavity. In this article, we will mainly focus on the LASSO penalty.\n\n2.5.1 LASSO penalty and LASSO estimator\nAgain, recall that the LASSO penalized likelihood is given by \\[\\ell_p=-\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|\\] and the LASSO estimator, denoted as \\((\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*)\\), can be obtained via\n\\[\n\\begin{align*}\n(\\boldsymbol{\\Lambda}^*,\\boldsymbol{\\Psi}^*) &= \\text{arg}\\max_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad -\\frac{n}{2}[p\\log(2\\pi)+\\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})]-\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|\\\\\n&=\\text{arg}\\min_{\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}} \\quad \\log(\\det(\\boldsymbol{\\Sigma}))+\\text{tr}(\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{S})+\\frac{2}{n}\\rho\\sum^p_{i=1}\\sum^k_{j=1}|\\lambda_{ij}|.\n\\end{align*}\n\\] Nevertheless, the objective function is non_differentiable at some point because of the penalization term. Therefore we need to find other approaches to obtain the estimator instead of solving",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "background.html#sec-EM",
    "href": "background.html#sec-EM",
    "title": "2  Background",
    "section": "2.6 The EM Algorithm",
    "text": "2.6 The EM Algorithm\nThe Expectation-Maximization (EM) algorithm is a widely used iterative method to find the MLE, especially when the model depends on latent variables. The EM algorithm iteratively apply two distinct steps: the Expectation step (E-step) and the Maximization step (M-step). More specifically, we define\n\\[Q(\\boldsymbol{\\theta}|\\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}\\left(\\ell (\\boldsymbol{\\theta}; \\mathbf{y})\\bigg|\\mathbf{y},\\boldsymbol{\\theta}^{(t)}\\right).\\] Then for the \\((t + 1)\\)-th interation, the steps involve:\n\nE-step: Compute \\(Q(\\boldsymbol{\\theta}|\\boldsymbol{\\theta}^{(t)})\\).\nM-step: Update \\(\\boldsymbol{\\theta}^{(t+1)}\\) as \\(\\boldsymbol{\\theta}\\) that maximses \\(Q(\\boldsymbol{\\theta}|\\boldsymbol{\\theta}^{(t)})\\).\n\nThe M-step may be replaced with updating \\(\\boldsymbol{\\theta}^{(t+1)}\\) such that \\(Q(\\boldsymbol{\\theta}^{(t+1)}|\\boldsymbol{\\theta}^{(t)}) \\geq Q(\\boldsymbol{\\theta}^{(t)}|\\boldsymbol{\\theta}^{(t)})\\).\nWe iterate between E-step and M-step until convergence. The convergence may be determined by a criteria such as \\(||\\boldsymbol{\\theta}^{(t+1)}-\\boldsymbol{\\theta}^{(t)}||_p\\leq \\epsilon\\) for some \\(p\\)-norm \\(||\\cdot||_p\\) and \\(\\epsilon &gt; 0\\).\nThe EM algorithm approaches the problem of solving the likelihood equation indirectly by proceeding iteratively in terms of \\(\\ell(\\boldsymbol{\\theta};\\mathbf{y})\\). But it is unobservable since it includes missing part of the data, then we use the conditional expectation given \\(\\mathbf{y}\\) and current fit for \\(\\boldsymbol{\\theta}\\).\nNow we need to show the correctness of the EM-algorithm. In other words, by iteratively processing the EM-algorithm, the likelihood will keep increasing (at least non-decreasing). First we need to show the following lemma.\n\nLemma 2.2 (Convergence of the EM Algorithm) Suppose the likelihood is upper-bounded. Then for all \\(\\epsilon &gt; 0\\), there exists \\(t &gt; t_0\\) such that \\(||\\boldsymbol{\\theta}^{(t)} - \\hat{\\boldsymbol{\\theta}}|| &lt; \\epsilon\\) where \\(\\hat{\\boldsymbol{\\theta}}\\) is the MLE.\n\n\nProof. By the definition of conditional likelihood, our likelihood of complete data, i.e. the observable and unobservable data, can be expressed by \\[\nL_{\\boldsymbol{X}}(\\boldsymbol{\\theta}) = f_{\\boldsymbol{X}}(\\boldsymbol{x};\\boldsymbol{\\theta})=L_{\\mathbf{y}}(\\boldsymbol{\\theta})f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}),\n\\] and hence the log-likelihood is given by \\[\n\\log L_{\\boldsymbol{X}}(\\boldsymbol{\\theta}) = \\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}) + \\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}).\n\\] Take expectation to both sides of the equation with respect to \\(\\boldsymbol{x|y}\\) and replace \\(\\boldsymbol{\\theta}\\) by \\(\\boldsymbol{\\theta}^{(k)}\\), we will have \\[\nQ(\\boldsymbol{\\theta};\\boldsymbol{\\theta}^{(k)}) = \\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}) + \\mathbb{E}_{\\boldsymbol{X}}[\\log f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta})|\\mathbf{y},\\boldsymbol{\\theta}^{(k)}].\n\\] Now consider the difference of log-likelihood of \\(\\mathbf{y}\\) function between two iterations, we have \\[\n\\begin{align*}\n  \\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(k+1)})-\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(k)}) =\n  &\\{Q(\\boldsymbol{\\theta}^{(k+1)};\\boldsymbol{\\theta}^{(k)})-Q(\\boldsymbol{\\theta}^{(k)};\\boldsymbol{\\theta}^{(k)})\\}\\\\\n  &-\\{\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}^{(k+1)})|\\mathbf{y},\\boldsymbol{\\theta}^{(k)}]\\\\\n  &-\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}^{(k)}|\\mathbf{y},\\boldsymbol{\\theta}^{(k)}]\\}.\n\\end{align*}\n\\] By the procedure of EM-algorithm, we always have \\(Q(\\boldsymbol{\\theta}^{(k+1)};\\boldsymbol{\\theta}^{(k)})\\geq Q(\\boldsymbol{\\theta}^{(k)};\\boldsymbol{\\theta}^{(k)})\\). By the Gibbs’s inequality, we have \\(\\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}^{(k+1)})|\\mathbf{y},\\boldsymbol{\\theta}^{(k)}] \\leq \\mathbb{E}_{\\boldsymbol{X}}[\\log         f_{\\boldsymbol{X|Y}}(\\boldsymbol{x}|\\mathbf{y};\\boldsymbol{\\theta}^{(k)}|\\mathbf{y},\\boldsymbol{\\theta}^{(k)}].\\) Therefore during iterations, the sequence of log-likelihood of observed data \\(\\mathbf{y}\\) given by \\(\\{\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(n)})\\}_{n\\geq 1}:=\\{\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(1)}),\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(2)}),\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(3)}),\\dots\\}\\), is increasing. Now replace the term \\(\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(k+1)})\\) by \\(\\ell_{\\mathbf{y}}(\\hat{\\boldsymbol{{\\theta}}})\\), we will have \\[\n\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(k)}) \\leq \\ell_{\\mathbf{y}}(\\hat{\\boldsymbol{{\\theta}}})\n\\] for all \\(k=1,2,\\dots\\). Therefore the upper-bound of the sequence \\(\\{\\ell_{\\mathbf{y}}(\\boldsymbol{\\theta}^{(n)})\\}_{n\\geq 1}\\) is \\(\\ell_{\\mathbf{y}}(\\hat{\\boldsymbol{\\theta}})\\). By the monotone convergence theorem, and the definition of convergence, we finished.\n\n\n\n\n\nHirose, Kei, and Michio Yamamoto. 2015. “Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.” Statistics and Computing 25 (5): 863–75. https://doi.org/10.1007/s11222-014-9458-0.\n\n\nJennrich, R. I. 2004. “Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.” Psychometrika 69: 257–73.\n\n\nMardia, K. V., J. T. Kent, and J. M. Bibby. 1979. Multivariate Analysis. Probability and Mathematical Statistics. London ; New York: Academic Press.\n\n\nRubin, Donald B., and Dorothy T. Thayer. 1982. “EM Algorithms for ML Factor Analysis.” Psychometrika 47: 69–76.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Model Selection for Factor Analysis",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "method.html#parameter-estimation-the-em-algorithm",
    "href": "method.html#parameter-estimation-the-em-algorithm",
    "title": "3  Method",
    "section": "",
    "text": "3.1.1 E-Step\n\nLemma 3.1 The joint log-likelihood of \\((\\boldsymbol{y},\\boldsymbol{f})\\) is given by \\[\\ell_{\\boldsymbol{y},\\boldsymbol{f}}(\\boldsymbol{\\theta})= -\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2}{\\psi_{jj}} - \\sum_{i=1}^n\\sum_{j=1}^pf_{ij}^2 + \\text{constant}.\\\\\\]\n\n\nProof. First, the joint log-likelihood of \\((\\boldsymbol{y},\\boldsymbol{f})\\) is given by \\[\n\\ell_{\\boldsymbol{y},\\boldsymbol{f}}(\\boldsymbol{\\theta})=\\sum_{i=1}^n\\log f(\\boldsymbol{y}_i,\\boldsymbol{f}_i) =\\sum_{i=1}^n\\log \\left(f(\\boldsymbol{y}_i|\\boldsymbol{f}_i)f(\\boldsymbol{f}_i)\\right)\n\\] where \\(\\boldsymbol{y}_i|\\boldsymbol{f}_i \\sim \\mathcal{N}(\\boldsymbol{\\Lambda}\\boldsymbol{f}_i, \\boldsymbol{\\Psi})\\) and \\(\\boldsymbol{f}_i \\sim \\mathcal{N}(\\boldsymbol{0}_m,\\boldsymbol{I}_m)\\). Therefore we have\n$$ \\[\\begin{align*}\n\\ell(\\boldsymbol{\\theta})=& \\sum_{i=1}^n\\log f(\\boldsymbol{y}_i|\\boldsymbol{f}_i) + \\sum_{i=1}^n\\log f(\\boldsymbol{f}_i)\\\\\n=& -\\frac{np}{2}\\log(2\\pi)-\\frac{n}{2}\\log\\left[\\det(\\boldsymbol{\\Psi})\\right]-\\frac{1}{2}\\sum_{i=1}^n(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)\\\\\n&\\quad -\\frac{kp}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\left[\\det(\\boldsymbol{I}_m)\\right]-\\frac{1}{2}(\\boldsymbol{f}_i-\\boldsymbol{0}_k)^\\top\\boldsymbol{I}_m^{-1}(\\boldsymbol{f}_i-\\boldsymbol{0}_m)\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2}\\sum_{j=1}^p\\log \\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\boldsymbol{y}_i-\\boldsymbol{\\Lambda}\\boldsymbol{f}_i)- \\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} + \\frac{1}{2}\\sum_{i=1}^n \\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\boldsymbol{y}_i+ \\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\boldsymbol{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\boldsymbol{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\\text{tr}\\left(\n\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top\\right) -\\frac{1}{2}\\sum_{i=1}^n\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i\\\\\n\\end{align*}\\] $$\n\nNow let us deduce the conditional expectation to \\(\\boldsymbol{f}\\) given \\(\\boldsymbol{y},\\boldsymbol{\\theta}_{(t)}\\), denoted as \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\). In Equation 2.3, the first term is independent of \\(\\boldsymbol{f}\\), hence stay the same under conditional expectation. The last term is independent of \\(\\boldsymbol{\\theta}\\), therefore we can regard it as a constant in \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\).\nWithout ambiguity, denote \\(\\mathbb{E}[f_{ik}|_{(t)}]\\) to be the conditional expectation \\(\\mathbb{E}[f_{ik}|\\boldsymbol{y},\\boldsymbol{\\theta}_{(t)}]\\) for simplification. Then the conditional expectation \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})\\right)\\) is given by\n\\[\n\\begin{align*}\n\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)=& \\ \\text{constant}-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj}}-\\\\\n&\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{\\mathbb{E}\\left[(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2|_{(t)}\\right]}{\\psi_{jj}} - \\frac{1}{2}\\sum_{i=1}^n\\mathbb{E}(\\boldsymbol{f}_i^\\top\\boldsymbol{f}_i|_{(t)})\\\\\n\\end{align*}\n\\]\nTo deal with \\(\\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]\\) and \\(\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]\\), we only need to know the mean and variance of conditional distribution \\(\\boldsymbol{f}_i|\\boldsymbol{Y},\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\), or equivalently \\(\\boldsymbol{f}_i|\\boldsymbol{y}_i,\\boldsymbol{\\Lambda}^\\top_{(t)},\\boldsymbol{\\Psi}_{(t)}\\) because of the independency of \\(\\boldsymbol{f}_i\\) to \\(\\boldsymbol{y}_j\\) for \\(i\\neq j\\). This is because we can always treat \\(\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]\\) as\n\\[\n\\mathbb{E}[\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)}]= \\mathbb{V}[\\boldsymbol{f}_i|_{(t)}]+ \\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]\\mathbb{E}[\\boldsymbol{f}_i|_{(t)}]^\\top.\n\\] where \\(\\mathbb{V}[\\boldsymbol{f}_i|_{(t)}]\\) is the variance of conditional distribution. To deal with this, wed need the following lemma.\nIn our scenario, using Lemma 5.1, we have\n\\[\n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{f}_i}= \\boldsymbol{0}_k, \\boldsymbol{\\Sigma}_{\\boldsymbol{f}_{i}}=\\boldsymbol{I}_k\\\\\n& \\boldsymbol{\\mu}_{\\boldsymbol{y}_i}= \\boldsymbol{0}_p, \\boldsymbol{\\Sigma}_{\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}\n\\end{align*}\n\\]\nand also\n\\[\n\\text{Cov}(\\boldsymbol{y}_i,\\boldsymbol{f}_i)=\\text{Cov}(\\boldsymbol{\\Lambda f}_i+\\boldsymbol{\\epsilon}_i,\\boldsymbol{f}_i)=\\boldsymbol{\\Lambda}^\\top.\n\\]\nTherefore we have\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(t)})=\\boldsymbol{\\mu}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{y}_i\\\\\n&\\mathbb{V}(\\boldsymbol{f}_i|_{(t)})=\\boldsymbol{\\Sigma}_{\\boldsymbol{f}_i|\\boldsymbol{y}_i}=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)}.\\\\\n\\end{align*}\n\\]\nFor simplification, let us denote\n\\[\n\\begin{align*}\n&\\boldsymbol{A}:=\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\\\\n&\\boldsymbol{B}:=\\boldsymbol{I}_k-\\boldsymbol{\\Lambda}_{(t)}^\\top(\\boldsymbol{\\Lambda}_{(t)}\\boldsymbol{\\Lambda}_{(t)}^\\top+\\boldsymbol{\\Psi}_{(t)})^{-1}\\boldsymbol{\\Lambda}_{(t)},\\\\\n\\end{align*}\n\\]\nwe will get\n\\[\n\\begin{align*}\n&\\mathbb{E}(\\boldsymbol{f}_i|_{(t)})= \\boldsymbol{A}\\boldsymbol{y}_i\\\\\n&\\mathbb{E}(\\boldsymbol{f}_i\\boldsymbol{f}_i^\\top|_{(t)})= \\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top.\n\\end{align*}\n\\]\nOur expectation will finally be confirmed by\n\\[\n\\begin{align*}\nEl_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})= & -\\frac{n}{2}\\sum_{j=1}^p\\log{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\Psi_{jj}}\\\\\n& -\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\Psi_{jj}}+\\text{constant}.\n\\end{align*}\n\\tag{3.1}\\]\n\n\n3.1.2 M-step\nIn M-step, we need to maximize so called \\(Q\\)-function with respect to parameters where \\(Q\\)-function is penalized conditional expectation of the log-likelihood, i.e. \\[\nQ(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) = El_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) - \\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\n\\] We add a coefficient \\(\\frac{1}{2}\\) before \\(P_{\\rho}(\\boldsymbol{\\Lambda})\\) for simplification since we notice that the same coefficient occurs in each term of conditional expectation \\(El_{(t)}(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})\\). When execute M-step, we use the following strategy(Ng, Krishnan, and McLachlan 2012):\n\nFind \\(\\boldsymbol{\\Psi}_{(k+1)}\\) using current \\(\\boldsymbol{\\Lambda}_{(t)}\\), i.e. \\[\n  \\boldsymbol{\\Psi}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Psi}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)}).\n\\]\nFind \\(\\boldsymbol{\\Lambda}_{(k+1)}\\) using \\(\\boldsymbol{\\Psi}_{(k+1)}\\) we got in previous step, i.e.  \\[\n  \\boldsymbol{\\Lambda}_{(k+1)} = \\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(k+1)}).\n\\]\n\nFor step 1, take partial derivative with respect to each \\(\\psi_{jj}\\) and let it equal to zero to find the local maximizer of \\(\\psi_{jj}\\), i.e. \n\\[\n\\frac{\\partial}{\\partial \\psi_{jj}}Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=0.\n\\] By simple calculation, we will have\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\psi_{jj}}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=\n&-\\frac{n}{2}\\frac{1}{\\psi_{jj}}+\\frac{1}{2}\\sum_{i=1}^n\n\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj}^2} \\\\\n&+ \\frac{1}{2}\\sum_{i=1}^n\\frac{\\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top}{\\psi_{jj}^2}.\\\\\n\\end{align*}\n\\]\nThus the update of \\(\\boldsymbol{\\Psi}\\) will be elementwisely given by\n\\[\n\\begin{equation}\n\\psi_{jj,(t+1)}=\\frac{1}{n}\\sum_{i=1}^n y_{ij}^2-\\frac{2}{n}\\sum_{i=1}^ny_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i + \\frac{1}{n}\\sum_{i=1}^n \\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top.\n\\end{equation}\n\\tag{3.2}\\]\nBut notice that the \\(Q\\)-function is not concave globally, therefore we may update \\(\\boldsymbol{\\Psi}\\) selectively. More specifically, we only update \\(\\psi_{jj}\\) when it satisfies\n\\[\n\\frac{\\partial^2}{\\partial\\psi_{jj}^2}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})\\leq 0,\n\\] i.e. \\[\n\\psi_{jj}\\leq \\frac{2}{n}[\\sum_{i=1}^n(y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}_{(t)}(j,)\\boldsymbol{A}\\boldsymbol{y}_i)+\\sum_{i=1}^n\\boldsymbol{\\Lambda}_{(t)}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}_{(t)}(j,)^\\top].\n\\tag{3.3}\\] For step 2, let us revise the update formula, we have\n\\[\n\\begin{align*}\n\\boldsymbol{\\Lambda}_{(t+1)}=&\\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda,\\boldsymbol{\\Psi}})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(t+1)})\\\\\n=&\\arg \\max_{\\boldsymbol{\\Lambda}} \\{\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(t+1)}}\\\\\n&-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}}-\\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\\\\\n&-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj,(t+1)}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2}{\\psi_{jj,(t+1)}}+\\text{constant}\\}.\\\\\n\\end{align*}\n\\]\nSince the last three terms do not contain any \\(\\boldsymbol{\\Lambda}\\), so they can be eliminated. After letting \\(P_\\rho(\\boldsymbol{\\Lambda}):=\\rho\\sum_{j=1}^p\\sum_{i=1}^k|\\lambda_{ji}|=\\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\\) as LASSO, we can rewrite it as\n$$ \\[\\begin{align*}\n\\boldsymbol{\\Lambda}_{(k+1)}\n\n=& \\arg \\min_{\\boldsymbol{\\Lambda}}\\{-\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(t+1)}}\\\\\n\n& + \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}} + \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n  \\}\n\\end{align*}\\] $$ The objective function enjoys a nice property given by the following lemma:\n\nLemma 3.2 The objective function is convex with respective to \\(\\boldsymbol{\\Lambda}_{q\\cdot}\\) for any given \\(q=1,2,\\dots,p\\).\n\n\nProof. To show Lemma 3.2, we only need to show \\(\\sum_{i=1}^n (\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)=n\\boldsymbol{B}+\\boldsymbol{AY}^\\top\\boldsymbol{YA}^\\top\\) is positive semi-definite. Consider an arbitrary vector \\(\\boldsymbol{\\eta}\\in\\mathbb{R}^k\\), we have \\[\n\\begin{align*}\n\\boldsymbol{\\eta}^\\top (n \\boldsymbol{B} + \\boldsymbol{AY}^\\top \\boldsymbol{YA}^\\top) \\boldsymbol{\\eta}& = n \\boldsymbol{\\eta}^\\top \\boldsymbol{B} \\boldsymbol{\\eta} + \\boldsymbol{\\eta}^\\top \\boldsymbol{AY}^\\top  \\boldsymbol{YA}^\\top \\boldsymbol{\\eta}\\\\\n& = n \\boldsymbol{\\eta}^\\top \\boldsymbol{B} \\boldsymbol{\\eta} + || \\boldsymbol{YA}^\\top \\boldsymbol{\\eta} ||_2^2\n\\end{align*}.\n\\] The second term is greater or equal to zero and for the first term, consider the following block-structured matrix \\[\n\\boldsymbol{P}=\n\\begin{pmatrix}\n\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi} & \\boldsymbol{\\Lambda}\\\\\n\\boldsymbol{\\Lambda}^\\top & \\boldsymbol{I}_k\\\\\n\\end{pmatrix}\n\\] We can verify that the top-left matrix, denoted as \\(\\boldsymbol{P}_{11}\\) is positive semi-definite since \\(\\boldsymbol{\\eta}^\\top (\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}) \\boldsymbol{\\eta} = ||\\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\eta}||_2^2 + \\boldsymbol{\\eta}^\\top \\boldsymbol{\\Psi} \\boldsymbol{\\eta}\\) and the \\(\\boldsymbol{Psi}\\) is a diagonal matrix with all the non-zero elements positive as definited. Therefore the matrix \\(\\boldsymbol{P}\\) is positive semi-definite and hence its Schur complementary is also positive semi-definite. The Schur complementary with respect to \\(\\boldsymbol{P}_{11}\\) is given by \\(\\boldsymbol{I}_k - \\boldsymbol{\\Lambda}^\\top (\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi})\\), which is \\(\\boldsymbol{B}\\) we previously defined.\n\nDue to the convexity proved before and non-differentiablity of the \\(L_1\\)-norm at certain points, a proximal gradient method can be necessary to optimize \\(\\boldsymbol{\\Lambda}\\) row by row.\nDenote the differentiable part (w.r.t. the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\)) of the objective function as \\[\nG(\\boldsymbol{\\Lambda}):= \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}} -\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{jj,(t+1)}},\n\\] while the non-differentiable part is denoted as \\[\nH(\\boldsymbol{\\Lambda}) := \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n\\] We will have a nice form for the objective function, under which a proximal gradient method can be applied for rowwisely upgrade the loading matrix.\n\nLemma 3.3 Consider the optimization problem \\[\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{G(\\boldsymbol{x})+H(\\boldsymbol{x})\\},\n\\] where\n\n\\(G(\\boldsymbol{x}): \\mathbb{R}^n \\to \\mathbb{R}\\) is convex, differentiable, and \\(L\\)-Lipschitz continuous,\n\\(H(\\boldsymbol{x}): \\mathbb{R}^n \\to (-\\infty,\\infty]\\) is proper, lower semi-continuous and convex.\n\nThe proximal gradient method is that pick an initial guess \\(\\boldsymbol{x}_0\\), for \\(k=0,1,2,\\dots\\), repeat \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{x}_k - s_k \\nabla G(\\boldsymbol{x})\n\\] \\[\n\\boldsymbol{x}_{k+1} := \\arg \\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{H(\\boldsymbol{x})+\\frac{1}{2s_k}||\\boldsymbol{x}-\\boldsymbol{\\xi}_k||^2_2\\}\n\\] with a properly chosen step size \\(0&lt;s_k&lt;\\frac{1}{L}\\).\n\nOne should notice that in practice, we may choose a sufficiently small constant step size \\(s_k=s\\). In our case, the strategy to update the loading matrix is\n\nFor each row of the \\(\\boldsymbol{\\Lambda}\\), say, \\(\\boldsymbol{\\Lambda}(q,)\\) where \\(q=1,2,\\dots,p\\), initialize it as the updated row of loading matrix in the last M-step respectively. Then for \\(k=1,2,\\dots,\\), update the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\) by repeating \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{\\Lambda}(q,)_{(t)} - s_k \\nabla G(\\boldsymbol{\\Lambda}(q,)_{(t)})\n\\tag{3.4}\\] \\[\n\\boldsymbol{\\Lambda}(q,)_{(t+1)}:=\\text{sign}(\\boldsymbol{\\xi}_k) \\cdot \\max(|\\boldsymbol{\\xi}_k|-\\rho,0)\n\\tag{3.5}\\] where \\(|\\cdot|\\) is the elementwise absolute value and \\(\\nabla G(\\boldsymbol{\\Lambda}(q,))\\) can be calculated by \\[\n\\nabla G(\\boldsymbol{\\Lambda}(q,)) = \\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(q,)^\\top}{\\psi_{qq,(k+1)}}-\\sum_{i=1}^n\\frac{2y_{iq}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{qq,(k+1)}}.\n\\tag{3.6}\\]\n\n\n\n3.1.3 Overall Algorithm for Fitting LASSO-penalized FA\n\n\n\n\n\n\nAlgorithm\n\n\n\n\nSet reasonable initial value \\(\\boldsymbol{\\Psi}_{(0)}\\) and \\(\\boldsymbol{\\Lambda}_{(0)}\\).\nCalculate the expectation using formula Equation 3.1.\nUpdate \\(\\boldsymbol{\\Psi}\\) using formulae Equation 3.2 and Equation 3.3.\nUpdate \\(\\boldsymbol{\\Lambda}\\) using proximal method Equation 3.5, Equation 3.4, and Equation 3.6\nRepeat 2,3, and 4 until convergence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "5  Appendix",
    "section": "",
    "text": "5.1 Multivariate Normal distribution\nSuppose that a \\(p\\)-vector random variable \\(\\boldsymbol{y} \\sim N_p(\\boldsymbol{\\mu},\\mathbf{\\Sigma})\\), then the probability density function of \\(\\boldsymbol{y}\\) is given as:\n\\[f(\\boldsymbol{y}) = (2\\pi)^{-\\frac{p}{2}}|\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}}\\exp\\left(-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu})\\right).\\]\n\\[\\log f(\\boldsymbol{y}) = -\\frac{p}{2}\\log(2\\pi)-\\frac{1}{2}\\log|\\boldsymbol{\\Sigma}|-\\frac{1}{2}(\\boldsymbol{y}-\\boldsymbol{\\mu})^\\top\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu}).\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "appendix.html#conditional",
    "href": "appendix.html#conditional",
    "title": "5  Appendix",
    "section": "5.2 Conditional",
    "text": "5.2 Conditional\n\nLemma 5.1 If \\(\\boldsymbol{\\alpha}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}},\\boldsymbol{\\Sigma_{\\boldsymbol{\\alpha}}})\\) and \\(\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}})\\), then we have \\(\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}\\sim N(\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}},\\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}})\\), where \\[\n\\begin{align*}\n& \\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}}|\\boldsymbol{\\beta}=\\boldsymbol{\\mu}_{\\boldsymbol{\\alpha}}+\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}\\boldsymbol{\\beta}\\\\\n& \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}|\\boldsymbol{\\beta}}= \\boldsymbol{\\Sigma}_{\\boldsymbol{\\alpha}}-\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]\\boldsymbol{\\Sigma}_{\\boldsymbol{\\beta}}^{-1}\\boldsymbol{Cov}[\\boldsymbol{\\alpha},\\boldsymbol{\\beta}]^\\top.\n\\end{align*}\n\\]\n\n\nLemma 5.2 Let \\(\\boldsymbol{x}\\sim \\mathcal{N}(\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}_1)\\) and \\(\\boldsymbol{y}\\sim \\mathcal{N}(\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}_2)\\), by definition of multivariate normal distribution, we have \\[\n\\begin{pmatrix}\n\\boldsymbol{x}\\\\\n\\boldsymbol{y}\n\\end{pmatrix}\n\\sim \\mathcal{N}(\n\\begin{pmatrix}\n\\boldsymbol{\\mu_x}\\\\\n\\boldsymbol{\\mu_y}\n\\end{pmatrix}\n,\n\\begin{pmatrix}\n\\boldsymbol{\\Sigma_{xx}} & \\boldsymbol{\\Sigma_{xy}}\\\\\n\\boldsymbol{\\Sigma_{yx}} & \\boldsymbol{\\Sigma_{yy}}\n\\end{pmatrix}\n).\n\\] The conditional distribution \\(\\boldsymbol{x}|\\boldsymbol{y}\\) is then given by \\[\n\\boldsymbol{x}|\\boldsymbol{y} \\sim \\mathcal{N}(\\boldsymbol{\\mu_{x|y}},\\boldsymbol{\\Sigma_{x|y}}),\n\\] where \\[\n\\begin{align*}\n& \\boldsymbol{\\mu_{x|y}} = \\boldsymbol{\\mu_x}+\\boldsymbol{\\Sigma_{xy}}\\boldsymbol{\\Sigma_{yy}}^{-1}(\\boldsymbol{y}-\\boldsymbol{\\mu_y})\\\\\n& \\boldsymbol{\\Sigma_{x|y}} = \\boldsymbol{\\Sigma_{xx}}-\\boldsymbol{\\Sigma_{xy}}\\boldsymbol{\\Sigma_{yy}}^{-1}\\boldsymbol{\\Sigma_{yx}}\n\\end{align*}.\n\\]\n\n\nProof. See Appendix A.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Appendix</span>"
    ]
  },
  {
    "objectID": "method.html#proximal-method-in-m-step",
    "href": "method.html#proximal-method-in-m-step",
    "title": "3  Method",
    "section": "3.2 Proximal Method in M-Step",
    "text": "3.2 Proximal Method in M-Step\nFor each row of the \\(\\boldsymbol{\\Lambda}\\), say, \\(\\boldsymbol{\\Lambda}(q,)\\) where \\(q=1,2,\\dots,p\\), initialize it as the updated row of loading matrix in the last M-step respectively. Then for \\(k=1,2,\\dots,\\), update the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\) by repeating \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{\\Lambda}(q,)_{(t)} - s_k \\nabla G(\\boldsymbol{\\Lambda}(q,)_{(t)})\n\\tag{3.4}\\] \\[\n\\boldsymbol{\\Lambda}(q,)_{(t+1)}:=\\text{sign}(\\boldsymbol{\\xi}_k) \\cdot \\max(|\\boldsymbol{\\xi}_k|-\\rho,0)\n\\tag{3.5}\\] where \\(|\\cdot|\\) is the elementwise absolute value and \\(\\nabla G(\\boldsymbol{\\Lambda}(q,))\\) can be calculated by \\[\n\\nabla G(\\boldsymbol{\\Lambda}(q,)) = \\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\boldsymbol{y}_i\\boldsymbol{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(q,)^\\top}{\\psi_{qq,(k+1)}}-\\sum_{i=1}^n\\frac{2y_{iq}\\boldsymbol{A}\\boldsymbol{y}_i}{\\psi_{qq,(k+1)}}.\n\\tag{3.6}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Multivariate data, where each observational unit consists of multiple outcomes, are abundant across multitude of disciplines (e.g. in psychology, multiple test scores from different courses for a student; in agriculture, yield measurements at different environments for the same crop variety; and in biology, gene expressions from genes of the same organism). A key analysis of multivariate data include understanding or predicting the characteristics of observational units. A simple analysis may assess one outcome at a time using some univariate technique, however, a more sophisticated form of an analysis would consider multiple outcomes simultaneously and exploit the correlated structure within the data. While the latter analysis would be more desirable to make the most out of the data, there are several challenges as explained next.\nSuppose we have a multivariate data denoted as a \\(n \\times p\\) matrix, \\(\\textbf{Y}\\), where \\(n \\in \\mathbb{Z}^+\\) is the number of observational units and \\(p \\in \\mathbb{Z}^+\\) is the number of dependent variables. The entry in the \\(i\\)-th row and \\(j\\)-th column of \\(\\textbf{Y}\\) is written as \\(y_{ij}\\). We may write the matrix as \\(\\mathbf{Y} = (y_{ij})\\), \\(\\mathbf{Y} = \\begin{bmatrix}\\boldsymbol{y}_{(1)} & \\cdots & \\boldsymbol{y}_{(p)}\\end{bmatrix}\\).\nNotation used throughout this thesis is presented in Table 1.1.\n\n\n\nTable 1.1: List of notations used in this report.\n\n\n\n\n\n\n\n\n\nNotation\nDescription\n\n\n\n\n\\(\\boldsymbol{A}^\\top\\)\nthe transpose of the matrix (or vector) \\(A\\)\n\n\n\\(\\mathbb{R}^{p}\\)\nthe space of all \\(p\\)-dimensional real column vectors like \\([a_1,a_2,\\dots,a_p]^\\top\\)\n\n\n\\(\\mathbb{R}^{p\\times q}\\)\nthe space of all real matrices with size \\(p\\times q\\)\n\n\n\\(\\mathbb{E}(\\cdot)\\)\nthe expectation, or mean of a random variable\n\n\n\\(\\mathbb{V}(\\cdot)\\)\nthe variance, or covariance of a random variable\n\n\n\\(\\boldsymbol{Cov}(\\cdot , \\cdot)\\)\nthe covariance of two random variables\n\n\n\\(\\boldsymbol{0}_{p}\\) and \\(\\boldsymbol{0}_{p\\times q}\\)\nthe \\(p\\)-dimensional \\(0\\) vector or \\(0\\) matrix in size \\(p\\times q\\) respectively\n\n\n\\(\\boldsymbol{I}_p\\)\nthe identity matrix in size \\(p\\times p\\)\n\n\n\\(\\det(\\cdot)\\)\nthe determinant of a matrix\n\n\n\\(\\boldsymbol{m}_i\\)\nthe \\(i^{th}\\) row of the matrix \\(\\textbf{M}\\)\n\n\n\\(\\boldsymbol{m}_{(j)}\\)\nthe \\(j^{th}\\) column of the matrix \\(\\textbf{M}\\)\n\n\n\n\n\n\nThis thesis is structured as follows. Chapter 2 outlines the background information on factor analytics models, EM algorithm, and penalised likelihood estimation. Chapter 3 describe the methods and simulation settings. Chapter 4 presents the results from the simulation with discussion.\nThis thesis is written reproducibly using the Quarto system (Allaire and Dervieux 2024) and all the code to reproduce the thesis and results can be found at https://github.com/ZhiningW/Master_Project.\n\n\n\n\nAllaire, JJ, and Christophe Dervieux. 2024. Quarto: R Interface to ’Quarto’ Markdown Publishing System. https://CRAN.R-project.org/package=quarto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "method.html#e-step",
    "href": "method.html#e-step",
    "title": "3  Method",
    "section": "",
    "text": "Lemma 3.1 The joint log-likelihood of \\((\\mathbf{y},\\mathbf{f})\\) is given by \\[\\ell_{\\mathbf{y},\\mathbf{f}}(\\boldsymbol{\\theta})= -\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p \\frac{(y_{ij} - \\sum_{k=1}^m \\lambda_{jk}f_{ik})^2}{\\psi_{jj}} - \\sum_{i=1}^n\\sum_{j=1}^pf_{ij}^2 + \\text{constant}.\\\\\\]\n\n\nProof. First, the joint log-likelihood of \\((\\mathbf{y},\\mathbf{f})\\) is given by \\[\n\\ell_{\\mathbf{y},\\mathbf{f}}(\\boldsymbol{\\theta})=\\sum_{i=1}^n\\log f(\\mathbf{y}_i,\\mathbf{f}_i) =\\sum_{i=1}^n\\log \\left(f(\\mathbf{y}_i|\\mathbf{f}_i)f(\\mathbf{f}_i)\\right)\n\\] where \\(\\mathbf{y}_i|\\mathbf{f}_i \\sim \\mathcal{N}(\\boldsymbol{\\Lambda}\\mathbf{f}_i, \\boldsymbol{\\Psi})\\) and \\(\\mathbf{f}_i \\sim \\mathcal{N}(\\boldsymbol{0}_m,\\boldsymbol{I}_m)\\). Therefore we have\n$$ \\[\\begin{align*}\n\\ell(\\boldsymbol{\\theta})=& \\sum_{i=1}^n\\log f(\\mathbf{y}_i|\\mathbf{f}_i) + \\sum_{i=1}^n\\log f(\\mathbf{f}_i)\\\\\n=& -\\frac{np}{2}\\log(2\\pi)-\\frac{n}{2}\\log\\left[\\det(\\boldsymbol{\\Psi})\\right]-\\frac{1}{2}\\sum_{i=1}^n(\\mathbf{y}_i-\\boldsymbol{\\Lambda}\\mathbf{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\Lambda}\\mathbf{f}_i)\\\\\n&\\quad -\\frac{kp}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\left[\\det(\\boldsymbol{I}_m)\\right]-\\frac{1}{2}(\\mathbf{f}_i-\\boldsymbol{0}_k)^\\top\\boldsymbol{I}_m^{-1}(\\mathbf{f}_i-\\boldsymbol{0}_m)\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2}\\sum_{j=1}^p\\log \\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n(\\mathbf{y}_i-\\boldsymbol{\\Lambda}\\mathbf{f}_i)^\\top\\boldsymbol{\\Psi}^{-1}(\\mathbf{y}_i-\\boldsymbol{\\Lambda}\\mathbf{f}_i)- \\frac{1}{2}\\sum_{i=1}^n\\mathbf{f}_i^\\top\\mathbf{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} + \\frac{1}{2}\\sum_{i=1}^n \\mathbf{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{y}_i+ \\frac{1}{2}\\sum_{i=1}^n\\mathbf{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\mathbf{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\mathbf{f}_i^\\top\\mathbf{f}_i\\\\\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\mathbf{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\n\\mathbf{f}_i^\\top\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i -\\frac{1}{2}\\sum_{i=1}^n\\mathbf{f}_i^\\top\\mathbf{f}_i\\\\\n\n=& -\\frac{(n+k)p}{2}\\log(2\\pi)-\\frac{n}{2} \\sum_{j=1}^p\\log\\psi_{jj}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j}^p \\frac{y_{ij}^2}{\\psi_{jj}} +   \\sum_{i=1}^n \\mathbf{y}_i^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i  - \\frac{1}{2}\\sum_{i=1}^n\\text{tr}\\left(\n\\mathbf{\\Lambda}^\\top\\mathbf{\\Psi}^{-1}\\mathbf{\\Lambda}\\mathbf{f}_i\\mathbf{f}_i^\\top\\right) -\\frac{1}{2}\\sum_{i=1}^n\\mathbf{f}_i^\\top\\mathbf{f}_i\\\\\n\\end{align*}\\] $$",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#m-step-standard-em-algorithm",
    "href": "method.html#m-step-standard-em-algorithm",
    "title": "3  Method",
    "section": "3.2 M-step: Standard EM-algorithm",
    "text": "3.2 M-step: Standard EM-algorithm\nIn M-step, we need to maximize so called \\(Q\\)-function with respect to parameters. In the standard scenario, the \\(Q\\)-function is directly the expectation given by Equation 3.1, i.e. \\[\nQ(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) = \\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)\n\\] When execute M-step, we use the following strategy(Ng, Krishnan, and McLachlan 2012):\n\nFind \\(\\boldsymbol{\\Psi}_{(t+1)}\\) using current \\(\\boldsymbol{\\Lambda}_{(t)}\\), i.e. \\[\n  \\boldsymbol{\\Psi}_{(t+1)} = \\arg \\max_{\\boldsymbol{\\Psi}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)}).\n\\]\nFind \\(\\boldsymbol{\\Lambda}_{(k+1)}\\) using \\(\\boldsymbol{\\Psi}_{(k+1)}\\) we got in previous step, i.e.  \\[\n  \\boldsymbol{\\Lambda}_{(t+1)} = \\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(t+1)}).\n\\]\n\nFor step 1, take partial derivative with respect to each \\(\\psi_{jj}\\) and let it equal to zero to find the local maximizer of \\(\\psi_{jj}\\), i.e. \n\\[\n\\frac{\\partial}{\\partial \\psi_{jj}}Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=0.\n\\]\nBy simple calculation, we will have\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial \\psi_{jj}}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})=\n&-\\frac{n}{2}\\frac{1}{\\psi_{jj}}+\\frac{1}{2}\\sum_{i=1}^n\n\\frac{y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}^{(t)}[j,:]\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj}^2} \\\\\n&+ \\frac{1}{2}\\sum_{i=1}^n\\frac{\\boldsymbol{\\Lambda}^{(t)}[j,:](\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}^{(t)}[j,:]^\\top}{\\psi_{jj}^2}.\\\\\n\\end{align*}\n\\]\nThus the update of \\(\\boldsymbol{\\Psi}\\) will be elementwisely given by\n\\[\n\\begin{align*}\n\\psi_{jj,(t+1)} &= \\frac{1}{n} \\sum_{i=1}^n y_{ij}^2 - \\frac{2}{n} \\sum_{i=1}^n y_{ij} \\boldsymbol{\\Lambda}^{(t)}[j,:] \\boldsymbol{A} \\mathbf{y}_i + \\frac{1}{n}\\sum_{i=1}^n \\boldsymbol{\\Lambda}^{(t)}[j,:] (\\boldsymbol{B} + \\boldsymbol{A} \\mathbf{y}_i \\mathbf{y}_i^\\top \\boldsymbol{A}^\\top) \\boldsymbol{\\Lambda}^{(t)}[j,:]^\\top \\\\\n&= \\frac{1}{n} \\mathbf{y}[,j]^\\top \\mathbf{y}[,j] - \\frac{2}{n} \\boldsymbol{\\Lambda}_{(t)}[j,] \\boldsymbol{A} \\mathbf{y}^\\top \\mathbf{y}[\\cdot,j] + \\frac{1}{n} \\boldsymbol{\\Lambda}_{(t)}[j,] (n\\boldsymbol{B} + \\boldsymbol{A} \\mathbf{y}^\\top \\mathbf{y}\\boldsymbol{A}^\\top) \\boldsymbol{\\Lambda}_{(t)}[j,]^\\top\n\\end{align*}.\n\\tag{3.2}\\]\nBut notice that the \\(Q\\)-function is not concave globally, therefore we may update \\(\\boldsymbol{\\Psi}\\) selectively. More specifically, we only update \\(\\psi_{jj}\\) when it satisfies\n\\[\n\\frac{\\partial^2}{\\partial\\psi_{jj}^2}Q((\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})|\\boldsymbol{\\Lambda}=\\boldsymbol{\\Lambda}_{(t)})\\leq 0,\n\\]\ni.e.\n\\[\n\\begin{align*}\n\\psi_{jj} &\\leq \\frac{2}{n}[\\sum_{i=1}^n(y_{ij}^2-2y_{ij}\\boldsymbol{\\Lambda}^{(t)}[j,:]\\boldsymbol{A}\\mathbf{y}_i)+\\sum_{i=1}^n\\boldsymbol{\\Lambda}^{(t)}[j,:](\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}^{(t)}[j,:]^\\top]\\\\\n&= \\frac{2}{n}( \\mathbf{y}[,j]^\\top \\mathbf{y}[,j] -  2\\boldsymbol{\\Lambda}_{(t)}[j,] \\boldsymbol{A} \\mathbf{y}^\\top \\mathbf{y}[\\cdot,j] + \\boldsymbol{\\Lambda}_{(t)}[j,] (n\\boldsymbol{B} + \\boldsymbol{A} \\mathbf{y}^\\top \\mathbf{y}\\boldsymbol{A}^\\top) \\boldsymbol{\\Lambda}_{(t)}[j,]^\\top)\n\\end{align*}\n\\tag{3.3}\\]\nFor step 2, let us revise the update formula, we have\n\\[\n\\begin{align*}\n\\boldsymbol{\\Lambda}_{(t+1)}=&\\arg \\max_{\\boldsymbol{\\Lambda}} (Q(\\boldsymbol{\\Lambda,\\boldsymbol{\\Psi}})|\\boldsymbol{\\Psi}=\\boldsymbol{\\Psi}_{(t+1)})\\\\\n=&\\arg \\max_{\\boldsymbol{\\Lambda}} \\{\\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj,(t+1)}}\\\\\n&-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}}\\\\\n&-\\frac{n}{2}\\sum_{j=1}^p\\log{\\psi_{jj,(t+1)}}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^p\\frac{y_{ij}^2}{\\psi_{jj,(t+1)}}+\\text{constant}\\}.\\\\\n\\end{align*}\n\\]\nSince the last three terms do not contain any \\(\\boldsymbol{\\Lambda}\\), so they can be eliminated. Therefore we have\n$$ \\[\\begin{align*}\n\\boldsymbol{\\Lambda}_{(t+1)}\n\n=& \\arg \\min_{\\boldsymbol{\\Lambda}}\\{-\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj,(t+1)}}\\\\\n\n& + \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}} \\}\n\\end{align*}\\] $$\nThe objective function, denoted as \\(G(\\boldsymbol{\\Lambda})\\) enjoys a nice convexity property given by the following lemma:\n\nLemma 3.2 The objective function is convex with respective to \\(\\boldsymbol{\\Lambda}_{q\\cdot}\\) for any given \\(q=1,2,\\dots,p\\).\n\n\nProof. To show Lemma 3.2, we only need to show \\(\\sum_{i=1}^n (\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)=n\\boldsymbol{B}+\\boldsymbol{AY}^\\top\\boldsymbol{YA}^\\top\\) is positive semi-definite. Consider an arbitrary vector \\(\\boldsymbol{\\eta}\\in\\mathbb{R}^k\\), we have \\[\n\\begin{align*}\n\\boldsymbol{\\eta}^\\top (n \\boldsymbol{B} + \\boldsymbol{AY}^\\top \\boldsymbol{YA}^\\top) \\boldsymbol{\\eta}& = n \\boldsymbol{\\eta}^\\top \\boldsymbol{B} \\boldsymbol{\\eta} + \\boldsymbol{\\eta}^\\top \\boldsymbol{AY}^\\top  \\boldsymbol{YA}^\\top \\boldsymbol{\\eta}\\\\\n& = n \\boldsymbol{\\eta}^\\top \\boldsymbol{B} \\boldsymbol{\\eta} + || \\boldsymbol{YA}^\\top \\boldsymbol{\\eta} ||_2^2\n\\end{align*}.\n\\] The second term is greater or equal to zero and for the first term, consider the following block-structured matrix\n\\[\n\\boldsymbol{P}=\n\\begin{pmatrix}\n\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi} & \\boldsymbol{\\Lambda}\\\\\n\\boldsymbol{\\Lambda}^\\top & \\boldsymbol{I}_k\\\\\n\\end{pmatrix}\n\\]\nWe can verify that the top-left matrix, denoted as \\(\\boldsymbol{P}_{11}\\) is positive semi-definite since \\(\\boldsymbol{\\eta}^\\top (\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi}) \\boldsymbol{\\eta} = ||\\boldsymbol{\\Lambda}^\\top \\boldsymbol{\\eta}||_2^2 + \\boldsymbol{\\eta}^\\top \\boldsymbol{\\Psi} \\boldsymbol{\\eta}\\) and the \\(\\boldsymbol{\\Psi}\\) is a diagonal matrix with all the non-zero elements positive as defined. Therefore the matrix \\(\\boldsymbol{P}\\) is positive semi-definite and hence its Schur complementary is also positive semi-definite. The Schur complementary with respect to \\(\\boldsymbol{P}_{11}\\) is given by \\(\\boldsymbol{I}_k - \\boldsymbol{\\Lambda}^\\top (\\boldsymbol{\\Lambda}\\boldsymbol{\\Lambda}^\\top+\\boldsymbol{\\Psi})^{-1}\\boldsymbol{\\Lambda}\\), which is \\(\\boldsymbol{B}\\) we previously defined. Therefore \\(n \\boldsymbol{\\eta}^\\top \\boldsymbol{B} \\boldsymbol{\\eta} \\geq 0\\), which leads to \\(\\boldsymbol{\\eta}^\\top (n \\boldsymbol{B} + \\boldsymbol{AY}^\\top \\boldsymbol{YA}^\\top) \\boldsymbol{\\eta} \\geq 0\\) for any \\(\\boldsymbol{\\eta}\\in\\mathbb{R}^k\\).\n\nTherefore we can update \\(\\boldsymbol{\\Lambda}\\) rowwisely using the critical points. Mathematically, the gradient of the object function with respect of the \\(q\\)-th row is given by\n\\[\n\\begin{align*}\n\\nabla G(\\boldsymbol{\\Lambda}(q,)) &= \\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(q,)^\\top}{\\psi_{qq,(t+1)}}-\\sum_{i=1}^n\\frac{2y_{iq}\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{qq,(t+1)}}\\\\\n&= \\frac{2}{\\psi_{qq}}(n\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}[q,]^\\top - \\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}[,q])\n\\end{align*}.\n\\tag{3.4}\\] Let \\(\\nabla G(\\boldsymbol{\\Lambda}(q,))=0\\), we have the formula to update \\(\\boldsymbol{\\Lambda}\\) as\n\\[\n\\boldsymbol{\\Lambda}_{(t+1)}(q,) =  ((n\\boldsymbol{B} + \\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}\\boldsymbol{A}^\\top)^{-1}\\boldsymbol{A}\\mathbf{y}^\\top\\mathbf{y}[,q])^\\top.\n\\tag{3.5}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#m-step-penalized-em-algorithm",
    "href": "method.html#m-step-penalized-em-algorithm",
    "title": "3  Method",
    "section": "3.3 M-step: Penalized EM-algorithm",
    "text": "3.3 M-step: Penalized EM-algorithm\nThe \\(Q\\)-function of the penalized EM-algorithm is the penalized expectation, i.e. \\[\nQ(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi}) = \\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right) - \\frac{1}{2}P_{\\rho}(\\boldsymbol{\\Lambda})\n\\tag{3.6}\\] We add a coefficient \\(\\frac{1}{2}\\) before \\(P_{\\rho}(\\boldsymbol{\\Lambda})\\) for simplification since we notice that the same coefficient occurs in each term of conditional expectation \\(\\mathbb{E}\\left(\\ell(\\boldsymbol{\\theta}_{(t)})|_{(t)}\\right)\\).\nThe maximization towards the \\(Q\\)-function with respect to \\(\\boldsymbol{\\Psi}\\) is exactly the same with Equation 3.2, since we did not add any penalty to matrix \\(\\boldsymbol{\\Psi}\\).\nNow consider the update for \\(\\boldsymbol{\\Lambda}\\), we have\n$$ \\[\\begin{align*}\n\\boldsymbol{\\Lambda}_{(t+1)} &= \\arg \\max_{\\boldsymbol{\\Lambda}}\\{Q(\\boldsymbol{\\Lambda},\\boldsymbol{\\Psi})| \\boldsymbol{\\Psi} = \\boldsymbol{\\Psi}_{t+1} \\} \\\\\n\n=& \\arg \\min_{\\boldsymbol{\\Lambda}}\\{-\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj,(t+1)}}\\\\\n\n& + \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}} + \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n  \\}\n\\end{align*}\\] $$\nDue to the convexity proved before and non-differentiablity of the \\(L_1\\)-norm at certain points, a proximal gradient method can be necessary to optimize \\(\\boldsymbol{\\Lambda}\\) row by row.\nDenote the differentiable part (w.r.t. the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\)) of the objective function as \\[\nG(\\boldsymbol{\\Lambda}):= \\sum_{i=1}^n\\sum_{j=1}^p\\frac{\\boldsymbol{\\Lambda}(j,)(\\boldsymbol{B}+\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(j,)^\\top}{\\psi_{jj,(t+1)}} -\\sum_{i=1}^n \\sum_{j=1}^p \\frac{2y_{ij}\\boldsymbol{\\Lambda}(j,)\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{jj,(t+1)}},\n\\] while the non-differentiable part is denoted as \\[\nH(\\boldsymbol{\\Lambda}) := \\rho \\sum_{j=1}^p||\\boldsymbol{\\Lambda}(j,)^\\top||_1\n\\] We will have a nice form for the objective function, under which a proximal gradient method can be applied for rowwisely upgrade the loading matrix.\n\nLemma 3.3 Consider the optimization problem \\[\n\\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{G(\\boldsymbol{x})+H(\\boldsymbol{x})\\},\n\\] where\n\n\\(G(\\boldsymbol{x}): \\mathbb{R}^n \\to \\mathbb{R}\\) is convex, differentiable, and \\(L\\)-Lipschitz continuous,\n\\(H(\\boldsymbol{x}): \\mathbb{R}^n \\to (-\\infty,\\infty]\\) is proper, lower semi-continuous and convex.\n\nThe proximal gradient method is that pick an initial guess \\(\\boldsymbol{x}_0\\), for \\(k=0,1,2,\\dots\\), repeat \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{x}_k - s_k \\nabla G(\\boldsymbol{x})\n\\] \\[\n\\boldsymbol{x}_{k+1} := \\arg \\min_{\\boldsymbol{x}\\in\\mathbb{R}^n}\\{H(\\boldsymbol{x})+\\frac{1}{2s_k}||\\boldsymbol{x}-\\boldsymbol{\\xi}_k||^2_2\\}\n\\] with a properly chosen step size \\(0&lt;s_k&lt;\\frac{1}{L}\\).\n\nOne should notice that in practice, we may choose a sufficiently small constant step size \\(s_k=s\\). In our case, the strategy to update the loading matrix is\n\n\n\n\n\n\nProximal method for updating loading matrix\n\n\n\nFor each row of the \\(\\boldsymbol{\\Lambda}\\), say, \\(\\boldsymbol{\\Lambda}(q,)\\) where \\(q=1,2,\\dots,p\\), initialize it as the updated row of loading matrix in the last M-step respectively. Then for \\(k=1,2,\\dots,\\), update the \\(q\\)-th row of \\(\\boldsymbol{\\Lambda}\\) by repeating \\[\n\\boldsymbol{\\xi}_k := \\boldsymbol{\\Lambda}(q,)_{(t)} - s_k \\nabla G(\\boldsymbol{\\Lambda}(q,)_{(t)})\n\\tag{3.7}\\] \\[\n\\boldsymbol{\\Lambda}(q,)_{(t+1)}:=\\text{sign}(\\boldsymbol{\\xi}_k) \\cdot \\max(|\\boldsymbol{\\xi}_k|-\\rho,0)\n\\tag{3.8}\\] where \\(|\\cdot|\\) is the elementwise absolute value and \\(\\nabla G(\\boldsymbol{\\Lambda}(q,))\\) can be calculated by \\[\n\\nabla G(\\boldsymbol{\\Lambda}(q,)) = \\sum_{i=1}^n\\frac{(2\\boldsymbol{B}+2\\boldsymbol{A}\\mathbf{y}_i\\mathbf{y}_i^\\top\\boldsymbol{A}^\\top)\\boldsymbol{\\Lambda}(q,)^\\top}{\\psi_{qq,(k+1)}}-\\sum_{i=1}^n\\frac{2y_{iq}\\boldsymbol{A}\\mathbf{y}_i}{\\psi_{qq,(k+1)}}.\n\\tag{3.9}\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  },
  {
    "objectID": "method.html#summary-of-algorithms-em-and-penalized-em",
    "href": "method.html#summary-of-algorithms-em-and-penalized-em",
    "title": "3  Method",
    "section": "3.4 Summary of algorithms: EM and penalized EM",
    "text": "3.4 Summary of algorithms: EM and penalized EM\n\n\n\n\n\n\nAlgorithm: Standard EM-algorithm\n\n\n\n\nSet reasonable initial value \\(\\boldsymbol{\\Psi}_{(0)}\\) and \\(\\boldsymbol{\\Lambda}_{(0)}\\).\nCalculate the expectation using formula Equation 3.1.\nUpdate \\(\\boldsymbol{\\Psi}\\) using formulae Equation 3.2 and Equation 3.3.\nUpdate \\(\\boldsymbol{\\Lambda}\\) using formula Equation 3.5.\nRepeat 2,3, and 4 until convergence.\n\n\n\n\n\n\n\n\n\nAlgorithm: Penalized EM-algorithm\n\n\n\n\nSet reasonable initial value \\(\\boldsymbol{\\Psi}_{(0)}\\) and \\(\\boldsymbol{\\Lambda}_{(0)}\\).\nCalculate the expectation using formula Equation 3.6.\nUpdate \\(\\boldsymbol{\\Psi}\\) using formulae Equation 3.2 and Equation 3.3.\nUpdate \\(\\boldsymbol{\\Lambda}\\) using proximal method Equation 3.8, Equation 3.7, and Equation 3.9\nRepeat 2,3, and 4 until convergence.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Method</span>"
    ]
  }
]