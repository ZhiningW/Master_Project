<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Background – {{&lt; var title &gt;}}</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./method.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./background.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Model Selection for Factor Analysis</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ZhiningW/Master_Project" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./appendix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Appendix</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#factor-analytic-model" id="toc-factor-analytic-model" class="nav-link active" data-scroll-target="#factor-analytic-model"><span class="header-section-number">2.1</span> Factor analytic model</a></li>
  <li><a href="#indeterminacy-of-the-loading-matrix" id="toc-indeterminacy-of-the-loading-matrix" class="nav-link" data-scroll-target="#indeterminacy-of-the-loading-matrix"><span class="header-section-number">2.2</span> Indeterminacy of the loading matrix</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="header-section-number">2.3</span> Parameter Estimation</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="header-section-number">2.3.1</span> Maximum Likelihood Estimation</a></li>
  </ul></li>
  <li><a href="#rotation-techniques" id="toc-rotation-techniques" class="nav-link" data-scroll-target="#rotation-techniques"><span class="header-section-number">2.4</span> Rotation Techniques</a>
  <ul class="collapse">
  <li><a href="#discussion-about-two-step-method" id="toc-discussion-about-two-step-method" class="nav-link" data-scroll-target="#discussion-about-two-step-method"><span class="header-section-number">2.4.1</span> Discussion about two-step method</a></li>
  </ul></li>
  <li><a href="#penalized-likelihood-method" id="toc-penalized-likelihood-method" class="nav-link" data-scroll-target="#penalized-likelihood-method"><span class="header-section-number">2.5</span> Penalized Likelihood Method</a>
  <ul class="collapse">
  <li><a href="#lasso-penalty-and-lasso-estimator" id="toc-lasso-penalty-and-lasso-estimator" class="nav-link" data-scroll-target="#lasso-penalty-and-lasso-estimator"><span class="header-section-number">2.5.1</span> LASSO penalty and LASSO estimator</a></li>
  </ul></li>
  <li><a href="#sec-EM" id="toc-sec-EM" class="nav-link" data-scroll-target="#sec-EM"><span class="header-section-number">2.6</span> The EM Algorithm</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/ZhiningW/Master_Project/edit/main/background.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ZhiningW/Master_Project/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-bg" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="factor-analytic-model" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="factor-analytic-model"><span class="header-section-number">2.1</span> Factor analytic model</h2>
<p>Factor analysis (FA) is a statistical method which attempts to use fewer underlying factors to explain the correlation between a large set of observed variables <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>. FA provides a useful tool for exploring the covariance structure among observable variables <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>.</p>
<p>An FA model assumes that the observations are explained by a small number of underlying factors. This assumption is suited to many areas, e.g.&nbsp;in psychology where certain variables, like intelligence, cannot be directly measured <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>.</p>
<p>Suppose we have an observable random vector <span class="math inline">\(\mathbf{y}_i\in \mathbb{R}^p\)</span> for the <span class="math inline">\(i\)</span>-th subject with mean <span class="math inline">\(\mathbb{E}(\mathbf{y}_i)=\boldsymbol{\mu}\)</span> and variance <span class="math inline">\(\mathbb{V}(\mathbf{y}_i)=\boldsymbol{\Sigma}\)</span>. Then a <span class="math inline">\(k\)</span>-order factor analysis model for <span class="math inline">\(\mathbf{y}_i\)</span> can be given by</p>
<p><span id="eq-fa"><span class="math display">\[
\mathbf{y}_i=\boldsymbol{\mu}+\boldsymbol{\Lambda} \boldsymbol{f}_i+\boldsymbol{\epsilon}_i,
\tag{2.1}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}\)</span>, <span class="math inline">\(\boldsymbol{f}_i \in \mathbb{R}^{k}\)</span> is and <span class="math inline">\(\boldsymbol{\epsilon}_i \in \mathbb{R}^{p}\)</span> are called the <em>loading matrix</em>, <em>common factors</em> and <em>unique (or specific) factors</em>, respectively. The order <span class="math inline">\(k\)</span> is usually much smaller than <span class="math inline">\(p\)</span>. For simplicity, we conduct a centralization to those <span class="math inline">\(\mathbf{y}_i\)</span> and assume that <span class="math inline">\(\boldsymbol{\mu} = \boldsymbol{0}_p\)</span>.</p>
<p>To make the model well-defined, we may assume</p>
<p><span class="math display">\[\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_k\\\boldsymbol{0}_p\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}_i\\\boldsymbol{\epsilon}_i\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_k &amp; \mathbf{0}_{k\times p}\\\mathbf{0}_{p\times k} &amp; \mathbf{\Psi}\end{bmatrix},\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Psi}\)</span> is a <span class="math inline">\(p\times p\)</span> diagonal matrix where we denote the <span class="math inline">\(i\)</span>-th diagonal entry as <span class="math inline">\(\psi_{ii}\)</span>. Based on this assumption, the covariance of observable vector <span class="math inline">\(\mathbf{y}_i\)</span> can be modelled by</p>
<p><span class="math display">\[\mathbb{V}(\mathbf{y}_i|\boldsymbol{\Lambda},\boldsymbol{\Psi} )=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span></p>
<p>Alternatively, we can write the model as</p>
<p><span class="math display">\[\mathbf{y} = (\mathbf{I}_n \otimes \boldsymbol{\Lambda})\boldsymbol{f} + \boldsymbol{\epsilon}\]</span> where <span class="math inline">\(\mathbf{y} =
\begin{pmatrix}
\mathbf{y}_1 \\ \mathbf{y}_2 \\ \vdots \\ \mathbf{y}_n
\end{pmatrix}\)</span>, <span class="math inline">\(\boldsymbol{f} =
\begin{pmatrix}
\boldsymbol{f}_1 \\ \boldsymbol{f}_2 \\ \vdots \\  \boldsymbol{f}_n
\end{pmatrix}\)</span> and <span class="math inline">\(\boldsymbol{\epsilon} =
\begin{pmatrix}
\boldsymbol{\epsilon}_1 \\ \boldsymbol{\epsilon}_2 \\ \vdots \\  \boldsymbol{\epsilon}_n
\end{pmatrix}\)</span>. Thus we have</p>
<p><span class="math display">\[\mathbb{E}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) = \begin{bmatrix}\boldsymbol{0}_{nk}\\\boldsymbol{0}_{np}\end{bmatrix}\quad\text{and}\quad\mathbb{V}\left(\begin{bmatrix}\boldsymbol{f}\\\boldsymbol{\epsilon}\end{bmatrix}\right) =\begin{bmatrix}\mathbf{I}_{nk} &amp; \mathbf{0}_{nk\times np}\\\mathbf{0}_{np\times nk} &amp; \mathbf{I}_n \otimes \mathbf{\Psi}\end{bmatrix}.\]</span></p>
</section>
<section id="indeterminacy-of-the-loading-matrix" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="indeterminacy-of-the-loading-matrix"><span class="header-section-number">2.2</span> Indeterminacy of the loading matrix</h2>
<p>One can easily see that if our factor analytic model is given by (1), then it can also be modelled as <span class="math display">\[\mathbf{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}\]</span> where the matrix <span class="math inline">\(\boldsymbol{M}\)</span> is orthogonal and simultaneously the variance of <span class="math inline">\(\mathbf{y}\)</span> given by (2) still holds, since <span class="math display">\[\mathbb{V}[\mathbf{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.\]</span> Therefore a rotated loading matrix <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{M}\)</span> is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span> <span class="math display">\[\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}\]</span></p>
</section>
<section id="parameter-estimation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">2.3</span> Parameter Estimation</h2>
<p>We denote the set of parameters by <span class="math inline">\(\boldsymbol{\theta} = \{\text{vec}(\boldsymbol{\Lambda}), \text{diag}(\boldsymbol{\Psi})\}\)</span> where <span class="math inline">\(\text{vec}(\cdot)\)</span> is the vectorisation of the input matrix and <span class="math inline">\(\text{diag}(\cdot)\)</span> is the diagonal elements of the input matrix.</p>
<p>Traditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.</p>
<section id="maximum-likelihood-estimation" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">2.3.1</span> Maximum Likelihood Estimation</h3>
<p>Suppose we have <span class="math inline">\(n\)</span> independent and identically distributed observations <span class="math inline">\(\mathbf{y}_1,\mathbf{y}_2,\dots,\mathbf{y}_n\)</span> from a <span class="math inline">\(p\)</span>-dimensional multi-variate normal distribution <span class="math inline">\(\mathcal{N}_p(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span>. Now denote <span class="math inline">\(f_{\mathbf{y}}(\mathbf{y};\boldsymbol{\theta})\)</span> as the probability density function of the random vector <span class="math inline">\(\mathbf{y}\)</span> corresponding to <span class="math inline">\(\mathbf{y}\)</span>. Then the likelihood is given by</p>
<p><span id="eq-likelihood"><span class="math display">\[
L(\boldsymbol{\theta};\mathbf{y})=f_{\mathbf{y}}(\mathbf{y};\boldsymbol{\theta}) = \prod^n_{i=1}f_{\mathbf{y}_i}(\mathbf{y}_i;\boldsymbol{\theta}) =  \prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\mathbf{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu}))\right].
\tag{2.2}\]</span></span></p>
<p>The maximum likelihood estimate (MLE) of <span class="math inline">\(\boldsymbol{\theta}\)</span>, denoted as <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>, is found by finding <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximizes <a href="#eq-likelihood" class="quarto-xref">Equation&nbsp;<span>2.2</span></a>. However, it is often more convenient to maximize the log likelihood function. To be more computational friendly, a better form of log likelihood is given by</p>
<div id="lem-MLE" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1</strong></span> The log-likelihood is given by</p>
<p><span id="eq-loglikelihood"><span class="math display">\[\ell(\boldsymbol{\theta}) =  -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right] \tag{2.3}\]</span></span></p>
<p>where <span class="math inline">\(\boldsymbol{S}\)</span> is the sample covariance defined as <span class="math inline">\(\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\mathbf{y}_i-\boldsymbol{\mu})(\mathbf{y}_i-\boldsymbol{\mu})^\top\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span><span class="math display">\[
\begin{align*}
\ell(\boldsymbol{\theta})=&amp; \sum^n_{i=1}\left[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\mathbf{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\frac{1}{n}\sum_{i=1}^n(\mathbf{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu})\right]\\
=&amp; -\frac{n}{2}\left[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})\right].
\end{align*}
\]</span></p>
<p>The last equality holds from the following fact: <span class="math display">\[
\begin{align*}
&amp;\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}(\mathbf{y}_i-\boldsymbol{\mu})(\mathbf{y}_i-\boldsymbol{\mu})^\top\\
\Leftrightarrow \ &amp; \boldsymbol{\Sigma}^{-1}\boldsymbol{S}=\frac{1}{n}\sum^n_{i=1}\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu})(\mathbf{y}_i-\boldsymbol{\mu})^\top\\
\Leftrightarrow \ &amp; \text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}) = \frac{1}{n} \sum^n_{i=1} \text{tr} (\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu})(\mathbf{y}_i-\boldsymbol{\mu})^\top)\\
\Leftrightarrow \ &amp; \text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}) = \frac{1}{n}\sum_{i=1}^n(\mathbf{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\mathbf{y}_i-\boldsymbol{\mu})
\end{align*}
\]</span></p>
</div>
<p>The MLE is obtained by seeking the roots of equation system (provided the likelihood is compact)</p>
<p><span id="eq-solveMLE"><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial \boldsymbol{\Lambda}}\ell(\boldsymbol{\theta})&amp;=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}\ell(\boldsymbol{\theta})&amp;= 0.
\end{align*}
\tag{2.4}\]</span></span></p>
<p>However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like Expectation-Maximization (EM) algorithm <span class="citation" data-cites="Rubin1982EMAlgorithms">(<a href="references.html#ref-Rubin1982EMAlgorithms" role="doc-biblioref">Rubin and Thayer 1982</a>)</span>, we will discuss this algorithm in the last part of background.</p>
</section>
</section>
<section id="rotation-techniques" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="rotation-techniques"><span class="header-section-number">2.4</span> Rotation Techniques</h2>
<p>After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>.</p>
<p>Suppose <span class="math inline">\(Q(\boldsymbol{\Lambda})\)</span> is an criterion for <span class="math inline">\(\boldsymbol{\Lambda}\)</span> in the rotation procedure, and we may express it as <span class="math inline">\(Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})\)</span> where <span class="math inline">\(P(\cdot)\)</span> is some loss function<span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. Specifically, if we set <span class="math inline">\(P(\cdot)=|\cdot|\)</span>, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as <span class="citation" data-cites="Jennrich2004Rotation">(<a href="references.html#ref-Jennrich2004Rotation" role="doc-biblioref">Jennrich 2004</a>)</span></p>
<p><span class="math display">\[
\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Lambda}_0\)</span> is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is</p>
<p><span class="math display">\[
\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\)</span> is the maximum likelihood estimator.</p>
<section id="discussion-about-two-step-method" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="discussion-about-two-step-method"><span class="header-section-number">2.4.1</span> Discussion about two-step method</h3>
<p>The traditional two-step method faces significant shortcomings, primarily its unsuitability <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by <span class="math display">\[
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 &amp; 0 \\
0 &amp; 0.8 \\
0.6 &amp; 0\\
0 &amp; 0.7\\
0 &amp; 0.6\\
\end{bmatrix}.
\]</span> and the real <span class="math inline">\(\boldsymbol{\Psi}\)</span> is given by <span class="math display">\[
\boldsymbol{\Psi}=\text{diag}(0.1,0.2,0.2,0.1,0.1).
\]</span> We generate <span class="math inline">\(\mathbf{y}\in \mathbb{R}^{{10000}\times 5}\)</span> from a multivariate normal distribution with mean <span class="math inline">\(\mathbb{0}_{5}\)</span> and covariance <span class="math inline">\(\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}\)</span>. We utilize the R-function <code>factanal()</code> to produce the loading matrix via MLE, where this R-function will unitize varimax method to achieve rotation automatically. What we finally get is</p>
<p><span class="math display">\[
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
0 &amp; 0.997\\
0.872 &amp; 0\\
0 &amp; 0.748\\
0.915 &amp; 0\\
0.882 &amp; 0\\
\end{bmatrix},
\hat{\boldsymbol{\Psi}}=\text{diag}(0.005,0.240,0.440,0.163,0.223)
\]</span> This result gives the same sparsity result. However, the result of loading matrix in the first and last two dimensions are much larger than the real one, which implies overfitness.</p>
<p>Simulation code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10000</span> <span class="co"># Number of observations</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># True loading matrix</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>true_loading <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="fl">0.8</span>,<span class="dv">0</span>,<span class="fl">0.6</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="fl">0.8</span>,<span class="dv">0</span>,<span class="fl">0.7</span>,<span class="fl">0.6</span>),<span class="at">nrow=</span><span class="dv">5</span>,<span class="at">ncol=</span><span class="dv">2</span>) </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># True psi matrix</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>true_psi <span class="ot">&lt;-</span> <span class="fu">diag</span>(<span class="fu">c</span>(<span class="fl">0.1</span>,<span class="fl">0.2</span>,<span class="fl">0.2</span>,<span class="fl">0.1</span>,<span class="fl">0.1</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples from multivariate normal distribution</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">mvrnorm</span>(<span class="at">n =</span> n, <span class="at">mu =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">5</span>), </span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>                         <span class="at">Sigma =</span> true_loading <span class="sc">%*%</span> <span class="fu">t</span>(true_loading) <span class="sc">+</span> true_psi)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Traditional two step procedure</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># the function factanal has already used varimax method to get the optimal loading</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>fa_result <span class="ot">&lt;-</span> <span class="fu">factanal</span>(<span class="at">factors =</span> <span class="dv">2</span>, <span class="at">covmat =</span> <span class="fu">cor</span>(samples))</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Result</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>fa_result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
factanal(factors = 2, covmat = cor(samples))

Uniquenesses:
[1] 0.005 0.240 0.440 0.163 0.222

Loadings:
     Factor1 Factor2
[1,]          0.997 
[2,]  0.872         
[3,]          0.748 
[4,]  0.915         
[5,]  0.882         

               Factor1 Factor2
SS loadings      2.375   1.555
Proportion Var   0.475   0.311
Cumulative Var   0.475   0.786

The degrees of freedom for the model is 1 and the fit was 2e-04 </code></pre>
</div>
</div>
<p>Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>. We have <span class="math display">\[\begin{align*}
  &amp;&amp;&amp;H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &amp;&amp;&amp;H_a: \text{ Otherwise}.
\end{align*}\]</span> The test statistics is given by <span class="math inline">\(\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})\)</span> which has an asymptotic <span class="math inline">\(\chi^2_s\)</span> distribution with <span class="math inline">\(s=\frac{1}{2}(p-k)^2-(p+k)\)</span> by the property of MLE, where <span class="math inline">\(F\)</span> is given by (<span class="citation" data-cites="mardiaMultivariateAnalysis1979">Mardia, Kent, and Bibby (<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">1979</a>)</span>) <span class="math display">\[
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
\]</span> Typically, the test is conducted at a <span class="math inline">\(5\%\)</span> significant level. We can start the procedure from a very small <span class="math inline">\(k\)</span>, say, <span class="math inline">\(k=1\)</span> or <span class="math inline">\(k=2\)</span> and then increase <span class="math inline">\(k\)</span> until the null hypothesis is not rejected.</p>
</section>
</section>
<section id="penalized-likelihood-method" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="penalized-likelihood-method"><span class="header-section-number">2.5</span> Penalized Likelihood Method</h2>
<p>Penalized likelihood method can be viewed as a generalization of two-step method mentioned above <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. A penalized factor analytic model can be obtained by solving following optimization problem <span class="math display">\[
\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}\ell_p := \ell(\boldsymbol{\theta})-P(\boldsymbol{\theta};\rho).
\]</span> where we call <span class="math inline">\(\ell_p\)</span> as the penalized likelihood, <span class="math inline">\(\rho\)</span> is called regularization parameter or tuning parameter, and we treat <span class="math inline">\(P(\boldsymbol{\theta};\rho)\)</span> as the penalty. There are many types of penalized functions developed, such as LASSO (<span class="math inline">\(P(\boldsymbol{\Lambda})=\sum_{i=1}^p\sum_{j=1}^k|\lambda_{ij}|\)</span>) and MC+ ($P(;;)=<em>{i=1}^p</em>{j=1}^k { n(|<em>{ij}|-)I(|</em>{ij}|&lt;)+I(|_{ij}|)} $)<span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>，where <span class="math inline">\(\gamma\)</span> is another tuning parameter to keep a balance between unbiasedness and concavity. In this article, we will mainly focus on the LASSO penalty.</p>
<section id="lasso-penalty-and-lasso-estimator" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="lasso-penalty-and-lasso-estimator"><span class="header-section-number">2.5.1</span> LASSO penalty and LASSO estimator</h3>
<p>Again, recall that the LASSO penalized likelihood is given by <span class="math display">\[\ell_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\lambda_{ij}|\]</span> and the LASSO estimator, denoted as <span class="math inline">\((\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)\)</span>, can be obtained via</p>
<p><span class="math display">\[
\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &amp;= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\lambda_{ij}|\\
&amp;=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\lambda_{ij}|.
\end{align*}
\]</span> Nevertheless, the objective function is non_differentiable at some point because of the penalization term. Therefore we need to find other approaches to obtain the estimator instead of solving</p>
</section>
</section>
<section id="sec-EM" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-EM"><span class="header-section-number">2.6</span> The EM Algorithm</h2>
<p>The Expectation-Maximization (EM) algorithm is a widely used iterative method to find the MLE, especially when the model depends on latent variables. The EM algorithm iteratively apply two distinct steps: the Expectation step (E-step) and the Maximization step (M-step). More specifically, we define</p>
<p><span class="math display">\[Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)}) = \mathbb{E}\left(\ell (\boldsymbol{\theta}; \mathbf{y})\bigg|\mathbf{y},\boldsymbol{\theta}^{(t)}\right).\]</span> Then for the <span class="math inline">\((t + 1)\)</span>-th interation, the steps involve:</p>
<ul>
<li><em>E-step</em>: Compute <span class="math inline">\(Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})\)</span>.</li>
<li><em>M-step</em>: Update <span class="math inline">\(\boldsymbol{\theta}^{(t+1)}\)</span> as <span class="math inline">\(\boldsymbol{\theta}\)</span> that maximses <span class="math inline">\(Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})\)</span>.</li>
</ul>
<p>The M-step may be replaced with updating <span class="math inline">\(\boldsymbol{\theta}^{(t+1)}\)</span> such that <span class="math inline">\(Q(\boldsymbol{\theta}^{(t+1)}|\boldsymbol{\theta}^{(t)}) \geq Q(\boldsymbol{\theta}^{(t)}|\boldsymbol{\theta}^{(t)})\)</span>.</p>
<p>We iterate between E-step and M-step until convergence. The convergence may be determined by a criteria such as <span class="math inline">\(||\boldsymbol{\theta}^{(t+1)}-\boldsymbol{\theta}^{(t)}||_p\leq \epsilon\)</span> for some <span class="math inline">\(p\)</span>-norm <span class="math inline">\(||\cdot||_p\)</span> and <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<p>The EM algorithm approaches the problem of solving the likelihood equation indirectly by proceeding iteratively in terms of <span class="math inline">\(\ell(\boldsymbol{\theta};\mathbf{y})\)</span>. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given <span class="math inline">\(\mathbf{y}\)</span> and current fit for <span class="math inline">\(\boldsymbol{\theta}\)</span>.</p>
<p>Now we need to show the correctness of the EM-algorithm. In other words, by iteratively processing the EM-algorithm, the likelihood will keep increasing (at least non-decreasing). First we need to show the following lemma.</p>
<div id="lem-convergence" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.2 (Convergence of the EM Algorithm)</strong></span> Suppose the likelihood is upper-bounded. Then for all <span class="math inline">\(\epsilon &gt; 0\)</span>, there exists <span class="math inline">\(t &gt; t_0\)</span> such that <span class="math inline">\(||\boldsymbol{\theta}^{(t)} - \hat{\boldsymbol{\theta}}|| &lt; \epsilon\)</span> where <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> is the MLE.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>By the definition of conditional likelihood, our likelihood of complete data, i.e.&nbsp;the observable and unobservable data, can be expressed by <span class="math display">\[
L_{\boldsymbol{X}}(\boldsymbol{\theta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\theta})=L_{\mathbf{y}}(\boldsymbol{\theta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}),
\]</span> and hence the log-likelihood is given by <span class="math display">\[
\log L_{\boldsymbol{X}}(\boldsymbol{\theta}) = \ell_{\mathbf{y}}(\boldsymbol{\theta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}).
\]</span> Take expectation to both sides of the equation with respect to <span class="math inline">\(\boldsymbol{x|y}\)</span> and replace <span class="math inline">\(\boldsymbol{\theta}\)</span> by <span class="math inline">\(\boldsymbol{\theta}^{(k)}\)</span>, we will have <span class="math display">\[
Q(\boldsymbol{\theta};\boldsymbol{\theta}^{(k)}) = \ell_{\mathbf{y}}(\boldsymbol{\theta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta})|\mathbf{y},\boldsymbol{\theta}^{(k)}].
\]</span> Now consider the difference of log-likelihood of <span class="math inline">\(\mathbf{y}\)</span> function between two iterations, we have <span class="math display">\[
\begin{align*}
  \ell_{\mathbf{y}}(\boldsymbol{\theta}^{(k+1)})-\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(k)}) =
  &amp;\{Q(\boldsymbol{\theta}^{(k+1)};\boldsymbol{\theta}^{(k)})-Q(\boldsymbol{\theta}^{(k)};\boldsymbol{\theta}^{(k)})\}\\
  &amp;-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}^{(k+1)})|\mathbf{y},\boldsymbol{\theta}^{(k)}]\\
  &amp;-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}^{(k)}|\mathbf{y},\boldsymbol{\theta}^{(k)}]\}.
\end{align*}
\]</span> By the procedure of EM-algorithm, we always have <span class="math inline">\(Q(\boldsymbol{\theta}^{(k+1)};\boldsymbol{\theta}^{(k)})\geq Q(\boldsymbol{\theta}^{(k)};\boldsymbol{\theta}^{(k)})\)</span>. By the Gibbs’s inequality, we have <span class="math inline">\(\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}^{(k+1)})|\mathbf{y},\boldsymbol{\theta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\mathbf{y};\boldsymbol{\theta}^{(k)}|\mathbf{y},\boldsymbol{\theta}^{(k)}].\)</span> Therefore during iterations, the sequence of log-likelihood of observed data <span class="math inline">\(\mathbf{y}\)</span> given by <span class="math inline">\(\{\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(n)})\}_{n\geq 1}:=\{\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(1)}),\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(2)}),\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(3)}),\dots\}\)</span>, is increasing. Now replace the term <span class="math inline">\(\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(k+1)})\)</span> by <span class="math inline">\(\ell_{\mathbf{y}}(\hat{\boldsymbol{{\theta}}})\)</span>, we will have <span class="math display">\[
\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(k)}) \leq \ell_{\mathbf{y}}(\hat{\boldsymbol{{\theta}}})
\]</span> for all <span class="math inline">\(k=1,2,\dots\)</span>. Therefore the upper-bound of the sequence <span class="math inline">\(\{\ell_{\mathbf{y}}(\boldsymbol{\theta}^{(n)})\}_{n\geq 1}\)</span> is <span class="math inline">\(\ell_{\mathbf{y}}(\hat{\boldsymbol{\theta}})\)</span>. By the monotone convergence theorem, and the definition of convergence, we finished.</p>
</div>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hiroseSparseEstimationNonconcave2015" class="csl-entry" role="listitem">
Hirose, Kei, and Michio Yamamoto. 2015. <span>“Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.”</span> <em>Statistics and Computing</em> 25 (5): 863–75. <a href="https://doi.org/10.1007/s11222-014-9458-0">https://doi.org/10.1007/s11222-014-9458-0</a>.
</div>
<div id="ref-Jennrich2004Rotation" class="csl-entry" role="listitem">
Jennrich, R. I. 2004. <span>“Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.”</span> <em>Psychometrika</em> 69: 257–73.
</div>
<div id="ref-mardiaMultivariateAnalysis1979" class="csl-entry" role="listitem">
Mardia, K. V., J. T. Kent, and J. M. Bibby. 1979. <em>Multivariate Analysis</em>. Probability and Mathematical Statistics. London ; New York: Academic Press.
</div>
<div id="ref-Rubin1982EMAlgorithms" class="csl-entry" role="listitem">
Rubin, Donald B., and Dorothy T. Thayer. 1982. <span>“EM Algorithms for ML Factor Analysis.”</span> <em>Psychometrika</em> 47: 69–76.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./method.html" class="pagination-link" aria-label="Method">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright © 2024, Zhining Wang</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ZhiningW/Master_Project/edit/main/background.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ZhiningW/Master_Project/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>