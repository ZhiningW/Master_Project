<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.30">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>{{&lt; var title &gt;}} – 2&nbsp; Background</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./method.html" rel="next">
<link href="./intro.html" rel="prev">
<link href="site_libs/quarto-contrib/quarto-project/anu-thesis/assets/ANU_Favicon_Inversed_Gold.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="site_libs/quarto-contrib/quarto-project/anu-thesis/assets/css/all.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./background.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Model Selection for Factor Analysis</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/ZhiningW/Master_Project" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./method.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar"><div class="quarto-margin-header"><div class="margin-header-item">
<p><img src="images/ANU_Primary_Vertical_GoldBlack.png" class="logo img-fluid"></p>
</div></div>
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#factor-analytic-model" id="toc-factor-analytic-model" class="nav-link active" data-scroll-target="#factor-analytic-model"><span class="header-section-number">2.1</span> Factor analytic model</a></li>
  <li><a href="#parameter-estimation" id="toc-parameter-estimation" class="nav-link" data-scroll-target="#parameter-estimation"><span class="header-section-number">2.2</span> Parameter Estimation</a>
  <ul class="collapse">
  <li><a href="#maximum-likelihood-estimation" id="toc-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#maximum-likelihood-estimation"><span class="header-section-number">2.2.1</span> Maximum Likelihood Estimation</a></li>
  <li><a href="#rotation-techniques" id="toc-rotation-techniques" class="nav-link" data-scroll-target="#rotation-techniques"><span class="header-section-number">2.2.2</span> Rotation techniques</a></li>
  <li><a href="#discussion-about-two-step-method" id="toc-discussion-about-two-step-method" class="nav-link" data-scroll-target="#discussion-about-two-step-method"><span class="header-section-number">2.2.3</span> Discussion about two-step method</a></li>
  <li><a href="#penalized-likelihood-method" id="toc-penalized-likelihood-method" class="nav-link" data-scroll-target="#penalized-likelihood-method"><span class="header-section-number">2.2.4</span> Penalized Likelihood Method</a></li>
  <li><a href="#lasso-penalty-and-lasso-estimator" id="toc-lasso-penalty-and-lasso-estimator" class="nav-link" data-scroll-target="#lasso-penalty-and-lasso-estimator"><span class="header-section-number">2.2.5</span> LASSO penalty and LASSO estimator</a></li>
  </ul></li>
  <li><a href="#sec-EM" id="toc-sec-EM" class="nav-link" data-scroll-target="#sec-EM"><span class="header-section-number">2.3</span> The EM Algorithm</a>
  <ul class="collapse">
  <li><a href="#the-e-step-and-m-step" id="toc-the-e-step-and-m-step" class="nav-link" data-scroll-target="#the-e-step-and-m-step"><span class="header-section-number">2.3.1</span> The E-step and M-step</a></li>
  <li><a href="#convergence-of-the-em-algorithm" id="toc-convergence-of-the-em-algorithm" class="nav-link" data-scroll-target="#convergence-of-the-em-algorithm"><span class="header-section-number">2.3.2</span> Convergence of the EM Algorithm</a></li>
  </ul></li>
  <li><a href="#sec-penEM" id="toc-sec-penEM" class="nav-link" data-scroll-target="#sec-penEM"><span class="header-section-number">2.4</span> The EM Algorithm in LASSO Factor Analytic Models</a>
  <ul class="collapse">
  <li><a href="#model-setup" id="toc-model-setup" class="nav-link" data-scroll-target="#model-setup"><span class="header-section-number">2.4.1</span> Model Setup</a></li>
  <li><a href="#the-em-algorithm" id="toc-the-em-algorithm" class="nav-link" data-scroll-target="#the-em-algorithm"><span class="header-section-number">2.4.2</span> The EM Algorithm</a></li>
  <li><a href="#e-step" id="toc-e-step" class="nav-link" data-scroll-target="#e-step"><span class="header-section-number">2.4.3</span> E-Step</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/ZhiningW/Master_Project/edit/main/background.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ZhiningW/Master_Project/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-bg" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Background</span></span></h1>
</div>


<div class="quarto-title-anu-block">


<div class="quarto-title-meta">

  
  
  </div>





</div>

</header>


<section id="factor-analytic-model" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="factor-analytic-model"><span class="header-section-number">2.1</span> Factor analytic model</h2>
<p>Factor analysis is a mathematical model which tries to use fewer underlying factors to explain the correlation between a large set of observed variables <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>. It provides a useful tool for exploring the covariance structure among observable variables <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. One of the major assumptions that factor analytic model stands on is that it is impossible for us to observe those underlying factors directly. This assumption is especially suited to subjects like psychology where we cannot observe exactly some concept like how intelligent our subjects are <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>.</p>
<p>Suppose we have a observable random vector <span class="math inline">\boldsymbol{y}\in \mathbb{R}^p</span> with mean <span class="math inline">\mathbb{E}[\boldsymbol{y}]=\boldsymbol{\mu}</span> and variance <span class="math inline">\mathbb{V}[\boldsymbol{y}]=\boldsymbol{\Sigma}</span>. Then a <span class="math inline">k</span>-order factor analysis model for <span class="math inline">\boldsymbol{y}</span> can be given by <span class="math display">\begin{equation}
\boldsymbol{y}=\boldsymbol{\Lambda} \boldsymbol{f}+\boldsymbol{\mu}+\boldsymbol{\epsilon},
\end{equation}</span> where <span class="math inline">\boldsymbol{\Lambda} \in \mathbb{R}^{p \times k}</span> is called <em>loading matrix</em>, we call <span class="math inline">\boldsymbol{f} \in \mathbb{R}^{k}</span> as <em>common factors</em> and <span class="math inline">\boldsymbol{\epsilon} \in \mathbb{R}^{p}</span> is <em>unique factors</em>. To make the model well-defined, we may assume <span class="math display">\mathbb{E}[\boldsymbol{f}]=\boldsymbol{0}_k, \mathbb{V}[\boldsymbol{f}]=\boldsymbol{I}_{k\times k}, \mathbb{E}[\boldsymbol{\epsilon}]=\boldsymbol{0}_p, \mathbb{V}[\boldsymbol{\epsilon}]=:\boldsymbol{\Psi}=\text{diag}(\Psi_{11},\dots,\Psi_{pp})</span> and also the independence between any elements from <span class="math inline">\boldsymbol{f}</span> and <span class="math inline">\boldsymbol{\epsilon}</span> separately, i.e. <span class="math display">Cov[f_i,\epsilon_j]=0, \text{for all } i\in\{1,2,\dots,k\} \text{ and } j \in \{1,2,\dots,p\}</span> Straightforwardly, the covariance of observable vector <span class="math inline">\boldsymbol{y}</span> can be modelled by<br>
<span class="math display">\begin{equation}
\mathbb{V}[\boldsymbol{y}]=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}
\end{equation}</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Indeterminacy of the loading matrix
</div>
</div>
<div class="callout-body-container callout-body">
<p>One can easily see that if our factor analytic model is given by (1), then it can also be modelled as <span class="math display">\boldsymbol{y}=(\boldsymbol{\Lambda}\boldsymbol{M})(\boldsymbol{M}^\top\boldsymbol{f}) +\boldsymbol{\mu}+\boldsymbol{\epsilon}</span> where the matrix <span class="math inline">\boldsymbol{M}</span> is orthogonal and simultaneously the variance of <span class="math inline">\boldsymbol{y}</span> given by (2) still holds, since <span class="math display">\mathbb{V}[\boldsymbol{y}]=(\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)\mathbb{V}[\boldsymbol{f}](\boldsymbol{\Lambda}\boldsymbol{M}\boldsymbol{M}^\top)^\top+\boldsymbol{\Psi}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}.</span> Therefore a rotated loading matrix <span class="math inline">\boldsymbol{\Lambda}\boldsymbol{M}</span> is still a valid loading matrix for a factor analytic model. Sometimes we resolve this problem by making the loading matrix to satisfy some constraints like <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span> <span class="math display">\boldsymbol{\Lambda}^\top \boldsymbol{\Psi}^{-1} \boldsymbol{\Lambda} \text{ is diagonal.}</span></p>
</div>
</div>
</section>
<section id="parameter-estimation" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="parameter-estimation"><span class="header-section-number">2.2</span> Parameter Estimation</h2>
<p>We denote the set of parameters by <span class="math inline">\beta := \{\text{vec}(\boldsymbol{\Lambda}),\text{vec}(\boldsymbol{\Psi})\}</span> where <span class="math inline">\text{vec}(\cdot)</span> is the vectorisation of the input.</p>
<p>Traditionally, a two-step procedure is used to construct a factor analytic model: estimate parameters by maximum likelihood estimation (aka, MLE) and then use rotation techniques to find an interpretable model.</p>
<section id="maximum-likelihood-estimation" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="maximum-likelihood-estimation"><span class="header-section-number">2.2.1</span> Maximum Likelihood Estimation</h3>
<p>Suppose we have <span class="math inline">n</span> independent and identically distributed observations <span class="math inline">\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_N</span> from a p-dimensional multi-variate normal distribution <span class="math inline">N_p(\boldsymbol{\mu},\boldsymbol{\Sigma})</span> and by our hypothesis, we have <span class="math inline">\boldsymbol{\Sigma}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}</span>. Then the likelihood function is given by <span class="math display">L(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\prod^n_{i=1}\left[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Sigma})^{-\frac{1}{2}}\exp(-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu}))\right].</span> and hence the log-likelihood is given by</p>
<p><span class="math display">\begin{align*}
l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&amp; \sum^n_{i=1}[-\frac{p}{2}\log(2\pi)-\frac{1}{2}\log(\det(\boldsymbol{\Sigma}))-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\mu})^\top\boldsymbol{\Sigma}^{-1}(\boldsymbol{y}_i-\boldsymbol{\mu})]\\
=&amp; -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})],
\end{align*}</span></p>
where <span class="math inline">\boldsymbol{S}</span> is the sample covariance defined as <span class="math inline">\boldsymbol{S}:=\frac{1}{n}\sum^n_{i=1}(\boldsymbol{y}_i-\boldsymbol{\mu})(\boldsymbol{y}_i-\boldsymbol{\mu})^\top</span>. To get the MLE of parameters, we seek the roots of equation system
<span class="math display">\begin{cases}
\frac{\partial}{\partial \boldsymbol{\Lambda}}l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=0 \\
\frac{\partial}{\partial \boldsymbol{\Psi}}l(\boldsymbol{\Lambda},\boldsymbol{\Psi})=0.
\end{cases}</span>
<p>However, there is no closed form of the roots. Many iterative algorithms are developed to get the roots of the equation, like EM algorithm developed by Rubin <span class="citation" data-cites="Rubin1982EMAlgorithms">(<a href="references.html#ref-Rubin1982EMAlgorithms" role="doc-biblioref">Rubin and Thayer 1982</a>)</span>.</p>
</section>
<section id="rotation-techniques" class="level3" data-number="2.2.2">
<h3 data-number="2.2.2" class="anchored" data-anchor-id="rotation-techniques"><span class="header-section-number">2.2.2</span> Rotation techniques</h3>
<p>After estimation, we want to rotate the loading matrix to possess a sparse matrix in order to interpret the observable variables by underlying factors better. Also there are many method to achieve rotation as well such as the varimax method and the promax method <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>.</p>
<p>Suppose <span class="math inline">Q(\boldsymbol{\Lambda})</span> is an criterion for <span class="math inline">\boldsymbol{\Lambda}</span> in the rotation procedure, and we may express it as <span class="math inline">Q(\boldsymbol{\Lambda}):= \sum^p_{i=1}\sum^d_{j=1}P(\boldsymbol{\Lambda}_{ij})</span> where <span class="math inline">P(\cdot)</span> is some loss function<span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. Specifically, if we set <span class="math inline">P(\cdot)=|\cdot|</span>, we have LASSO to generate a theoretically sparse loading matrix. If we rewrite this in a optimization problem, it can be given as <span class="citation" data-cites="Jennrich2004Rotation">(<a href="references.html#ref-Jennrich2004Rotation" role="doc-biblioref">Jennrich 2004</a>)</span></p>
<p><span class="math display">\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\boldsymbol{\Lambda}_0\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}</span></p>
<p>where <span class="math inline">\boldsymbol{\Lambda}_0</span> is an initial guess. Since we execute this technique after obtaining the MLE of parameters, therefore what we want is</p>
<p><span class="math display">\begin{align*}
&amp;\min_{\boldsymbol{\Lambda}} \sum^p_{i=1}\sum^k_{j=1}P(\boldsymbol{\Lambda}_{ij})\\
&amp;\text{subject to } \boldsymbol{\Lambda}=\hat{\boldsymbol{\Lambda}}_{\text{MLE}}\boldsymbol{M} \text{ and } \boldsymbol{M}^\top\boldsymbol{M}=\boldsymbol{I}_k,
\end{align*}</span></p>
<p>where <span class="math inline">\hat{\boldsymbol{\Lambda}}_{\text{MLE}}</span> is the maximum likelihood estimator.</p>
</section>
<section id="discussion-about-two-step-method" class="level3" data-number="2.2.3">
<h3 data-number="2.2.3" class="anchored" data-anchor-id="discussion-about-two-step-method"><span class="header-section-number">2.2.3</span> Discussion about two-step method</h3>
<p>The traditional two-step method faces significant shortcomings, primarily its unsuitability <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. Similar to challenges in regression models, MLE can result in overfitting, and rotation techniques might not yield a sufficiently sparse loading matrix. See this as an example: Suppose the true loading matrix is given by <span class="math display">
\boldsymbol{\Lambda}=
\begin{bmatrix}
0.8 &amp; 0 \\
0 &amp; 0.8 \\
0 &amp; 0\\
0 &amp; 0\\
0 &amp; 0\\
\end{bmatrix}.
</span> We generate <span class="math inline">\boldsymbol{Y}\in \mathbb{R}^{{100}\times 5}</span> using common factor <span class="math inline">\boldsymbol{f}\in \mathbb{R}^{{100}\times 2}</span> where each factor is generated randomly from standard normal distribution <span class="math inline">N(0,1)</span>. We utilize the R-function <code>factanal()</code> to produce the loading matrix via MLE and then call varimax() to rotate our loading matrix. What we finally get is</p>
<p><span class="math display">
\hat{\boldsymbol{\Lambda}}=
\begin{bmatrix}
-0.158 &amp; 0.985\\
0 &amp; 0\\
0.997 &amp; 0\\
0.207 &amp; 0\\
0 &amp; 0\\
\end{bmatrix},
</span> which is neither precise nor sparse.</p>
<p>Simulation code:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># Number of observations</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">5</span>   <span class="co"># Number of features</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>factor1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co"># set factors</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>factor2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow=</span>n, <span class="at">ncol=</span>p)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.8</span><span class="sc">*</span>factor1 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Strong loading</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fl">0.8</span><span class="sc">*</span>factor2 <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Strong loading</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  <span class="co"># Noise, no strong loading on any factor</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">4</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)  </span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X[,<span class="dv">5</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>) </span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>fa_result <span class="ot">&lt;-</span> <span class="fu">factanal</span>(<span class="at">factors =</span> <span class="dv">2</span>, <span class="at">covmat =</span> <span class="fu">cor</span>(X))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>rotated_fa <span class="ot">&lt;-</span> <span class="fu">varimax</span>(fa_result<span class="sc">$</span>loadings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Another problem is that the absence of model selection complicates efficient modeling by making it difficult to determine the precise number of underlying factors, that is, the order of the factor analytic model. A usual but troublesome method is conducting a hypothesis test for the model <span class="citation" data-cites="mardiaMultivariateAnalysis1979">(<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">Mardia, Kent, and Bibby 1979</a>)</span>. We have <span class="math display">\begin{align*}
  &amp;&amp;&amp;H_k: k \text{ common factors are sufficient to describe the data } \\
  \longleftrightarrow &amp;&amp;&amp;H_a: \text{ Otherwise}.
\end{align*}</span> The test statistics is given by <span class="math inline">\text{TS}=nF(\hat{\boldsymbol{\Lambda}},\hat{\boldsymbol{\Psi}})</span> which has an asymptotic <span class="math inline">\chi^2_s</span> distribution with <span class="math inline">s=\frac{1}{2}(p-k)^2-(p+k)</span> by the property of MLE, where <span class="math inline">F</span> is given by (<span class="citation" data-cites="mardiaMultivariateAnalysis1979">Mardia, Kent, and Bibby (<a href="references.html#ref-mardiaMultivariateAnalysis1979" role="doc-biblioref">1979</a>)</span>) <span class="math display">
F(\boldsymbol{\Lambda},\boldsymbol{\Psi})=\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})-\log(\det(\boldsymbol{\Sigma}^{-1}\boldsymbol{S}))-p.
</span> Typically, the test is conducted at a <span class="math inline">5\%</span> significant level. We can start the procedure from a very small <span class="math inline">k</span>, say, <span class="math inline">k=1</span> or <span class="math inline">k=2</span> and then increase <span class="math inline">k</span> until the null hypothesis is not rejected.</p>
</section>
<section id="penalized-likelihood-method" class="level3" data-number="2.2.4">
<h3 data-number="2.2.4" class="anchored" data-anchor-id="penalized-likelihood-method"><span class="header-section-number">2.2.4</span> Penalized Likelihood Method</h3>
<p>Penalized likelihood method can be viewed as a generalization of two-step method mentioned above <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. A penalized factor analytic model can be obtained by solving following optimization problem <span class="math display">
\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}}l_p := l(\boldsymbol{\Lambda},\boldsymbol{\Psi})-\rho\sum^p_{i=1}\sum^k_{j=1}P(|\boldsymbol{\Lambda}_{ij}|).
</span> where we call <span class="math inline">l_p</span> as the penalized likelihood, <span class="math inline">\rho</span> is called regularization parameter and we can treat <span class="math inline">P(\cdot)</span> as a penalized function. There are many types of penalized functions developed, such as LASSO (<span class="math inline">P(\cdot)=|\cdot|</span>) and MC+ (<span class="math inline">P(|\theta|;\rho;\gamma)=n(|\theta|-\frac{\theta^2}{2\rho\gamma})I(|\theta|&lt;\rho\gamma)+\frac{\rho^2\gamma}{2}I(|\theta|\geq\rho\gamma)</span>) <span class="citation" data-cites="hiroseSparseEstimationNonconcave2015">(<a href="references.html#ref-hiroseSparseEstimationNonconcave2015" role="doc-biblioref">Hirose and Yamamoto 2015</a>)</span>. In this article, we will mainly focus on the LASSO penalty.</p>
</section>
<section id="lasso-penalty-and-lasso-estimator" class="level3" data-number="2.2.5">
<h3 data-number="2.2.5" class="anchored" data-anchor-id="lasso-penalty-and-lasso-estimator"><span class="header-section-number">2.2.5</span> LASSO penalty and LASSO estimator</h3>
<p>Again, recall that the LASSO penalized likelihood is given by <span class="math display">l_p=-\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|</span> and the LASSO estimator, denoted as <span class="math inline">(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*)</span>, can be obtained via</p>
<p><span class="math display">\begin{align*}
(\boldsymbol{\Lambda}^*,\boldsymbol{\Psi}^*) &amp;= \text{arg}\max_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad -\frac{n}{2}[p\log(2\pi)+\log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})]-\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|\\
&amp;=\text{arg}\min_{\boldsymbol{\Lambda},\boldsymbol{\Psi}} \quad \log(\det(\boldsymbol{\Sigma}))+\text{tr}(\boldsymbol{\Sigma}^{-1}\boldsymbol{S})+\frac{2}{n}\rho\sum^p_{i=1}\sum^k_{j=1}|\Lambda_{ij}|.
\end{align*}</span></p>
<p>An E-M algorithm can be applied for evaluating the LASSO estimator.</p>
</section>
</section>
<section id="sec-EM" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-EM"><span class="header-section-number">2.3</span> The EM Algorithm</h2>
<p>The Expectation-Maximization (EM) algorithm is a widely used iterative method to compute the maximum likelihood estimation, especially when we have some unobserved data <span class="citation" data-cites="Ng2012EMAlgorithm">(<a href="references.html#ref-Ng2012EMAlgorithm" role="doc-biblioref">Ng, Krishnan, and McLachlan 2012</a>)</span>. As we mentioned, the key of maximum likelihood estimation is solving equation <span class="math display">
\frac{\partial}{\partial \beta}l=0.
</span> However, challenges often arise from the complex nature of the log-likelihood function, especially with data that is grouped, censored, or truncated. To navigate these difficulties, the EM algorithm introduces an ingenious approach by conceptualizing an equivalent statistical problem that incorporates both observed and unobserved data. Here, <em>augmented data</em>(or complete) refers to the integration of this unobserved component, enhancing the algorithm’s ability to iteratively estimate through two distinct phases: the Expectation step (E-step) and the Maximization step (M-step). The iteration between these steps facilitates the efficient of parameter estimates, making the EM algorithm an essential tool for handling incomplete data sets effectively.</p>
<section id="the-e-step-and-m-step" class="level3" data-number="2.3.1">
<h3 data-number="2.3.1" class="anchored" data-anchor-id="the-e-step-and-m-step"><span class="header-section-number">2.3.1</span> The E-step and M-step</h3>
<p>Let <span class="math inline">\boldsymbol{x}</span> denote the vector containing complete data, <span class="math inline">\boldsymbol{y}</span> denote the observed incomplete data and <span class="math inline">\boldsymbol{z}</span> denote the vector containing the missing data. Here ‘missing data’ is not necessarily missing, even if it does not at first appear to be missed, we can formulating it to be as such to facilitate the computation (we may see this later) <span class="citation" data-cites="Ng2012EMAlgorithm">(<a href="references.html#ref-Ng2012EMAlgorithm" role="doc-biblioref">Ng, Krishnan, and McLachlan 2012</a>)</span>. Also we assume <span class="math inline">\boldsymbol{\beta}</span> as the parameter we want to estimate over the parameter space <span class="math inline">\boldsymbol{\Omega}</span>.</p>
<p>Now denote <span class="math inline">f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})</span> as the probability density function (p.d.f.) of the random vector <span class="math inline">\boldsymbol{X}</span> corresponding to <span class="math inline">\boldsymbol{x}</span>. Then the complete-data log-likelihood function when complete data is fully observed can be given by</p>
<p><span class="math display">\log L_{\boldsymbol{X}}(\boldsymbol{\beta})=\log f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta}).</span></p>
<p>The EM algorithm approaches the problem of solving the incomplete-data likelihood equation indirectly by proceeding iteratively in terms of <span class="math inline">\log L_{\boldsymbol{X}}(\boldsymbol{\beta})</span>. But it is unobservable since it includes missing part of the data, then we use the conditional expectation given <span class="math inline">\boldsymbol{y}</span> and current fit for <span class="math inline">\boldsymbol{\beta}</span>.</p>
<p>On the <span class="math inline">(k+1)^th</span> iteration, we have <span class="math display">
\begin{align*}
&amp;\text{E-step: Compute } Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}):=\mathbb{E}_{\boldsymbol{X}}[\log L_{\boldsymbol{X}}(\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
&amp;\text{M-step: Update }\boldsymbol{\beta}^{(k+1)} \text{ as }\boldsymbol{{\beta}}^{(k+1)}:=\text{arg}\max_{\boldsymbol{\beta}} Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) \text{ (Original EM)}\\
&amp; \text{ Or update }\boldsymbol{{\beta}}^{(k+1)} \text{ such that } Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\text{ (Generalized EM)}.
\end{align*}
</span> We keep iterating between E-step and M-step until convergence, which may be determined by a criteria such as <span class="math inline">||\boldsymbol{\beta}^{(k+1)}-\boldsymbol{\beta}^{(k)}||_p\leq \epsilon</span> for some p-norm <span class="math inline">||\cdot||_p</span> and positive <span class="math inline">\epsilon</span>.</p>
<p>Meanwhile, the M-step in both original and generalized algorithms defines a mapping from the parameter space <span class="math inline">\boldsymbol{\Omega}</span> to itself by <span class="math display">
\begin{align*}
M: \boldsymbol{\Omega} &amp;\to \boldsymbol{\Omega}\\
   \boldsymbol{\beta}^{(k)} &amp;\mapsto \boldsymbol{\beta}^{(k+1)}.
\end{align*}
</span></p>
</section>
<section id="convergence-of-the-em-algorithm" class="level3" data-number="2.3.2">
<h3 data-number="2.3.2" class="anchored" data-anchor-id="convergence-of-the-em-algorithm"><span class="header-section-number">2.3.2</span> Convergence of the EM Algorithm</h3>
<p>By the definition of conditional likelihood, our likelihood of complete data can be expressed by <span class="math display">
L_{\boldsymbol{X}}(\boldsymbol{\beta}) = f_{\boldsymbol{X}}(\boldsymbol{x};\boldsymbol{\beta})=L_{\boldsymbol{Y}}(\boldsymbol{\beta})f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}),
</span> and hence the log-likelihood is given by <span class="math display">
\log L_{\boldsymbol{X}}(\boldsymbol{\beta}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}).
</span> Take expectation to both sides of the equation with respect to <span class="math inline">\boldsymbol{x|y}</span> and replace <span class="math inline">\boldsymbol{\beta}</span> by <span class="math inline">\boldsymbol{\beta}^{(k)}</span>, we will have <span class="math display">
Q(\boldsymbol{\beta};\boldsymbol{\beta}^{(k)}) = \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}) + \mathbb{E}_{\boldsymbol{X}}[\log f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].
</span> Now consider the difference of log-likelihood of <span class="math inline">\boldsymbol{Y}</span> function between two iterations, we have <span class="math display">
\begin{align*}
  \log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k+1)})-\log L_{\boldsymbol{Y}}(\boldsymbol{\beta}^{(k)}) =
  &amp;\{Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})-Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})\}\\
  &amp;-\{\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\\
  &amp;-\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}]\}.
\end{align*}
</span> By the procedure of EM-algorithm, we always have <span class="math inline">Q(\boldsymbol{\beta}^{(k+1)};\boldsymbol{\beta}^{(k)})\geq Q(\boldsymbol{\beta}^{(k)};\boldsymbol{\beta}^{(k)})</span>. By the Jensen’s inequality, we have <span class="math inline">\mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k+1)})|\boldsymbol{y},\boldsymbol{\beta}^{(k)}] \leq \mathbb{E}_{\boldsymbol{X}}[\log         f_{\boldsymbol{X|Y}}(\boldsymbol{x}|\boldsymbol{y};\boldsymbol{\beta}^{(k)}|\boldsymbol{y},\boldsymbol{\beta}^{(k)}].</span> Therefore during iterations, the log-likelihood of observed data <span class="math inline">\boldsymbol{Y}</span> keeps increasing. If the log-likelihood is bounded, then we can promise that it must converge to some maximum.</p>
</section>
</section>
<section id="sec-penEM" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-penEM"><span class="header-section-number">2.4</span> The EM Algorithm in LASSO Factor Analytic Models</h2>
<section id="model-setup" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="model-setup"><span class="header-section-number">2.4.1</span> Model Setup</h3>
<p>Suppose we have the <span class="math inline">n</span> centralized observations, <span class="math inline">\{\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\}</span>, where <span class="math inline">\boldsymbol{y}_j\in \mathbb{R}^p</span> and hence the mean, <span class="math inline">\boldsymbol{\mu}_j=\boldsymbol{0}_p</span> (<span class="math inline">j=1,2,\dots,n</span>). For each of the observation <span class="math inline">\boldsymbol{y}_j</span>, the common factor and unique factor are <span class="math inline">\boldsymbol{f}_j</span> and <span class="math inline">\boldsymbol{\epsilon}_j</span> respectively. Denote the response matrix as <span class="math inline">\boldsymbol{Y}=[\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n]^\top</span>, the common factor matrix as <span class="math inline">\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_n]^\top</span>, and the unique factor matrix as <span class="math inline">\boldsymbol{\hat\epsilon}=[\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2,\dots,\boldsymbol{\epsilon}_n]^\top</span>. Then the model can be written as <span class="math display">\boldsymbol{Y}=\boldsymbol{F}\boldsymbol{\Lambda}^\top+\boldsymbol{\hat\epsilon}.</span><br>
We also assume</p>
<ol type="1">
<li><p><span class="math inline">\boldsymbol{\epsilon}_j</span> follows a normal distribution with mean <span class="math inline">0</span> and variance <span class="math inline">\boldsymbol{\Psi}</span> for <span class="math inline">j=1,2,\dots,n</span>, where <span class="math inline">\boldsymbol{\Psi}</span> is a diagonal matrix defined by <span class="math inline">\boldsymbol{\Psi}=diag(\psi_{11},\psi_{22},\dots,\psi_{pp})</span>.</p></li>
<li><p><span class="math inline">\boldsymbol{f}_j</span> follows a normal distribution with mean <span class="math inline">0</span> and variance <span class="math inline">\boldsymbol{I}_k</span> for <span class="math inline">j=1,2,\dots,n</span>.</p></li>
<li><p><span class="math inline">\boldsymbol{y}_j</span> follows a normal distribution with mean <span class="math inline">0</span> and variance <span class="math inline">\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}</span> for <span class="math inline">j=1,2,\dots,n</span>. Those <span class="math inline">\boldsymbol{y}_j</span> are pairwisely independent.</p></li>
</ol>
</section>
<section id="the-em-algorithm" class="level3" data-number="2.4.2">
<h3 data-number="2.4.2" class="anchored" data-anchor-id="the-em-algorithm"><span class="header-section-number">2.4.2</span> The EM Algorithm</h3>
<p>We treat the common factor matrix <span class="math inline">\boldsymbol{F}</span> as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of <span class="math inline">(\boldsymbol{Y},\boldsymbol{F})</span> given <span class="math inline">\boldsymbol{Y}</span> and <span class="math inline">(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})</span>, where <span class="math inline">(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})</span> is the parameter we got in the <span class="math inline">k</span>-th iteration (<span class="math inline">k&gt;1</span>) and <span class="math inline">(\boldsymbol{\Lambda}_{(0)},\boldsymbol{\Psi}_{(0)})</span> is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get <span class="math inline">(\boldsymbol{\Lambda}_{(k+1)},\boldsymbol{\Psi}_{(k+1)})</span>.</p>
</section>
<section id="e-step" class="level3" data-number="2.4.3">
<h3 data-number="2.4.3" class="anchored" data-anchor-id="e-step"><span class="header-section-number">2.4.3</span> E-Step</h3>
<p>First, the joint likelihood of <span class="math inline">(\boldsymbol{Y},\boldsymbol{F})</span>, denoted as <span class="math inline">L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})</span>, is given by</p>
<p><span class="math display">
\begin{align*}
L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&amp;\prod_{i=1}^nf(\boldsymbol{y}_i,\boldsymbol{f}_i)\\
=&amp;\prod_{i=1}^nf(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\\
=&amp;\prod_{i=1}^nN(\boldsymbol{y}_i;\boldsymbol{\Lambda}\boldsymbol{f}_i,\boldsymbol{\Psi})N(\boldsymbol{f}_i;\boldsymbol{0}_k,\boldsymbol{I}_k)\\
=&amp;\prod_{i=1}^n[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Psi})^{-\frac{1}{2}}\exp\{-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}\\
&amp;(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\}][(2\pi)^{-\frac{k}{2}}\det({\boldsymbol{I}_k})^{-\frac{1}{2}}\exp\{-\frac{1}{2}\boldsymbol{f}_i^\top\boldsymbol{I}_k^{-1}\boldsymbol{f}_i\}]\\
= &amp;(2\pi)^{-\frac{n}{2}(p+k)}(\prod_{i=1}^p\psi_{jj})^{-\frac{n}{2}}\exp\{-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}\}\\
&amp;\exp\{-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\}.
\end{align*}
</span></p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-hiroseSparseEstimationNonconcave2015" class="csl-entry" role="listitem">
Hirose, Kei, and Michio Yamamoto. 2015. <span>“Sparse Estimation via Nonconcave Penalized Likelihood in Factor Analysis Model.”</span> <em>Statistics and Computing</em> 25 (5): 863–75. <a href="https://doi.org/10.1007/s11222-014-9458-0">https://doi.org/10.1007/s11222-014-9458-0</a>.
</div>
<div id="ref-Jennrich2004Rotation" class="csl-entry" role="listitem">
Jennrich, R. I. 2004. <span>“Rotation to Simple Loadings Using Component Loss Functions: The Orthogonal Case.”</span> <em>Psychometrika</em> 69: 257–73.
</div>
<div id="ref-mardiaMultivariateAnalysis1979" class="csl-entry" role="listitem">
Mardia, K. V., J. T. Kent, and J. M. Bibby. 1979. <em>Multivariate Analysis</em>. Probability and Mathematical Statistics. London ; New York: Academic Press.
</div>
<div id="ref-Ng2012EMAlgorithm" class="csl-entry" role="listitem">
Ng, S. K., T. Krishnan, and G. J. McLachlan. 2012. <span>“The EM Algorithm.”</span> In <em>Handbook of Computational Statistics: Concepts and Methods</em>, 139–72. Springer.
</div>
<div id="ref-Rubin1982EMAlgorithms" class="csl-entry" role="listitem">
Rubin, Donald B., and Dorothy T. Thayer. 1982. <span>“EM Algorithms for ML Factor Analysis.”</span> <em>Psychometrika</em> 47: 69–76.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./method.html" class="pagination-link" aria-label="Method">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Method</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>Copyright © 2024, Zhining Wang</p>
<div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/ZhiningW/Master_Project/edit/main/background.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/ZhiningW/Master_Project/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>