---
bibliography: ref.bib
---
# The EM Algorithm in LASSO Factor Analytic Models

## Model Setup

Suppose we have the $n$ centralized observations, $\{\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n\}$, where $\boldsymbol{y}_j\in \mathbb{R}^p$ and hence the mean, $\boldsymbol{\mu}_j=\boldsymbol{0}_p$ ($j=1,2,\dots,n$). For each of the observation $\boldsymbol{y}_j$, the common factor and unique factor are $\boldsymbol{f}_j$ and $\boldsymbol{\epsilon}_j$ respectively. Denote the response matrix as $\boldsymbol{Y}=[\boldsymbol{y}_1,\boldsymbol{y}_2,\dots,\boldsymbol{y}_n]^\top$, the common factor matrix as $\boldsymbol{F}=[\boldsymbol{f}_1,\boldsymbol{f}_2,\dots,\boldsymbol{f}_n]^\top$, and the unique factor matrix as $\boldsymbol{\hat\epsilon}=[\boldsymbol{\epsilon}_1,\boldsymbol{\epsilon}_2,\dots,\boldsymbol{\epsilon}_n]^\top$. Then the model can be written as $$\boldsymbol{Y}=\boldsymbol{F}\boldsymbol{\Lambda}^\top+\boldsymbol{\hat\epsilon}.$$\
We also assume

1.  $\boldsymbol{\epsilon}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{\Psi}$ for $j=1,2,\dots,n$, where $\boldsymbol{\Psi}$ is a diagonal matrix defined by $\boldsymbol{\Psi}=diag(\psi_{11},\psi_{22},\dots,\psi_{pp})$.

2.  $\boldsymbol{f}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{I}_k$ for $j=1,2,\dots,n$.

3.  $\boldsymbol{y}_j$ follows a normal distribution with mean $0$ and variance $\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}$ for $j=1,2,\dots,n$. Those $\boldsymbol{y}_j$ are pairwisely independent.

## The EM Algorithm

We treat the common factor matrix $\boldsymbol{F}$ as the latent variable. In E-step, we need to compute the conditional expectation of the joint likelihood of $(\boldsymbol{Y},\boldsymbol{F})$ given $\boldsymbol{Y}$ and $(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})$, where $(\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)})$ is the parameter we got in the $k$-th iteration ($k>1$) and $(\boldsymbol{\Lambda}_{(0)},\boldsymbol{\Psi}_{(0)})$ is the initial value we set. In M-step, we maximize the conditional expectation over parameters and get $(\boldsymbol{\Lambda}_{(k+1)},\boldsymbol{\Psi}_{(k+1)})$.

### E-Step

First, the joint likelihood of $(\boldsymbol{Y},\boldsymbol{F})$, denoted as $L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$, is given by

$$
\begin{align*}
L_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&\prod_{i=1}^nf(\boldsymbol{y}_i,\boldsymbol{f}_i)\\
=&\prod_{i=1}^nf(\boldsymbol{y}_i|\boldsymbol{f}_i)f(\boldsymbol{f}_i)\\
=&\prod_{i=1}^nN(\boldsymbol{y}_i;\boldsymbol{\Lambda}\boldsymbol{f}_i,\boldsymbol{\Psi})N(\boldsymbol{f}_i;\boldsymbol{0}_k,\boldsymbol{I}_k)\\
=&\prod_{i=1}^n[(2\pi)^{-\frac{p}{2}}\det(\boldsymbol{\Psi})^{-\frac{1}{2}}\exp\{-\frac{1}{2}(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)^\top\boldsymbol{\Psi}^{-1}\\
&(\boldsymbol{y}_i-\boldsymbol{\Lambda}\boldsymbol{f}_i)\}][(2\pi)^{-\frac{k}{2}}\det({\boldsymbol{I}_k})^{-\frac{1}{2}}\exp\{-\frac{1}{2}\boldsymbol{f}_i^\top\boldsymbol{I}_k^{-1}\boldsymbol{f}_i\}]\\
= &(2\pi)^{-\frac{n}{2}(p+k)}(\prod_{i=1}^p\psi_{jj})^{-\frac{n}{2}}\exp\{-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}\}\\
&\exp\{-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i\}.
\end{align*}
$$

Furthermore, the log-likelihood, denoted as $l_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$, is given by

$$
l_{\boldsymbol{Y},\boldsymbol{F}}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2}{\psi_{jj}}-\frac{1}{2}\sum_{i=1}^n\boldsymbol{f}_i^\top\boldsymbol{f}_i+\text{constant}.
$$ {#eq-loglikelihood}

Now let us deduce the conditional expectation to $\boldsymbol{F}$ given $\boldsymbol{Y},\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)}$, denoted as $El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$. In @eq-loglikelihood, the first term is independent on $\boldsymbol{F}$, hence stay the same under conditional expectation. The last term is independent on $(\boldsymbol{\Lambda},\boldsymbol{\Psi})$, therefore we can regard it as a constant in $El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$. We only need to consider the term inside the sum in the second term, more specifically, the numerator, we have

$$
(y_{ij}-\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i)^2 = y_{ij}^2 - 2 y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{f}_i + \boldsymbol{\Lambda}(j,)\boldsymbol{f}_i\boldsymbol{f}_i^\top \boldsymbol{\Lambda}^\top(,j).
$$

Without ambiguity, denote $\mathbb{E}[\boldsymbol{f}_i|_{(k)}]$ to be the conditional expectation $\mathbb{E}[\boldsymbol{f}_i|\boldsymbol{Y},\boldsymbol{\Lambda}_{(k)},\boldsymbol{\Psi}_{(k)}]$ for simplification. Then the conditional expectation $El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$ is given by

$$
\begin{align*}
El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})=&\text{constant}-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj}}-\\
&\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2 - 2 y_{ij}{\boldsymbol{\Lambda}}(j,)\mathbb{E}[\boldsymbol{f}_i|_{(k)}]\boldsymbol+\boldsymbol{\Lambda}(j,) \mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}^\top_i|_{(k)}]\boldsymbol{\Lambda}^\top(,j)}{\psi_{jj}}\\
\end{align*}
$$

To deal with $\mathbb{E}[\boldsymbol{f}_i|_{(k)}]$ and $\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]$, we only need to know the mean and variance of conditional distribution $\boldsymbol{f}_i|\boldsymbol{Y},\boldsymbol{\Lambda}^\top_{(k)},\boldsymbol{\Psi}_{(k)}$, or equivalently $\boldsymbol{f}_i|\boldsymbol{y}_i,\boldsymbol{\Lambda}^\top_{(k)},\boldsymbol{\Psi}_{(k)}$ because of the independency of $\boldsymbol{f}_i$ to $\boldsymbol{y}_j$ for ($i\neq j$). This is because we can always treat $\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]$ as

$$
\mathbb{E}[\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)}]= \mathbb{V}[\boldsymbol{f}_i|_{(k)}]+ \mathbb{E}[\boldsymbol{f}_i|_{(k)}]\mathbb{E}[\boldsymbol{f}_i|_{(k)}]^\top.
$$ where $\mathbb{V}[\boldsymbol{f}_i|_{(k)}]$ is the variance of conditional distribution. We have

:::{#lem-1} 

If $\boldsymbol{\alpha}\sim N(\boldsymbol{\mu}_\boldsymbol{\alpha},\boldsymbol{\Sigma_\boldsymbol{\alpha}})$ and $\boldsymbol{\beta}\sim N(\boldsymbol{\mu}_\boldsymbol{\beta},\boldsymbol{\Sigma}_\boldsymbol{\beta})$, then we have $\boldsymbol{\alpha}|\boldsymbol{\beta}\sim N(\boldsymbol{\mu}_{\boldsymbol{\alpha}|\boldsymbol{\beta}},\boldsymbol{\Sigma}_{\boldsymbol{\alpha}|\boldsymbol{\beta}})$, where 
$$
\begin{align*}
& \boldsymbol{\mu}_{\boldsymbol{\alpha}|\boldsymbol{\beta}}=\boldsymbol{\mu}_\boldsymbol{\alpha}+\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]\boldsymbol{\Sigma}_\boldsymbol{\beta}^{-1}\boldsymbol{\beta}\\
& \boldsymbol{\Sigma}_{\boldsymbol{\alpha}|\boldsymbol{\beta}}= \boldsymbol{\Sigma}_\boldsymbol{\alpha}-\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]\boldsymbol{\Sigma}_\boldsymbol{\beta}^{-1}\boldsymbol{Cov}[\boldsymbol{\alpha},\boldsymbol{\beta}]^\top.
\end{align*}
$$
:::

In our scenario, we have 

$$
\begin{align*}
& \boldsymbol{\mu}_{\boldsymbol{f}_i}= \boldsymbol{0}_k, \boldsymbol{\Sigma}_{\boldsymbol{f}_{i}}=\boldsymbol{I}_k\\
& \boldsymbol{\mu}_{\boldsymbol{y}_i}= \boldsymbol{0}_p, \boldsymbol{\Sigma}_{\boldsymbol{y}_i}=\boldsymbol{\Lambda}\boldsymbol{\Lambda}^\top+\boldsymbol{\Psi}
\end{align*} 
$$

and also 

$$
\boldsymbol{Cov}(\boldsymbol{y_i},\boldsymbol{f_i})=\boldsymbol{Cov}(\boldsymbol{\Lambda f}_i+\boldsymbol{\epsilon}_i,\boldsymbol{f}_i)=\boldsymbol{\Lambda}^\top.
$$ 

Therefore we have 

$$
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(k)})=\boldsymbol{\mu}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{y}_i\\
&\mathbb{V}(\boldsymbol{f}_i|_{(k)})=\boldsymbol{\Sigma}_{\boldsymbol{f}_i|\boldsymbol{y}_i}=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{\Lambda}_{(k)}.\\
\end{align*} 
$$

For simplification, let us denote 

$$
\begin{align*}
&\boldsymbol{A}:=\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\\
&\boldsymbol{B}:=\boldsymbol{I}_k-\boldsymbol{\Lambda}_{(k)}^\top(\boldsymbol{\Lambda}_{(k)}\boldsymbol{\Lambda}_{(k)}^\top+\boldsymbol{\Psi}_{(k)})^{-1}\boldsymbol{\Lambda}_{(k)},\\
\end{align*} 
$$

we will get 

$$
\begin{align*}
&\mathbb{E}(\boldsymbol{f}_i|_{(k)})= \boldsymbol{A}\boldsymbol{y}_i\\
&\mathbb{E}(\boldsymbol{f}_i\boldsymbol{f}_i^\top|_{(k)})= \boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top.
\end{align*}
$$

Our expectation will finally be confirmed by 

$$
\begin{align*}
El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})= & -\frac{n}{2}\sum_{j=1}^p\log{\Psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2+2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\Psi_{jj}}\\
& -\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\Psi_{jj}}+\text{constant}.
\end{align*}
$$

### M-step

In M-step, we need to maximize so called $Q$-function with respect to parameters where $Q$-function is penalized conditional expectation of the log-likelihood, i.e.
$$
Q(\boldsymbol{\Lambda},\boldsymbol{\Psi}) = El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi}) - \frac{1}{2}P_{\rho}(\boldsymbol{\Lambda})
$$
We add a coefficient $\frac{1}{2}$ before $P_{\rho}(\boldsymbol{\Lambda})$ for simplification since we notice that the same coefficient occurs in each term of conditional expectation $El_{(k)}(\boldsymbol{\Lambda},\boldsymbol{\Psi})$.
When execute M-step, we use the following strategy[@Ng2012EMAlgorithm]:

1. Find $\boldsymbol{\Psi}_{(k+1)}$ using current $\boldsymbol{\Lambda}_{(k)}$, i.e.
$$
  \boldsymbol{\Psi}_{(k+1)} = \arg \max_{\boldsymbol{\Psi}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)}).
$$
2. Find $\boldsymbol{\Lambda}_{(k+1)}$ using $\boldsymbol{\Psi}_{(k+1)}$ we got in previous step, i.e. 
$$
  \boldsymbol{\Lambda}_{(k+1)} = \arg \max_{\boldsymbol{\Lambda}} (Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(k+1)}).
$$

For step 1, take partial derivative with respect to each $\psi_{jj}$ and let it equal to zero to find the local maximizer of $\psi_{jj}$, i.e. 

$$
\frac{\partial}{\partial \psi_{jj}}Q(\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)})=0.
$$
By simple calculation, we will have 

$$
\begin{align*}
\frac{\partial}{\partial \psi_{jj}}Q((\boldsymbol{\Lambda},\boldsymbol{\Psi})|\boldsymbol{\Lambda}=\boldsymbol{\Lambda}_{(k)})=
&-\frac{n}{2}\frac{1}{\psi_{jj}}+\frac{1}{2}\sum_{i=1}^n
\frac{y_{ij}^2+2y_{ij}\boldsymbol{\Lambda}_{(k)}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj}^2} \\
&+ \frac{1}{2}\sum_{i=1}^n\frac{\boldsymbol{\Lambda}_{(k)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(k)}(j,)^\top}{\psi_{jj}^2}.\\
\end{align*}
$$

Thus the update of $\boldsymbol{\Psi}$ will be elementwisely given by 

$$
\begin{equation}
\psi_{jj,(k+1)}=\frac{1}{n}\sum_{i=1}^n y_{ij}^2+\frac{2}{n}\sum_{i=1}^ny_{ij}\boldsymbol{\Lambda}_{(k)}(j,)\boldsymbol{A}\boldsymbol{y}_i + \frac{1}{n}\sum_{i=1}^n \boldsymbol{\Lambda}_{(k)}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}_{(k)}(j,)^\top.
\end{equation}
$$

For step 2, let us revise the update formula, we have 

$$
\begin{align*}
\boldsymbol{\Lambda}_{(k+1)}=&\arg \max_\boldsymbol{\Lambda} (Q(\boldsymbol{\Lambda,\boldsymbol{\Psi}})|\boldsymbol{\Psi}=\boldsymbol{\Psi}_{(k+1)})\\
=&\arg \max_\boldsymbol{\Lambda} \{-\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}\\
&-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}-\frac{1}{2}P_{\rho}(\boldsymbol{\Lambda})\\
&-\frac{n}{2}\sum_{j=1}^p\log{\psi_{jj,(k+1)}}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^p\frac{y_{ij}^2}{\psi_{jj,(k+1)}}+\text{constant}\}.\\
\end{align*}
$$

Since the last three terms do not contain any $\boldsymbol{\Lambda}$, so they can be eliminated. After letting $P_\rho(\boldsymbol{\Lambda}):=\rho\sum_{j=1}^p\sum_{i=1}^k|\lambda_{ji}|=\rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1$ as LASSO, we can rewrite it as

$$
\begin{align*}
\boldsymbol{\Lambda}_{(k+1)}

=& \arg \min_{\boldsymbol{\Lambda}}\{\sum_{i=1}^n \sum_{j=1}^p \frac{2y_{ij}\boldsymbol{\Lambda}(j,)\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}\\

& + \sum_{i=1}^n\sum_{j=1}^p\frac{\boldsymbol{\Lambda}(j,)(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}} + \rho \sum_{j=1}^p||\boldsymbol{\Lambda}(j,)^\top||_1
  \}
\end{align*} 
$$

Notice that the objective function with respect to $\boldsymbol{\Lambda}(j,)$ for any given $j=1,2,\dots,p$ is convex, and due to non-differentiablity of the $L_1$-norm at certain points, a subgradient approach can be necessary to optimize $\boldsymbol{\Lambda}$ row by row.

Denote the objective function as $g(\boldsymbol{\Lambda})$, the subdifferential of $g(\boldsymbol{\Lambda})$ with respect to some given $j=1,2,\dots,p$ is given by 
\begin{align*}
\partial_{(j)}g(\boldsymbol{\Lambda})&=\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(\boldsymbol{B}+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top+\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top+\boldsymbol{B}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \partial||\boldsymbol{\Lambda}(j,)^\top||_1\\
&=\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \partial||\boldsymbol{\Lambda}(j,)^\top||_1\\
\end{align*}

We have 

:::{#lem-subd}
For the set $\partial||\boldsymbol{\Lambda}(j,)||_1$, we have 
$$
\text{sign}(\boldsymbol{\Lambda}(j,)^\top)\in \partial||\boldsymbol{\Lambda}(j,)^\top||_1,
$$
where the sign function is given by $\text{sign}(x)=
\begin{cases}
1 &\text{if } x>0\\
0 &\text{if } x=0\\
-1 &\text{if } x<0\\
\end{cases}
$
elementwisely. 
:::

Therefore a iterative subgradient method for finding $\boldsymbol{\Lambda}_{(k+1)}$ rowwisely given by $\boldsymbol{\Lambda}_{(k)}$ is given by

:::{.callout-note}

## algorithm 
For each $j=1,2,...,p$, denote $\boldsymbol{\Lambda}_{(k+1)}^{(l)}(j,)$ as the $l^{th}$ iteration when executing the subgradient method to find the $j^{th}$ row of $\boldsymbol{\Lambda}_{(k+1)}$, we iterate as following

1. Set $\boldsymbol{\Lambda}_{(k+1)}^{(0)}(j,):=\boldsymbol{\Lambda}_{(k)}(j,)$.
2. For $l\geq 1$, calculate 
$$
\partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})=\sum_{i=1}^n\frac{2y_{ij}\boldsymbol{A}\boldsymbol{y}_i}{\psi_{jj,(k+1)}}+\sum_{i=1}^n\frac{(2\boldsymbol{B}+2\boldsymbol{A}\boldsymbol{y}_i\boldsymbol{y}_i^\top\boldsymbol{A}^\top)\boldsymbol{\Lambda}^{(l)}(j,)^\top}{\psi_{jj,(k+1)}}+\rho \text{ sign}(\boldsymbol{\Lambda}^{(l)}(j,)^\top) .
$$
3. Update $\boldsymbol{\Lambda}_{(k+1)}^{(l+1)}(j,)$ as 
$$
\boldsymbol{\Lambda}_{(k+1)}^{(l+1)}(j,)=\boldsymbol{\Lambda}_{(k+1)}^{(l)}(j,)-t^{(l)}[\partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})]^\top
$$
where $t^{(l)}$ is the step size in the $l^{th}$ iteration and a widely used choice is letting $t^{(l)}=\frac{1}{(l+1)||[\partial_{(j)}g(\boldsymbol{\Lambda}^{(l)})]^\top||_2}$, where $||\cdot||_2$ is the $L_2$ norm.(@boyd2003subgradient)
4. Repeat step 3 and 4 until converge.
:::

the EM-Algorithm for LASSO FA

